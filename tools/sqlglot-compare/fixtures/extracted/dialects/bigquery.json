{
  "dialect": "bigquery",
  "identity": [
    {
      "sql": "SELECT * FROM x-0.y",
      "expected": null
    },
    {
      "sql": "SELECT `p.d.UdF`(data) FROM `p.d.t`",
      "expected": null
    },
    {
      "sql": "SELECT EXP(1)",
      "expected": null
    },
    {
      "sql": "ARRAY_CONCAT_AGG(x ORDER BY ARRAY_LENGTH(x) LIMIT 2)",
      "expected": null
    },
    {
      "sql": "ARRAY_CONCAT_AGG(x LIMIT 2)",
      "expected": null
    },
    {
      "sql": "ARRAY_CONCAT_AGG(x ORDER BY ARRAY_LENGTH(x))",
      "expected": null
    },
    {
      "sql": "ARRAY_CONCAT_AGG(x)",
      "expected": null
    },
    {
      "sql": "PARSE_TIMESTAMP('%FT%H:%M:%E*S%z', x)",
      "expected": null
    },
    {
      "sql": "SELECT ARRAY_CONCAT([1])",
      "expected": null
    },
    {
      "sql": "SELECT * FROM READ_CSV('bla.csv')",
      "expected": null
    },
    {
      "sql": "CAST(x AS STRUCT<list ARRAY<INT64>>)",
      "expected": null
    },
    {
      "sql": "assert.true(1 = 1)",
      "expected": null
    },
    {
      "sql": "SELECT jsondoc['some_key']",
      "expected": null
    },
    {
      "sql": "SELECT `p.d.UdF`(data).* FROM `p.d.t`",
      "expected": null
    },
    {
      "sql": "SELECT * FROM `my-project.my-dataset.my-table`",
      "expected": null
    },
    {
      "sql": "CREATE OR REPLACE TABLE `a.b.c` CLONE `a.b.d`",
      "expected": null
    },
    {
      "sql": "SELECT x, 1 AS y GROUP BY 1 ORDER BY 1",
      "expected": null
    },
    {
      "sql": "SELECT * FROM x.*",
      "expected": null
    },
    {
      "sql": "SELECT * FROM x.y*",
      "expected": null
    },
    {
      "sql": "CASE A WHEN 90 THEN 'red' WHEN 50 THEN 'blue' ELSE 'green' END",
      "expected": null
    },
    {
      "sql": "CREATE SCHEMA x DEFAULT COLLATE 'en'",
      "expected": null
    },
    {
      "sql": "CREATE TABLE x (y INT64) DEFAULT COLLATE 'en'",
      "expected": null
    },
    {
      "sql": "PARSE_JSON('{}', wide_number_mode => 'exact')",
      "expected": null
    },
    {
      "sql": "FOO(values)",
      "expected": null
    },
    {
      "sql": "STRUCT(values AS value)",
      "expected": null
    },
    {
      "sql": "SELECT SEARCH(data_to_search, 'search_query')",
      "expected": null
    },
    {
      "sql": "SELECT SEARCH(data_to_search, 'search_query', json_scope => 'JSON_KEYS_AND_VALUES')",
      "expected": null
    },
    {
      "sql": "SELECT SEARCH(data_to_search, 'search_query', analyzer => 'PATTERN_ANALYZER')",
      "expected": null
    },
    {
      "sql": "SELECT SEARCH(data_to_search, 'search_query', analyzer_options => 'analyzer_options_values')",
      "expected": null
    },
    {
      "sql": "SELECT SEARCH(data_to_search, 'search_query', json_scope => 'JSON_VALUES', analyzer => 'LOG_ANALYZER')",
      "expected": null
    },
    {
      "sql": "SELECT SEARCH(data_to_search, 'search_query', analyzer => 'PATTERN_ANALYZER', analyzer_options => 'options')",
      "expected": null
    },
    {
      "sql": "ARRAY_AGG(x IGNORE NULLS LIMIT 1)",
      "expected": null
    },
    {
      "sql": "ARRAY_AGG(x IGNORE NULLS ORDER BY x LIMIT 1)",
      "expected": null
    },
    {
      "sql": "ARRAY_AGG(DISTINCT x IGNORE NULLS ORDER BY x LIMIT 1)",
      "expected": null
    },
    {
      "sql": "ARRAY_AGG(x IGNORE NULLS)",
      "expected": null
    },
    {
      "sql": "ARRAY_AGG(DISTINCT x IGNORE NULLS HAVING MAX x ORDER BY x LIMIT 1)",
      "expected": null
    },
    {
      "sql": "SELECT * FROM dataset.my_table TABLESAMPLE SYSTEM (10 PERCENT)",
      "expected": null
    },
    {
      "sql": "TIME('2008-12-25 15:30:00+08')",
      "expected": null
    },
    {
      "sql": "TIME('2008-12-25 15:30:00+08', 'America/Los_Angeles')",
      "expected": null
    },
    {
      "sql": "SELECT '\\n\\r\\a\\v\\f\\t'",
      "expected": null
    },
    {
      "sql": "SELECT * FROM tbl FOR SYSTEM_TIME AS OF z",
      "expected": null
    },
    {
      "sql": "SELECT PARSE_TIMESTAMP('%c', 'Thu Dec 25 07:30:00 2008', 'UTC')",
      "expected": null
    },
    {
      "sql": "SELECT ANY_VALUE(fruit HAVING MAX sold) FROM fruits",
      "expected": null
    },
    {
      "sql": "SELECT ANY_VALUE(fruit HAVING MIN sold) FROM fruits",
      "expected": null
    },
    {
      "sql": "SELECT `project-id`.udfs.func(call.dir)",
      "expected": null
    },
    {
      "sql": "SELECT CAST(CURRENT_DATE AS STRING FORMAT 'DAY') AS current_day",
      "expected": null
    },
    {
      "sql": "SAFE_CAST(encrypted_value AS STRING FORMAT 'BASE64')",
      "expected": null
    },
    {
      "sql": "CAST(encrypted_value AS STRING FORMAT 'BASE64')",
      "expected": null
    },
    {
      "sql": "DATE(2016, 12, 25)",
      "expected": null
    },
    {
      "sql": "DATE(CAST('2016-12-25 23:59:59' AS DATETIME))",
      "expected": null
    },
    {
      "sql": "SELECT foo IN UNNEST(bar) AS bla",
      "expected": null
    },
    {
      "sql": "SELECT * FROM x-0.a",
      "expected": null
    },
    {
      "sql": "SELECT * FROM pivot CROSS JOIN foo",
      "expected": null
    },
    {
      "sql": "SAFE_CAST(x AS STRING)",
      "expected": null
    },
    {
      "sql": "SELECT * FROM a-b-c.mydataset.mytable",
      "expected": null
    },
    {
      "sql": "SELECT * FROM abc-def-ghi",
      "expected": null
    },
    {
      "sql": "SELECT * FROM a-b-c",
      "expected": null
    },
    {
      "sql": "SELECT * FROM my-table",
      "expected": null
    },
    {
      "sql": "SELECT * FROM my-project.mydataset.mytable",
      "expected": null
    },
    {
      "sql": "SELECT * FROM pro-ject_id.c.d CROSS JOIN foo-bar",
      "expected": null
    },
    {
      "sql": "SELECT * FROM foo.bar.25",
      "expected": "SELECT * FROM foo.bar.`25`"
    },
    {
      "sql": "SELECT * FROM foo.bar.25_",
      "expected": "SELECT * FROM foo.bar.`25_`"
    },
    {
      "sql": "SELECT * FROM foo.bar.25x a",
      "expected": "SELECT * FROM foo.bar.`25x` AS a"
    },
    {
      "sql": "SELECT * FROM foo.bar.25ab c",
      "expected": "SELECT * FROM foo.bar.`25ab` AS c"
    },
    {
      "sql": "x <> ''",
      "expected": null
    },
    {
      "sql": "DATE_TRUNC(col, WEEK(MONDAY))",
      "expected": null
    },
    {
      "sql": "DATE_TRUNC(col, MONTH, 'UTC+8')",
      "expected": null
    },
    {
      "sql": "SELECT b'abc'",
      "expected": null
    },
    {
      "sql": "SELECT AS STRUCT 1 AS a, 2 AS b",
      "expected": null
    },
    {
      "sql": "SELECT DISTINCT AS STRUCT 1 AS a, 2 AS b",
      "expected": null
    },
    {
      "sql": "SELECT AS VALUE STRUCT(1 AS a, 2 AS b)",
      "expected": null
    },
    {
      "sql": "SELECT * FROM q UNPIVOT(values FOR quarter IN (b, c))",
      "expected": null
    },
    {
      "sql": "CREATE TABLE x (a STRUCT<values ARRAY<INT64>>)",
      "expected": null
    },
    {
      "sql": "CREATE TABLE x (a STRUCT<b STRING OPTIONS (description='b')>)",
      "expected": null
    },
    {
      "sql": "CAST(x AS TIMESTAMP)",
      "expected": null
    },
    {
      "sql": "BEGIN DECLARE y INT64",
      "expected": null
    },
    {
      "sql": "LOOP SET x = x + 1",
      "expected": null
    },
    {
      "sql": "REPEAT SET x = x + 1",
      "expected": null
    },
    {
      "sql": "SELECT MAKE_INTERVAL(100, 11, 1, 12, 30, 10)",
      "expected": null
    },
    {
      "sql": "WHILE i < ARRAY_LENGTH(batches) DO SET x = batches[OFFSET(i)]",
      "expected": null
    },
    {
      "sql": "BEGIN TRANSACTION",
      "expected": null
    },
    {
      "sql": "COMMIT TRANSACTION",
      "expected": null
    },
    {
      "sql": "ROLLBACK TRANSACTION",
      "expected": null
    },
    {
      "sql": "CAST(x AS BIGNUMERIC)",
      "expected": null
    },
    {
      "sql": "SELECT y + 1 FROM x GROUP BY y + 1 ORDER BY 1",
      "expected": null
    },
    {
      "sql": "SELECT TIMESTAMP_SECONDS(2) AS t",
      "expected": null
    },
    {
      "sql": "SELECT TIMESTAMP_MILLIS(2) AS t",
      "expected": null
    },
    {
      "sql": "UPDATE x SET y = NULL",
      "expected": null
    },
    {
      "sql": "LOG(n, b)",
      "expected": null
    },
    {
      "sql": "SELECT COUNT(x RESPECT NULLS)",
      "expected": null
    },
    {
      "sql": "SELECT LAST_VALUE(x IGNORE NULLS) OVER y AS x",
      "expected": null
    },
    {
      "sql": "SELECT ARRAY((SELECT AS STRUCT 1 AS a, 2 AS b))",
      "expected": null
    },
    {
      "sql": "SELECT ARRAY((SELECT AS STRUCT 1 AS a, 2 AS b) LIMIT 10)",
      "expected": null
    },
    {
      "sql": "CAST(x AS CHAR)",
      "expected": "CAST(x AS STRING)"
    },
    {
      "sql": "CAST(x AS NCHAR)",
      "expected": "CAST(x AS STRING)"
    },
    {
      "sql": "CAST(x AS NVARCHAR)",
      "expected": "CAST(x AS STRING)"
    },
    {
      "sql": "CAST(x AS TIMESTAMPTZ)",
      "expected": "CAST(x AS TIMESTAMP)"
    },
    {
      "sql": "CAST(x AS RECORD)",
      "expected": "CAST(x AS STRUCT)"
    },
    {
      "sql": "SELECT * FROM x WHERE x.y >= (SELECT MAX(a) FROM b-c) - 20",
      "expected": null
    },
    {
      "sql": "SELECT FORMAT_TIMESTAMP('%F %T', CURRENT_TIMESTAMP(), 'Europe/Berlin') AS ts",
      "expected": null
    },
    {
      "sql": "SELECT cars, apples FROM some_table PIVOT(SUM(total_counts) FOR products IN ('general.cars' AS cars, 'food.apples' AS apples))",
      "expected": null
    },
    {
      "sql": "MERGE INTO dataset.NewArrivals USING (SELECT * FROM UNNEST([('microwave', 10, 'warehouse #1'), ('dryer', 30, 'warehouse #1'), ('oven', 20, 'warehouse #2')])) ON FALSE WHEN NOT MATCHED THEN INSERT ROW WHEN NOT MATCHED BY SOURCE THEN DELETE",
      "expected": null
    },
    {
      "sql": "SELECT * FROM test QUALIFY a IS DISTINCT FROM b WINDOW c AS (PARTITION BY d)",
      "expected": null
    },
    {
      "sql": "FOR record IN (SELECT word, word_count FROM bigquery-public-data.samples.shakespeare LIMIT 5) DO SELECT record.word, record.word_count",
      "expected": null
    },
    {
      "sql": "DATE(CAST('2016-12-25 05:30:00+07' AS DATETIME), 'America/Los_Angeles')",
      "expected": null
    },
    {
      "sql": "CREATE TABLE x (a STRING OPTIONS (description='x')) OPTIONS (table_expiration_days=1)",
      "expected": null
    },
    {
      "sql": "SELECT * FROM (SELECT * FROM `t`) AS a UNPIVOT((c) FOR c_name IN (v1, v2))",
      "expected": null
    },
    {
      "sql": "CREATE TABLE IF NOT EXISTS foo AS SELECT * FROM bla EXCEPT DISTINCT (SELECT * FROM bar) LIMIT 0",
      "expected": null
    },
    {
      "sql": "SELECT ROW() OVER (y ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) FROM x WINDOW y AS (PARTITION BY CATEGORY)",
      "expected": null
    },
    {
      "sql": "SELECT item, purchases, LAST_VALUE(item) OVER (item_window ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS most_popular FROM Produce WINDOW item_window AS (ORDER BY purchases)",
      "expected": null
    },
    {
      "sql": "SELECT LAST_VALUE(a IGNORE NULLS) OVER y FROM x WINDOW y AS (PARTITION BY CATEGORY)",
      "expected": null
    },
    {
      "sql": "CREATE OR REPLACE VIEW test (tenant_id OPTIONS (description='Test description on table creation')) AS SELECT 1 AS tenant_id, 1 AS customer_id",
      "expected": null
    },
    {
      "sql": "SELECT b\"\\x0a$'x'00\"",
      "expected": "SELECT b'\\x0a$\\'x\\'00'"
    },
    {
      "sql": "--c\nARRAY_AGG(v IGNORE NULLS)",
      "expected": "ARRAY_AGG(v IGNORE NULLS) /* c */"
    },
    {
      "sql": "SELECT * FROM t1, t2",
      "expected": "SELECT * FROM t1 CROSS JOIN t2"
    },
    {
      "sql": "SELECT r\"\\t\"",
      "expected": "SELECT '\\\\t'"
    },
    {
      "sql": "ARRAY(SELECT AS STRUCT e.x AS y, e.z AS bla FROM UNNEST(bob))::ARRAY<STRUCT<y STRING, bro NUMERIC>>",
      "expected": "CAST(ARRAY(SELECT AS STRUCT e.x AS y, e.z AS bla FROM UNNEST(bob)) AS ARRAY<STRUCT<y STRING, bro NUMERIC>>)"
    },
    {
      "sql": "SELECT * FROM `proj.dataset.INFORMATION_SCHEMA.SOME_VIEW`",
      "expected": "SELECT * FROM `proj.dataset.INFORMATION_SCHEMA.SOME_VIEW` AS `proj.dataset.INFORMATION_SCHEMA.SOME_VIEW`"
    },
    {
      "sql": "SELECT * FROM region_or_dataset.INFORMATION_SCHEMA.TABLES",
      "expected": "SELECT * FROM region_or_dataset.`INFORMATION_SCHEMA.TABLES` AS TABLES"
    },
    {
      "sql": "SELECT * FROM region_or_dataset.INFORMATION_SCHEMA.TABLES AS some_name",
      "expected": "SELECT * FROM region_or_dataset.`INFORMATION_SCHEMA.TABLES` AS some_name"
    },
    {
      "sql": "SELECT * FROM proj.region_or_dataset.INFORMATION_SCHEMA.TABLES",
      "expected": "SELECT * FROM proj.region_or_dataset.`INFORMATION_SCHEMA.TABLES` AS TABLES"
    },
    {
      "sql": "CREATE VIEW `d.v` OPTIONS (expiration_timestamp=TIMESTAMP '2020-01-02T04:05:06.007Z') AS SELECT 1 AS c",
      "expected": "CREATE VIEW `d.v` OPTIONS (expiration_timestamp=CAST('2020-01-02T04:05:06.007Z' AS TIMESTAMP)) AS SELECT 1 AS c"
    },
    {
      "sql": "SELECT ARRAY(SELECT AS STRUCT 1 a, 2 b)",
      "expected": "SELECT ARRAY(SELECT AS STRUCT 1 AS a, 2 AS b)"
    },
    {
      "sql": "select array_contains([1, 2, 3], 1)",
      "expected": "SELECT EXISTS(SELECT 1 FROM UNNEST([1, 2, 3]) AS _col WHERE _col = 1)"
    },
    {
      "sql": "SELECT SPLIT(foo)",
      "expected": "SELECT SPLIT(foo, ',')"
    },
    {
      "sql": "SELECT 1 AS hash",
      "expected": "SELECT 1 AS `hash`"
    },
    {
      "sql": "SELECT 1 AS at",
      "expected": "SELECT 1 AS `at`"
    },
    {
      "sql": "x <> \"\"",
      "expected": "x <> ''"
    },
    {
      "sql": "x <> \"\"\"\"\"\"",
      "expected": "x <> ''"
    },
    {
      "sql": "x <> ''''''",
      "expected": "x <> ''"
    },
    {
      "sql": "SELECT a overlaps",
      "expected": "SELECT a AS overlaps"
    },
    {
      "sql": "SELECT y + 1 z FROM x GROUP BY y + 1 ORDER BY z",
      "expected": "SELECT y + 1 AS z FROM x GROUP BY z ORDER BY z"
    },
    {
      "sql": "SELECT y + 1 z FROM x GROUP BY y + 1",
      "expected": "SELECT y + 1 AS z FROM x GROUP BY y + 1"
    },
    {
      "sql": "SELECT JSON '\"foo\"' AS json_data",
      "expected": "SELECT PARSE_JSON('\"foo\"') AS json_data"
    },
    {
      "sql": "SELECT * FROM (SELECT a, b, c FROM test) PIVOT(SUM(b) d, COUNT(*) e FOR c IN ('x', 'y'))",
      "expected": "SELECT * FROM (SELECT a, b, c FROM test) PIVOT(SUM(b) AS d, COUNT(*) AS e FOR c IN ('x', 'y'))"
    },
    {
      "sql": "SELECT CAST(1 AS BYTEINT)",
      "expected": "SELECT CAST(1 AS INT64)"
    },
    {
      "sql": "CREATE TEMPORARY FUNCTION FOO()\nRETURNS STRING\nLANGUAGE js AS\n'return \"Hello world!\"'",
      "expected": null
    },
    {
      "sql": "[a, a(1, 2,3,4444444444444444, tttttaoeunthaoentuhaoentuheoantu, toheuntaoheutnahoeunteoahuntaoeh), b(3, 4,5), c, d, tttttttttttttttteeeeeeeeeeeeeett, 12312312312]",
      "expected": "[\n  a,\n  a(\n    1,\n    2,\n    3,\n    4444444444444444,\n    tttttaoeunthaoentuhaoentuheoantu,\n    toheuntaoheutnahoeunteoahuntaoeh\n  ),\n  b(3, 4, 5),\n  c,\n  d,\n  tttttttttttttttteeeeeeeeeeeeeett,\n  12312312312\n]"
    },
    {
      "sql": "CREATE TEMP TABLE foo AS SELECT 1",
      "expected": "CREATE TEMPORARY TABLE foo AS SELECT 1"
    },
    {
      "sql": "SELECT * FROM a-b c",
      "expected": "SELECT * FROM a-b AS c"
    },
    {
      "sql": "CONTAINS_SUBSTR(a, b, json_scope => 'JSON_KEYS_AND_VALUES')",
      "expected": null
    },
    {
      "sql": "EXPORT DATA OPTIONS (URI='gs://path*.csv.gz', FORMAT='CSV') AS SELECT * FROM all_rows",
      "expected": null
    },
    {
      "sql": "EXPORT DATA WITH CONNECTION myproject.us.myconnection OPTIONS (URI='gs://path*.csv.gz', FORMAT='CSV') AS SELECT * FROM all_rows",
      "expected": null
    },
    {
      "sql": "ARRAY_FIRST(['a', 'b'])",
      "expected": null
    },
    {
      "sql": "ARRAY_LAST(['a', 'b'])",
      "expected": null
    },
    {
      "sql": "JSON_TYPE(PARSE_JSON('1'))",
      "expected": null
    },
    {
      "sql": "SELECT PARSE_DATETIME('%a %b %e %I:%M:%S %Y', 'Thu Dec 25 07:30:00 2008')",
      "expected": null
    },
    {
      "sql": "FORMAT_TIME('%R', CAST('15:30:00' AS TIME))",
      "expected": null
    },
    {
      "sql": "PARSE_TIME('%I:%M:%S', '07:30:00')",
      "expected": null
    },
    {
      "sql": "BYTE_LENGTH('foo')",
      "expected": null
    },
    {
      "sql": "BYTE_LENGTH(b'foo')",
      "expected": null
    },
    {
      "sql": "CODE_POINTS_TO_STRING([65, 255])",
      "expected": null
    },
    {
      "sql": "APPROX_TOP_COUNT(col, 2)",
      "expected": null
    },
    {
      "sql": "ARPOX_TOP_SUM(col, 1.5, 2)",
      "expected": null
    },
    {
      "sql": "SAFE_CONVERT_BYTES_TO_STRING(b'Ã‚')",
      "expected": null
    },
    {
      "sql": "FROM_HEX('foo')",
      "expected": null
    },
    {
      "sql": "TO_CODE_POINTS('foo')",
      "expected": null
    },
    {
      "sql": "CODE_POINTS_TO_BYTES([65, 98])",
      "expected": null
    },
    {
      "sql": "PARSE_BIGNUMERIC('1.2')",
      "expected": null
    },
    {
      "sql": "PARSE_NUMERIC('1.2')",
      "expected": null
    },
    {
      "sql": "BOOL(PARSE_JSON('true'))",
      "expected": null
    },
    {
      "sql": "FLOAT64(PARSE_JSON('9.8'))",
      "expected": null
    },
    {
      "sql": "FLOAT64(PARSE_JSON('9.8'), wide_number_mode => 'round')",
      "expected": null
    },
    {
      "sql": "FLOAT64(PARSE_JSON('9.8'), wide_number_mode => 'exact')",
      "expected": null
    },
    {
      "sql": "NORMALIZE_AND_CASEFOLD('foo')",
      "expected": null
    },
    {
      "sql": "NORMALIZE_AND_CASEFOLD('foo', NFKC)",
      "expected": null
    },
    {
      "sql": "OCTET_LENGTH('foo')",
      "expected": "BYTE_LENGTH('foo')"
    },
    {
      "sql": "OCTET_LENGTH(b'foo')",
      "expected": "BYTE_LENGTH(b'foo')"
    },
    {
      "sql": "JSON_ARRAY_APPEND(PARSE_JSON('[\"a\", \"b\", \"c\"]'), '$', [1, 2], append_each_element => FALSE)",
      "expected": null
    },
    {
      "sql": "JSON_ARRAY_INSERT(PARSE_JSON('[\"a\", \"b\", \"c\"]'), '$[1]', [1, 2], insert_each_element => FALSE)",
      "expected": null
    },
    {
      "sql": "JSON_KEYS(PARSE_JSON('{\"a\": {\"b\":1}}'))",
      "expected": null
    },
    {
      "sql": "JSON_KEYS(PARSE_JSON('{\"a\": {\"b\":1}}', 1))",
      "expected": null
    },
    {
      "sql": "JSON_KEYS(PARSE_JSON('{\"a\": {\"b\":1}}'), 1, mode => 'lax')",
      "expected": null
    },
    {
      "sql": "JSON_SET(PARSE_JSON('{\"a\": 1}'), '$.b', 999, create_if_missing => FALSE)",
      "expected": null
    },
    {
      "sql": "JSON_STRIP_NULLS(PARSE_JSON('[1, null, 2, null, [null]]'))",
      "expected": null
    },
    {
      "sql": "JSON_STRIP_NULLS(PARSE_JSON('[1, null, 2, null]'), include_arrays => FALSE)",
      "expected": null
    },
    {
      "sql": "JSON_STRIP_NULLS(PARSE_JSON('{\"a\": {\"b\": {\"c\": null}}, \"d\": [null], \"e\": [], \"f\": 1}'), include_arrays => FALSE, remove_empty => TRUE)",
      "expected": null
    },
    {
      "sql": "JSON_EXTRACT_STRING_ARRAY(PARSE_JSON('{\"fruits\": [\"apples\", \"oranges\", \"grapes\"]}'), '$.fruits')",
      "expected": "JSON_VALUE_ARRAY(PARSE_JSON('{\"fruits\": [\"apples\", \"oranges\", \"grapes\"]}'), '$.fruits')"
    },
    {
      "sql": "TO_JSON(STRUCT(1 AS id, [10, 20] AS cords))",
      "expected": null
    },
    {
      "sql": "TO_JSON(9999999999, stringify_wide_numbers => FALSE)",
      "expected": null
    },
    {
      "sql": "RANGE_BUCKET(20, [0, 10, 20, 30, 40])",
      "expected": null
    },
    {
      "sql": "SELECT TRANSLATE(MODEL, 'in', 't') FROM (SELECT 'input' AS MODEL)",
      "expected": null
    },
    {
      "sql": "SELECT GRANT FROM (SELECT 'input' AS GRANT)",
      "expected": null
    },
    {
      "sql": "CREATE TEMPORARY FUNCTION a(x FLOAT64, y FLOAT64) RETURNS FLOAT64 NOT DETERMINISTIC LANGUAGE js AS 'return x*y;'",
      "expected": null
    },
    {
      "sql": "CREATE TEMPORARY FUNCTION udf(x ANY TYPE) AS (x)",
      "expected": null
    },
    {
      "sql": "CREATE TEMPORARY FUNCTION a(x FLOAT64, y FLOAT64) AS ((x + 4) / y)",
      "expected": null
    },
    {
      "sql": "CREATE TABLE FUNCTION a(x INT64) RETURNS TABLE <q STRING, r INT64> AS SELECT s, t",
      "expected": null
    },
    {
      "sql": "CREATE TEMPORARY FUNCTION string_length_0(strings ARRAY<STRING>) RETURNS FLOAT64 LANGUAGE js AS \"\"\"'use strict'; function string_length(strings) { return _.sum(_.map(strings, ((x) => x.length))); } return string_length(strings);\"\"\" OPTIONS (library=['gs://ibis-testing-libraries/lodash.min.js'])",
      "expected": "CREATE TEMPORARY FUNCTION string_length_0(strings ARRAY<STRING>) RETURNS FLOAT64 LANGUAGE js OPTIONS (library=['gs://ibis-testing-libraries/lodash.min.js']) AS '\\'use strict\\'; function string_length(strings) { return _.sum(_.map(strings, ((x) => x.length))); } return string_length(strings);'"
    },
    {
      "sql": "CREATE TABLE test (a NUMERIC(10, 2))",
      "expected": null
    },
    {
      "sql": "INSERT INTO test (cola, colb) VALUES (CAST(7 AS STRING(10)), CAST(14 AS STRING(10)))",
      "expected": "INSERT INTO test (cola, colb) VALUES (CAST(7 AS STRING), CAST(14 AS STRING))"
    },
    {
      "sql": "SELECT CAST(1 AS NUMERIC(10, 2))",
      "expected": "SELECT CAST(1 AS NUMERIC)"
    },
    {
      "sql": "SELECT CAST('1' AS STRING(10)) UNION ALL SELECT CAST('2' AS STRING(10))",
      "expected": "SELECT CAST('1' AS STRING) UNION ALL SELECT CAST('2' AS STRING)"
    },
    {
      "sql": "SELECT cola FROM (SELECT CAST('1' AS STRING(10)) AS cola UNION ALL SELECT CAST('2' AS STRING(10)) AS cola)",
      "expected": "SELECT cola FROM (SELECT CAST('1' AS STRING) AS cola UNION ALL SELECT CAST('2' AS STRING) AS cola)"
    },
    {
      "sql": "SELECT * FROM GAP_FILL(TABLE device_data, ts_column => 'time', bucket_width => INTERVAL '1' MINUTE, value_columns => [('signal', 'locf')]) ORDER BY time",
      "expected": null
    },
    {
      "sql": "SELECT a, b, c, d, e FROM GAP_FILL(TABLE foo, ts_column => 'b', partitioning_columns => ['a'], value_columns => [('c', 'bar'), ('d', 'baz'), ('e', 'bla')], bucket_width => INTERVAL '1' DAY)",
      "expected": null
    },
    {
      "sql": "SELECT * FROM GAP_FILL(TABLE device_data, ts_column => 'time', bucket_width => INTERVAL '1' MINUTE, value_columns => [('signal', 'linear')], ignore_null_values => FALSE) ORDER BY time",
      "expected": null
    },
    {
      "sql": "SELECT * FROM GAP_FILL(TABLE device_data, ts_column => 'time', bucket_width => INTERVAL '1' MINUTE) ORDER BY time",
      "expected": null
    },
    {
      "sql": "SELECT * FROM GAP_FILL(TABLE device_data, ts_column => 'time', bucket_width => INTERVAL '1' MINUTE, value_columns => [('signal', 'null')], origin => CAST('2023-11-01 09:30:01' AS DATETIME)) ORDER BY time",
      "expected": null
    },
    {
      "sql": "SELECT * FROM GAP_FILL(TABLE device_data, ts_column => 'time', bucket_width => INTERVAL '1' MINUTE, value_columns => [('signal', 'locf')]) ORDER BY time",
      "expected": null
    },
    {
      "sql": "CREATE OR REPLACE MODEL foo OPTIONS (model_type='linear_reg') AS SELECT bla FROM foo WHERE cond",
      "expected": null
    },
    {
      "sql": "CREATE OR REPLACE MODEL m\nTRANSFORM(\n  ML.FEATURE_CROSS(STRUCT(f1, f2)) AS cross_f,\n  ML.QUANTILE_BUCKETIZE(f3) OVER () AS buckets,\n  label_col\n)\nOPTIONS (\n  model_type='linear_reg',\n  input_label_cols=['label_col']\n) AS\nSELECT\n  *\nFROM t",
      "expected": null
    },
    {
      "sql": "CREATE MODEL project_id.mydataset.mymodel\nINPUT(\n  f1 INT64,\n  f2 FLOAT64,\n  f3 STRING,\n  f4 ARRAY<INT64>\n)\nOUTPUT(\n  out1 INT64,\n  out2 INT64\n)\nREMOTE WITH CONNECTION myproject.us.test_connection\nOPTIONS (\n  ENDPOINT='https://us-central1-aiplatform.googleapis.com/v1/projects/myproject/locations/us-central1/endpoints/1234'\n)",
      "expected": null
    },
    {
      "sql": "SELECT * FROM ML.PREDICT(MODEL mydataset.mymodel, (SELECT label, column1, column2 FROM mydataset.mytable))",
      "expected": null
    },
    {
      "sql": "SELECT label, predicted_label1, predicted_label AS predicted_label2 FROM ML.PREDICT(MODEL mydataset.mymodel2, (SELECT * EXCEPT (predicted_label), predicted_label AS predicted_label1 FROM ML.PREDICT(MODEL mydataset.mymodel1, TABLE mydataset.mytable)))",
      "expected": null
    },
    {
      "sql": "SELECT * FROM ML.PREDICT(MODEL mydataset.mymodel, (SELECT custom_label, column1, column2 FROM mydataset.mytable), STRUCT(0.55 AS threshold))",
      "expected": null
    },
    {
      "sql": "SELECT COSH(1.5)",
      "expected": null
    },
    {
      "sql": "SELECT * FROM ML.PREDICT(MODEL `my_project`.my_dataset.my_model, (SELECT * FROM input_data))",
      "expected": null
    },
    {
      "sql": "SELECT * FROM ML.PREDICT(MODEL my_dataset.vision_model, (SELECT uri, ML.RESIZE_IMAGE(ML.DECODE_IMAGE(data), 480, 480, FALSE) AS input FROM my_dataset.object_table))",
      "expected": null
    },
    {
      "sql": "SELECT * FROM ML.PREDICT(MODEL my_dataset.vision_model, (SELECT uri, ML.CONVERT_COLOR_SPACE(ML.RESIZE_IMAGE(ML.DECODE_IMAGE(data), 224, 280, TRUE), 'YIQ') AS input FROM my_dataset.object_table WHERE content_type = 'image/jpeg'))",
      "expected": null
    },
    {
      "sql": "SELECT * FROM ML.FEATURES_AT_TIME((SELECT 1), num_rows => 1)",
      "expected": null
    },
    {
      "sql": "SELECT * FROM ML.FEATURES_AT_TIME(TABLE mydataset.feature_table, time => '2022-06-11 10:00:00+00', num_rows => 1, ignore_feature_nulls => TRUE)",
      "expected": null
    },
    {
      "sql": "SELECT * FROM VECTOR_SEARCH(TABLE mydataset.base_table, 'column_to_search', TABLE mydataset.query_table, 'query_column_to_search', top_k => 2, distance_type => 'cosine', options => '{\"fraction_lists_to_search\":0.15}')",
      "expected": null
    },
    {
      "sql": "SELECT * FROM VECTOR_SEARCH(TABLE mydataset.base_table, 'column_to_search', TABLE mydataset.query_table, query_column_to_search => 'query_column_to_search', top_k => 2, distance_type => 'cosine', options => '{\"fraction_lists_to_search\":0.15}')",
      "expected": null
    },
    {
      "sql": "SELECT * FROM VECTOR_SEARCH((SELECT * FROM mydataset.base_table), 'column_to_search', (SELECT * FROM mydataset.query_table), 'query_column_to_search')",
      "expected": null
    },
    {
      "sql": "SELECT * FROM VECTOR_SEARCH(TABLE mydataset.base_table, 'column_to_search', TABLE mydataset.query_table)",
      "expected": null
    },
    {
      "sql": "SELECT * FROM ML.TRANSLATE(MODEL `mydataset.mytranslatemodel`, TABLE `mydataset.mybqtable`, STRUCT('translate_text' AS translate_mode, 'zh-CN' AS target_language_code))",
      "expected": null
    },
    {
      "sql": "SELECT * FROM ML.FORECAST(MODEL `mydataset.mymodel`, STRUCT(2 AS horizon))",
      "expected": null
    },
    {
      "sql": "SELECT * FROM ML.FORECAST(MODEL `mydataset.mymodel`, TABLE `mydataset.mybqtable`, STRUCT(2 AS horizon, 4 AS confidence_level))",
      "expected": null
    },
    {
      "sql": "SELECT * FROM ML.FORECAST(MODEL `mydataset.mymodel`, (SELECT * FROM mydataset.query_table), STRUCT())",
      "expected": null
    },
    {
      "sql": "SELECT JSON_OBJECT() AS json_data",
      "expected": null
    },
    {
      "sql": "SELECT JSON_OBJECT('foo', 10, 'bar', TRUE) AS json_data",
      "expected": null
    },
    {
      "sql": "SELECT JSON_OBJECT('foo', 10, 'bar', ['a', 'b']) AS json_data",
      "expected": null
    },
    {
      "sql": "SELECT JSON_OBJECT('a', 10, 'a', 'foo') AS json_data",
      "expected": null
    },
    {
      "sql": "SELECT JSON_OBJECT(['a', 'b'], [10, NULL]) AS json_data",
      "expected": "SELECT JSON_OBJECT('a', 10, 'b', NULL) AS json_data"
    },
    {
      "sql": "SELECT JSON_OBJECT(['a', 'b'], [JSON '10', JSON '\"foo\"']) AS json_data",
      "expected": "SELECT JSON_OBJECT('a', PARSE_JSON('10'), 'b', PARSE_JSON('\"foo\"')) AS json_data"
    },
    {
      "sql": "SELECT JSON_OBJECT(['a', 'b'], [STRUCT(10 AS id, 'Red' AS color), STRUCT(20 AS id, 'Blue' AS color)]) AS json_data",
      "expected": "SELECT JSON_OBJECT('a', STRUCT(10 AS id, 'Red' AS color), 'b', STRUCT(20 AS id, 'Blue' AS color)) AS json_data"
    },
    {
      "sql": "SELECT JSON_OBJECT(['a', 'b'], [TO_JSON(10), TO_JSON(['foo', 'bar'])]) AS json_data",
      "expected": "SELECT JSON_OBJECT('a', TO_JSON(10), 'b', TO_JSON(['foo', 'bar'])) AS json_data"
    },
    {
      "sql": "SELECT MOD((SELECT 1), 2)",
      "expected": null
    },
    {
      "sql": "MOD((a + 1), b)",
      "expected": "MOD(a + 1, b)"
    },
    {
      "sql": "SELECT STRUCT<ARRAY<STRING>>([\"2023-01-17\"])",
      "expected": "SELECT CAST(STRUCT(['2023-01-17']) AS STRUCT<ARRAY<STRING>>)"
    },
    {
      "sql": "SELECT STRUCT<STRING>((SELECT 'foo')).*",
      "expected": "SELECT CAST(STRUCT((SELECT 'foo')) AS STRUCT<STRING>).*"
    },
    {
      "sql": "SELECT RANGE(CAST('2022-12-01' AS DATE), CAST('2022-12-31' AS DATE))",
      "expected": null
    },
    {
      "sql": "SELECT RANGE(NULL, CAST('2022-12-31' AS DATE))",
      "expected": null
    },
    {
      "sql": "SELECT RANGE(CAST('2022-10-01 14:53:27' AS DATETIME), CAST('2022-10-01 16:00:00' AS DATETIME))",
      "expected": null
    },
    {
      "sql": "SELECT RANGE(CAST('2022-10-01 14:53:27 America/Los_Angeles' AS TIMESTAMP), CAST('2022-10-01 16:00:00 America/Los_Angeles' AS TIMESTAMP))",
      "expected": null
    },
    {
      "sql": "REGEXP_EXTRACT(x, '(?<)')",
      "expected": null
    },
    {
      "sql": "REGEXP_EXTRACT(`foo`, 'bar: (.+?)', 1, 1)",
      "expected": null
    },
    {
      "sql": "REGEXP_EXTRACT(svc_plugin_output, r'\\\\\\((.*)')",
      "expected": "REGEXP_EXTRACT(svc_plugin_output, '\\\\\\\\\\\\((.*)')"
    },
    {
      "sql": "REGEXP_SUBSTR(value, pattern, position, occurrence)",
      "expected": "REGEXP_EXTRACT(value, pattern, position, occurrence)"
    },
    {
      "sql": "STRING_AGG(a, ' & ')",
      "expected": null
    },
    {
      "sql": "STRING_AGG(DISTINCT a, ' & ')",
      "expected": null
    },
    {
      "sql": "STRING_AGG(a, ' & ' ORDER BY LENGTH(a))",
      "expected": null
    },
    {
      "sql": "STRING_AGG(foo, b'|' ORDER BY bar)",
      "expected": null
    },
    {
      "sql": "STRING_AGG(a)",
      "expected": null
    },
    {
      "sql": "STRING_AGG(DISTINCT v, sep LIMIT 3)",
      "expected": null
    },
    {
      "sql": "STRING_AGG(DISTINCT a ORDER BY b DESC, c DESC LIMIT 10)",
      "expected": null
    },
    {
      "sql": "SELECT a, GROUP_CONCAT(b) FROM table GROUP BY a",
      "expected": "SELECT a, STRING_AGG(b) FROM table GROUP BY a"
    },
    {
      "sql": "SELECT 1 AS foo INNER UNION ALL SELECT 3 AS foo, 4 AS bar",
      "expected": null
    },
    {
      "sql": "SELECT 1 AS x UNION ALL CORRESPONDING SELECT 2 AS x",
      "expected": "SELECT 1 AS x INNER UNION ALL BY NAME SELECT 2 AS x"
    },
    {
      "sql": "SELECT 1 AS x UNION ALL CORRESPONDING BY (foo, bar) SELECT 2 AS x",
      "expected": "SELECT 1 AS x INNER UNION ALL BY NAME ON (foo, bar) SELECT 2 AS x"
    },
    {
      "sql": "SELECT 1 AS x LEFT UNION ALL CORRESPONDING SELECT 2 AS x",
      "expected": "SELECT 1 AS x LEFT UNION ALL BY NAME SELECT 2 AS x"
    },
    {
      "sql": "SELECT 1 AS x UNION ALL STRICT CORRESPONDING SELECT 2 AS x",
      "expected": "SELECT 1 AS x UNION ALL BY NAME SELECT 2 AS x"
    },
    {
      "sql": "SELECT 1 AS x UNION ALL STRICT CORRESPONDING BY (foo, bar) SELECT 2 AS x",
      "expected": "SELECT 1 AS x UNION ALL BY NAME ON (foo, bar) SELECT 2 AS x"
    },
    {
      "sql": "SELECT * FROM UNNEST(x) WITH OFFSET EXCEPT DISTINCT SELECT * FROM UNNEST(y) WITH OFFSET",
      "expected": "SELECT * FROM UNNEST(x) WITH OFFSET AS offset EXCEPT DISTINCT SELECT * FROM UNNEST(y) WITH OFFSET AS offset"
    },
    {
      "sql": "JSON_ARRAY()",
      "expected": null
    },
    {
      "sql": "JSON_ARRAY(10)",
      "expected": null
    },
    {
      "sql": "JSON_ARRAY([])",
      "expected": null
    },
    {
      "sql": "JSON_ARRAY(STRUCT(10 AS a, 'foo' AS b))",
      "expected": null
    },
    {
      "sql": "JSON_ARRAY(10, ['foo', 'bar'], [20, 30])",
      "expected": null
    },
    {
      "sql": "DECLARE X INT64",
      "expected": null
    },
    {
      "sql": "DECLARE X INT64 DEFAULT 1",
      "expected": null
    },
    {
      "sql": "DECLARE X FLOAT64 DEFAULT 0.9",
      "expected": null
    },
    {
      "sql": "DECLARE X INT64 DEFAULT (SELECT MAX(col) FROM foo)",
      "expected": null
    },
    {
      "sql": "DECLARE X, Y, Z INT64",
      "expected": null
    },
    {
      "sql": "DECLARE X, Y, Z INT64 DEFAULT 42",
      "expected": null
    },
    {
      "sql": "DECLARE X, Y, Z INT64 DEFAULT (SELECT 42)",
      "expected": null
    },
    {
      "sql": "DECLARE START_DATE DATE DEFAULT CURRENT_DATE - 1",
      "expected": null
    },
    {
      "sql": "DECLARE TS TIMESTAMP DEFAULT CURRENT_TIMESTAMP() - INTERVAL '1' HOUR",
      "expected": null
    },
    {
      "sql": "DATE_TRUNC(date, WEEK(MONDAY))",
      "expected": null
    },
    {
      "sql": "LAST_DAY(DATETIME '2008-11-10 15:30:00', WEEK(SUNDAY))",
      "expected": "LAST_DAY(CAST('2008-11-10 15:30:00' AS DATETIME), WEEK)"
    },
    {
      "sql": "DATE_DIFF('2017-12-18', '2017-12-17', WEEK(SATURDAY))",
      "expected": null
    },
    {
      "sql": "DATETIME_DIFF('2017-12-18', '2017-12-17', WEEK(MONDAY))",
      "expected": null
    },
    {
      "sql": "EXTRACT(WEEK(THURSDAY) FROM DATE '2013-12-25')",
      "expected": "EXTRACT(WEEK(THURSDAY) FROM CAST('2013-12-25' AS DATE))"
    },
    {
      "sql": "APPROX_QUANTILES(foo, 2)",
      "expected": null
    },
    {
      "sql": "APPROX_QUANTILES(DISTINCT foo, 2 RESPECT NULLS)",
      "expected": null
    },
    {
      "sql": "APPROX_QUANTILES(DISTINCT foo, 2 IGNORE NULLS)",
      "expected": null
    },
    {
      "sql": "LAX_BOOL(PARSE_JSON('true'))",
      "expected": null
    },
    {
      "sql": "LAX_FLOAT64(PARSE_JSON('9.8'))",
      "expected": null
    },
    {
      "sql": "LAX_INT64(PARSE_JSON('10'))",
      "expected": null
    },
    {
      "sql": "LAX_STRING(PARSE_JSON('\"str\"'))",
      "expected": null
    },
    {
      "sql": "SAFE_NEGATE(x)",
      "expected": null
    },
    {
      "sql": "SELECT col FROM t WHERE _PARTITIONTIME BETWEEN a AND b",
      "expected": null
    },
    {
      "sql": "SELECT _DBT_MAX_PARTITION FROM t",
      "expected": null
    },
    {
      "sql": "APPROX_QUANTILES(x, 2)",
      "expected": null
    },
    {
      "sql": "APPROX_QUANTILES(FALSE OR TRUE, 2)",
      "expected": null
    },
    {
      "sql": "APPROX_QUANTILES((SELECT 1 AS val), CAST(2.1 AS INT64))",
      "expected": null
    },
    {
      "sql": "APPROX_QUANTILES(DISTINCT x, 2)",
      "expected": null
    },
    {
      "sql": "APPROX_QUANTILES(x, 2 RESPECT NULLS)",
      "expected": null
    },
    {
      "sql": "APPROX_QUANTILES(x, 2 IGNORE NULLS)",
      "expected": null
    },
    {
      "sql": "APPROX_QUANTILES(DISTINCT x, 2 RESPECT NULLS)",
      "expected": null
    },
    {
      "sql": "WITH cte(c) AS (SELECT * FROM t) SELECT * FROM cte",
      "expected": "WITH cte AS (SELECT * FROM t) SELECT * FROM cte"
    },
    {
      "sql": "SELECT * FROM t AS t(c1, c2)",
      "expected": "SELECT * FROM t AS t"
    },
    {
      "sql": "SELECT * FROM t AS t(c1, c2)",
      "expected": "SELECT * FROM t AS t"
    },
    {
      "sql": "SAFE.SOME_RANDOM_FUNC(a, b, c)",
      "expected": null
    },
    {
      "sql": "TIMESTAMP(foo, zone)",
      "expected": null
    },
    {
      "sql": "SELECT `db.t`.`c` FROM `db.t`",
      "expected": null
    },
    {
      "sql": "TRANSLATE(x, y, z)",
      "expected": null
    },
    {
      "sql": "DATE_TRUNC(x, @foo)",
      "expected": null
    },
    {
      "sql": "SELECT * FROM ML.TRANSLATE(MODEL `mydataset.mymodel`, (SELECT comment AS text_content FROM mydataset.mytable), STRUCT('translate_text' AS translate_mode, 'en' AS target_language_code))",
      "expected": null
    },
    {
      "sql": "SAFE.SUBSTR('foo', 0, -2)",
      "expected": null
    },
    {
      "sql": "SAFE.TIMESTAMP(foo, zone)",
      "expected": null
    },
    {
      "sql": "SAFE.PARSE_DATE('%Y-%m-%d', '2024-01-15')",
      "expected": "SAFE.PARSE_DATE('%F', '2024-01-15')"
    },
    {
      "sql": "SAFE.PARSE_DATETIME('%Y-%m-%d %H:%M:%S', '2024-01-15 10:30:00')",
      "expected": "SAFE.PARSE_DATETIME('%F %T', '2024-01-15 10:30:00')"
    },
    {
      "sql": "SAFE.PARSE_TIMESTAMP('%Y-%m-%d %H:%M:%S', '2024-01-15 10:30:00')",
      "expected": "SAFE.PARSE_TIMESTAMP('%F %T', '2024-01-15 10:30:00')"
    },
    {
      "sql": "NET.HOST('http://example.com')",
      "expected": null
    },
    {
      "sql": "NET.REG_DOMAIN('http://example.com')",
      "expected": null
    },
    {
      "sql": "WITH t AS (SELECT '{\"x-y\": \"z\"}' AS c) SELECT JSON_EXTRACT(c, '$.x-y') FROM t",
      "expected": null
    }
  ],
  "transpilation": [
    {
      "sql": "SELECT ANY_VALUE(fruit HAVING MAX sold) FROM Store",
      "read": {},
      "write": {
        "bigquery": "SELECT ANY_VALUE(fruit HAVING MAX sold) FROM Store",
        "duckdb": "SELECT ARG_MAX_NULL(fruit, sold) FROM Store"
      }
    },
    {
      "sql": "SELECT ANY_VALUE(fruit HAVING MIN sold) FROM Store",
      "read": {},
      "write": {
        "bigquery": "SELECT ANY_VALUE(fruit HAVING MIN sold) FROM Store",
        "duckdb": "SELECT ARG_MIN_NULL(fruit, sold) FROM Store"
      }
    },
    {
      "sql": "SELECT category, ANY_VALUE(product HAVING MAX price), ANY_VALUE(product HAVING MIN cost), ANY_VALUE(supplier) FROM products GROUP BY category",
      "read": {},
      "write": {
        "bigquery": "SELECT category, ANY_VALUE(product HAVING MAX price), ANY_VALUE(product HAVING MIN cost), ANY_VALUE(supplier) FROM products GROUP BY category",
        "duckdb": "SELECT category, ARG_MAX_NULL(product, price), ARG_MIN_NULL(product, cost), ANY_VALUE(supplier) FROM products GROUP BY category"
      }
    },
    {
      "sql": "WITH data AS (SELECT \"A\" AS fruit, 20 AS sold UNION ALL SELECT NULL AS fruit, 25 AS sold) SELECT ANY_VALUE(fruit HAVING MAX sold) FROM data",
      "read": {},
      "write": {
        "duckdb": "WITH data AS (SELECT 'A' AS fruit, 20 AS sold UNION ALL SELECT NULL AS fruit, 25 AS sold) SELECT ARG_MAX_NULL(fruit, sold) FROM data"
      }
    },
    {
      "sql": "SELECT TRUE IS TRUE",
      "read": {},
      "write": {
        "bigquery": "SELECT TRUE IS TRUE",
        "snowflake": "SELECT TRUE"
      }
    },
    {
      "sql": "SELECT REPEAT(' ', 2)",
      "read": {
        "hive": "SELECT SPACE(2)",
        "spark": "SELECT SPACE(2)",
        "databricks": "SELECT SPACE(2)",
        "trino": "SELECT REPEAT(' ', 2)"
      },
      "write": {}
    },
    {
      "sql": "SELECT purchases, LAST_VALUE(item) OVER item_window AS most_popular FROM Produce WINDOW item_window AS (PARTITION BY purchases ORDER BY purchases ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING)",
      "read": {},
      "write": {
        "bigquery": "SELECT purchases, LAST_VALUE(item) OVER item_window AS most_popular FROM Produce WINDOW item_window AS (PARTITION BY purchases ORDER BY purchases ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING)",
        "clickhouse": "SELECT purchases, LAST_VALUE(item) OVER item_window AS most_popular FROM Produce WINDOW item_window AS (PARTITION BY purchases ORDER BY purchases NULLS FIRST ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING)",
        "databricks": "SELECT purchases, LAST_VALUE(item) OVER item_window AS most_popular FROM Produce WINDOW item_window AS (PARTITION BY purchases ORDER BY purchases ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING)",
        "duckdb": "SELECT purchases, LAST_VALUE(item) OVER item_window AS most_popular FROM Produce WINDOW item_window AS (PARTITION BY purchases ORDER BY purchases NULLS FIRST ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING)",
        "mysql": "SELECT purchases, LAST_VALUE(item) OVER item_window AS most_popular FROM Produce WINDOW item_window AS (PARTITION BY purchases ORDER BY purchases ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING)",
        "oracle": "SELECT purchases, LAST_VALUE(item) OVER item_window AS most_popular FROM Produce WINDOW item_window AS (PARTITION BY purchases ORDER BY purchases NULLS FIRST ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING)",
        "postgres": "SELECT purchases, LAST_VALUE(item) OVER item_window AS most_popular FROM Produce WINDOW item_window AS (PARTITION BY purchases ORDER BY purchases NULLS FIRST ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING)",
        "presto": "SELECT purchases, LAST_VALUE(item) OVER (PARTITION BY purchases ORDER BY purchases NULLS FIRST ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS most_popular FROM Produce",
        "redshift": "SELECT purchases, LAST_VALUE(item) OVER (PARTITION BY purchases ORDER BY purchases NULLS FIRST ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS most_popular FROM Produce",
        "snowflake": "SELECT purchases, LAST_VALUE(item) OVER (PARTITION BY purchases ORDER BY purchases NULLS FIRST ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS most_popular FROM Produce",
        "spark": "SELECT purchases, LAST_VALUE(item) OVER item_window AS most_popular FROM Produce WINDOW item_window AS (PARTITION BY purchases ORDER BY purchases ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING)",
        "trino": "SELECT purchases, LAST_VALUE(item) OVER item_window AS most_popular FROM Produce WINDOW item_window AS (PARTITION BY purchases ORDER BY purchases NULLS FIRST ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING)",
        "tsql": "SELECT purchases, LAST_VALUE(item) OVER item_window AS most_popular FROM Produce WINDOW item_window AS (PARTITION BY purchases ORDER BY purchases ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING)"
      }
    },
    {
      "sql": "SELECT DATE(2024, 1, 15)",
      "read": {},
      "write": {
        "bigquery": "SELECT DATE(2024, 1, 15)",
        "duckdb": "SELECT MAKE_DATE(2024, 1, 15)"
      }
    },
    {
      "sql": "EXTRACT(HOUR FROM DATETIME(2008, 12, 25, 15, 30, 00))",
      "read": {},
      "write": {
        "bigquery": "EXTRACT(HOUR FROM DATETIME(2008, 12, 25, 15, 30, 00))",
        "duckdb": "EXTRACT(HOUR FROM MAKE_TIMESTAMP(2008, 12, 25, 15, 30, 00))",
        "snowflake": "DATE_PART(HOUR, TIMESTAMP_FROM_PARTS(2008, 12, 25, 15, 30, 00))"
      }
    },
    {
      "sql": "SELECT STRUCT(1, 2, 3), STRUCT(), STRUCT('abc'), STRUCT(1, t.str_col), STRUCT(1 as a, 'abc' AS b), STRUCT(str_col AS abc)",
      "read": {},
      "write": {
        "bigquery": "SELECT STRUCT(1, 2, 3), STRUCT(), STRUCT('abc'), STRUCT(1, t.str_col), STRUCT(1 AS a, 'abc' AS b), STRUCT(str_col AS abc)",
        "duckdb": "SELECT {'_0': 1, '_1': 2, '_2': 3}, {}, {'_0': 'abc'}, {'_0': 1, 'str_col': t.str_col}, {'a': 1, 'b': 'abc'}, {'abc': str_col}",
        "hive": "SELECT STRUCT(1, 2, 3), STRUCT(), STRUCT('abc'), STRUCT(1, t.str_col), STRUCT(1, 'abc'), STRUCT(str_col)",
        "spark2": "SELECT STRUCT(1, 2, 3), STRUCT(), STRUCT('abc'), STRUCT(1, t.str_col), STRUCT(1 AS a, 'abc' AS b), STRUCT(str_col AS abc)",
        "spark": "SELECT STRUCT(1, 2, 3), STRUCT(), STRUCT('abc'), STRUCT(1, t.str_col), STRUCT(1 AS a, 'abc' AS b), STRUCT(str_col AS abc)",
        "snowflake": "SELECT OBJECT_CONSTRUCT('_0', 1, '_1', 2, '_2', 3), OBJECT_CONSTRUCT(), OBJECT_CONSTRUCT('_0', 'abc'), OBJECT_CONSTRUCT('_0', 1, '_1', t.str_col), OBJECT_CONSTRUCT('a', 1, 'b', 'abc'), OBJECT_CONSTRUCT('abc', str_col)",
        "trino": "SELECT ROW(1, 2, 3), ROW(), ROW('abc'), ROW(1, t.str_col), CAST(ROW(1, 'abc') AS ROW(a INTEGER, b VARCHAR)), ROW(str_col)"
      }
    },
    {
      "sql": "PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%E6S%z', x)",
      "read": {},
      "write": {
        "bigquery": "PARSE_TIMESTAMP('%FT%H:%M:%E6S%z', x)",
        "duckdb": "STRPTIME(x, '%Y-%m-%dT%H:%M:%S.%f%z')"
      }
    },
    {
      "sql": "SELECT DATE_SUB(CURRENT_DATE(), INTERVAL 2 DAY)",
      "read": {},
      "write": {
        "bigquery": "SELECT DATE_SUB(CURRENT_DATE, INTERVAL '2' DAY)",
        "databricks": "SELECT DATE_ADD(CURRENT_DATE, -2)"
      }
    },
    {
      "sql": "SELECT DATE_SUB(DATE '2008-12-25', INTERVAL 5 DAY)",
      "read": {},
      "write": {
        "bigquery": "SELECT DATE_SUB(CAST('2008-12-25' AS DATE), INTERVAL '5' DAY)",
        "duckdb": "SELECT CAST('2008-12-25' AS DATE) - INTERVAL '5' DAY",
        "snowflake": "SELECT DATEADD(DAY, '5' * -1, CAST('2008-12-25' AS DATE))"
      }
    },
    {
      "sql": "EDIT_DISTANCE(col1, col2, max_distance => 3)",
      "read": {},
      "write": {
        "bigquery": "EDIT_DISTANCE(col1, col2, max_distance => 3)",
        "postgres": "LEVENSHTEIN_LESS_EQUAL(col1, col2, 3)",
        "snowflake": "EDITDISTANCE(col1, col2, 3)"
      }
    },
    {
      "sql": "EDIT_DISTANCE(a, b)",
      "read": {},
      "write": {
        "bigquery": "EDIT_DISTANCE(a, b)",
        "duckdb": "LEVENSHTEIN(a, b)"
      }
    },
    {
      "sql": "SAFE_CAST(some_date AS DATE FORMAT 'DD MONTH YYYY')",
      "read": {},
      "write": {
        "bigquery": "SAFE_CAST(some_date AS DATE FORMAT 'DD MONTH YYYY')",
        "duckdb": "CAST(TRY_STRPTIME(some_date, '%d %B %Y') AS DATE)"
      }
    },
    {
      "sql": "SAFE_CAST(some_date AS DATE FORMAT 'YYYY-MM-DD') AS some_date",
      "read": {},
      "write": {
        "bigquery": "SAFE_CAST(some_date AS DATE FORMAT 'YYYY-MM-DD') AS some_date",
        "duckdb": "CAST(TRY_STRPTIME(some_date, '%Y-%m-%d') AS DATE) AS some_date"
      }
    },
    {
      "sql": "SAFE_CAST(x AS TIMESTAMP)",
      "read": {},
      "write": {
        "bigquery": "SAFE_CAST(x AS TIMESTAMP)",
        "snowflake": "CAST(x AS TIMESTAMPTZ)"
      }
    },
    {
      "sql": "SELECT t.c1, h.c2, s.c3 FROM t1 AS t, UNNEST(t.t2) AS h, UNNEST(h.t3) AS s",
      "read": {},
      "write": {
        "bigquery": "SELECT t.c1, h.c2, s.c3 FROM t1 AS t CROSS JOIN UNNEST(t.t2) AS h CROSS JOIN UNNEST(h.t3) AS s",
        "duckdb": "SELECT t.c1, h.c2, s.c3 FROM t1 AS t CROSS JOIN UNNEST(t.t2) AS _t0(h) CROSS JOIN UNNEST(h.t3) AS _t1(s)"
      }
    },
    {
      "sql": "PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%E6S%z', x)",
      "read": {},
      "write": {
        "bigquery": "PARSE_TIMESTAMP('%FT%H:%M:%E6S%z', x)",
        "duckdb": "STRPTIME(x, '%Y-%m-%dT%H:%M:%S.%f%z')"
      }
    },
    {
      "sql": "SELECT results FROM Coordinates, Coordinates.position AS results",
      "read": {},
      "write": {
        "bigquery": "SELECT results FROM Coordinates CROSS JOIN UNNEST(Coordinates.position) AS results",
        "presto": "SELECT results FROM Coordinates CROSS JOIN UNNEST(Coordinates.position) AS _t0(results)"
      }
    },
    {
      "sql": "SELECT results FROM Coordinates, `Coordinates.position` AS results",
      "read": {},
      "write": {
        "bigquery": "SELECT results FROM Coordinates CROSS JOIN `Coordinates.position` AS results",
        "presto": "SELECT results FROM Coordinates CROSS JOIN \"Coordinates\".\"position\" AS results"
      }
    },
    {
      "sql": "SELECT results FROM Coordinates AS c, UNNEST(c.position) AS results",
      "read": {
        "presto": "SELECT results FROM Coordinates AS c, UNNEST(c.position) AS _t(results)",
        "redshift": "SELECT results FROM Coordinates AS c, c.position AS results"
      },
      "write": {
        "bigquery": "SELECT results FROM Coordinates AS c CROSS JOIN UNNEST(c.position) AS results",
        "presto": "SELECT results FROM Coordinates AS c CROSS JOIN UNNEST(c.position) AS _t0(results)",
        "redshift": "SELECT results FROM Coordinates AS c CROSS JOIN c.position AS results"
      }
    },
    {
      "sql": "TIMESTAMP(x)",
      "read": {},
      "write": {
        "bigquery": "TIMESTAMP(x)",
        "duckdb": "CAST(x AS TIMESTAMPTZ)",
        "snowflake": "CAST(x AS TIMESTAMPTZ)",
        "presto": "CAST(x AS TIMESTAMP WITH TIME ZONE)"
      }
    },
    {
      "sql": "SELECT TIMESTAMP('2008-12-25 15:30:00', 'America/Los_Angeles')",
      "read": {},
      "write": {
        "bigquery": "SELECT TIMESTAMP('2008-12-25 15:30:00', 'America/Los_Angeles')",
        "duckdb": "SELECT CAST('2008-12-25 15:30:00' AS TIMESTAMP) AT TIME ZONE 'America/Los_Angeles'",
        "snowflake": "SELECT CONVERT_TIMEZONE('America/Los_Angeles', CAST('2008-12-25 15:30:00' AS TIMESTAMP))"
      }
    },
    {
      "sql": "SELECT SUM(x IGNORE NULLS) AS x",
      "read": {
        "bigquery": "SELECT SUM(x IGNORE NULLS) AS x",
        "duckdb": "SELECT SUM(x IGNORE NULLS) AS x",
        "spark": "SELECT SUM(x) IGNORE NULLS AS x",
        "snowflake": "SELECT SUM(x) IGNORE NULLS AS x"
      },
      "write": {
        "bigquery": "SELECT SUM(x IGNORE NULLS) AS x",
        "duckdb": "SELECT SUM(x) AS x",
        "spark": "SELECT SUM(x) IGNORE NULLS AS x",
        "snowflake": "SELECT SUM(x) IGNORE NULLS AS x"
      }
    },
    {
      "sql": "SELECT SUM(x RESPECT NULLS) AS x",
      "read": {
        "bigquery": "SELECT SUM(x RESPECT NULLS) AS x",
        "spark": "SELECT SUM(x) RESPECT NULLS AS x",
        "snowflake": "SELECT SUM(x) RESPECT NULLS AS x"
      },
      "write": {
        "bigquery": "SELECT SUM(x RESPECT NULLS) AS x",
        "duckdb": "SELECT SUM(x) AS x",
        "spark": "SELECT SUM(x) RESPECT NULLS AS x",
        "snowflake": "SELECT SUM(x) RESPECT NULLS AS x"
      }
    },
    {
      "sql": "SELECT PERCENTILE_CONT(x, 0.5 RESPECT NULLS) OVER ()",
      "read": {},
      "write": {
        "bigquery": "SELECT PERCENTILE_CONT(x, 0.5 RESPECT NULLS) OVER ()",
        "duckdb": "SELECT QUANTILE_CONT(x, 0.5) OVER ()",
        "spark": "SELECT PERCENTILE_CONT(x, 0.5) RESPECT NULLS OVER ()"
      }
    },
    {
      "sql": "SELECT ARRAY_AGG(DISTINCT x IGNORE NULLS ORDER BY a, b DESC LIMIT 10) AS x",
      "read": {},
      "write": {
        "bigquery": "SELECT ARRAY_AGG(DISTINCT x IGNORE NULLS ORDER BY a, b DESC LIMIT 10) AS x",
        "duckdb": "SELECT ARRAY_AGG(DISTINCT x ORDER BY a NULLS FIRST, b DESC LIMIT 10) AS x",
        "spark": "SELECT COLLECT_LIST(DISTINCT x ORDER BY a, b DESC LIMIT 10) IGNORE NULLS AS x"
      }
    },
    {
      "sql": "SELECT ARRAY_AGG(DISTINCT x IGNORE NULLS ORDER BY a, b DESC LIMIT 1, 10) AS x",
      "read": {},
      "write": {
        "bigquery": "SELECT ARRAY_AGG(DISTINCT x IGNORE NULLS ORDER BY a, b DESC LIMIT 1, 10) AS x",
        "duckdb": "SELECT ARRAY_AGG(DISTINCT x ORDER BY a NULLS FIRST, b DESC LIMIT 1, 10) AS x",
        "spark": "SELECT COLLECT_LIST(DISTINCT x ORDER BY a, b DESC LIMIT 1, 10) IGNORE NULLS AS x"
      }
    },
    {
      "sql": "SELECT * FROM Produce UNPIVOT((first_half_sales, second_half_sales) FOR semesters IN ((Q1, Q2) AS 'semester_1', (Q3, Q4) AS 'semester_2'))",
      "read": {
        "spark": "SELECT * FROM Produce UNPIVOT((first_half_sales, second_half_sales) FOR semesters IN ((Q1, Q2) AS semester_1, (Q3, Q4) AS semester_2))"
      },
      "write": {
        "bigquery": "SELECT * FROM Produce UNPIVOT((first_half_sales, second_half_sales) FOR semesters IN ((Q1, Q2) AS 'semester_1', (Q3, Q4) AS 'semester_2'))",
        "spark": "SELECT * FROM Produce UNPIVOT((first_half_sales, second_half_sales) FOR semesters IN ((Q1, Q2) AS semester_1, (Q3, Q4) AS semester_2))"
      }
    },
    {
      "sql": "SELECT * FROM Produce UNPIVOT((first_half_sales, second_half_sales) FOR semesters IN ((Q1, Q2) AS 1, (Q3, Q4) AS 2))",
      "read": {},
      "write": {
        "bigquery": "SELECT * FROM Produce UNPIVOT((first_half_sales, second_half_sales) FOR semesters IN ((Q1, Q2) AS 1, (Q3, Q4) AS 2))",
        "spark": "SELECT * FROM Produce UNPIVOT((first_half_sales, second_half_sales) FOR semesters IN ((Q1, Q2) AS `1`, (Q3, Q4) AS `2`))"
      }
    },
    {
      "sql": "SELECT UNIX_DATE(DATE '2008-12-25')",
      "read": {},
      "write": {
        "bigquery": "SELECT UNIX_DATE(CAST('2008-12-25' AS DATE))",
        "duckdb": "SELECT DATE_DIFF('DAY', CAST('1970-01-01' AS DATE), CAST('2008-12-25' AS DATE))"
      }
    },
    {
      "sql": "SELECT LAST_DAY(CAST('2008-11-25' AS DATE), MONTH)",
      "read": {
        "snowflake": "SELECT LAST_DAY(CAST('2008-11-25' AS DATE), MONS)"
      },
      "write": {
        "bigquery": "SELECT LAST_DAY(CAST('2008-11-25' AS DATE), MONTH)",
        "duckdb": "SELECT LAST_DAY(CAST('2008-11-25' AS DATE))",
        "clickhouse": "SELECT LAST_DAY(CAST('2008-11-25' AS Nullable(DATE)))",
        "mysql": "SELECT LAST_DAY(CAST('2008-11-25' AS DATE))",
        "oracle": "SELECT LAST_DAY(CAST('2008-11-25' AS DATE))",
        "postgres": "SELECT CAST(DATE_TRUNC('MONTH', CAST('2008-11-25' AS DATE)) + INTERVAL '1 MONTH' - INTERVAL '1 DAY' AS DATE)",
        "presto": "SELECT LAST_DAY_OF_MONTH(CAST('2008-11-25' AS DATE))",
        "redshift": "SELECT LAST_DAY(CAST('2008-11-25' AS DATE))",
        "snowflake": "SELECT LAST_DAY(CAST('2008-11-25' AS DATE), MONTH)",
        "spark": "SELECT LAST_DAY(CAST('2008-11-25' AS DATE))",
        "tsql": "SELECT EOMONTH(CAST('2008-11-25' AS DATE))"
      }
    },
    {
      "sql": "SELECT LAST_DAY(CAST('2008-11-25' AS DATE), QUARTER)",
      "read": {
        "snowflake": "SELECT LAST_DAY(CAST('2008-11-25' AS DATE), QUARTER)"
      },
      "write": {
        "bigquery": "SELECT LAST_DAY(CAST('2008-11-25' AS DATE), QUARTER)",
        "snowflake": "SELECT LAST_DAY(CAST('2008-11-25' AS DATE), QUARTER)"
      }
    },
    {
      "sql": "SELECT TIME(15, 30, 00)",
      "read": {
        "duckdb": "SELECT MAKE_TIME(15, 30, 00)",
        "mysql": "SELECT MAKETIME(15, 30, 00)",
        "postgres": "SELECT MAKE_TIME(15, 30, 00)",
        "snowflake": "SELECT TIME_FROM_PARTS(15, 30, 00)"
      },
      "write": {
        "bigquery": "SELECT TIME(15, 30, 00)",
        "duckdb": "SELECT MAKE_TIME(15, 30, 00)",
        "mysql": "SELECT MAKETIME(15, 30, 00)",
        "postgres": "SELECT MAKE_TIME(15, 30, 00)",
        "snowflake": "SELECT TIME_FROM_PARTS(15, 30, 00)",
        "tsql": "SELECT TIMEFROMPARTS(15, 30, 00, 0, 0)"
      }
    },
    {
      "sql": "SELECT TIME('2008-12-25 15:30:00')",
      "read": {},
      "write": {
        "bigquery": "SELECT TIME('2008-12-25 15:30:00')",
        "duckdb": "SELECT CAST('2008-12-25 15:30:00' AS TIME)",
        "mysql": "SELECT CAST('2008-12-25 15:30:00' AS TIME)",
        "postgres": "SELECT CAST('2008-12-25 15:30:00' AS TIME)",
        "redshift": "SELECT CAST('2008-12-25 15:30:00' AS TIME)",
        "spark": "SELECT CAST('2008-12-25 15:30:00' AS TIMESTAMP)",
        "tsql": "SELECT CAST('2008-12-25 15:30:00' AS TIME)"
      }
    },
    {
      "sql": "SELECT COUNTIF(x)",
      "read": {
        "clickhouse": "SELECT countIf(x)",
        "duckdb": "SELECT COUNT_IF(x)"
      },
      "write": {
        "bigquery": "SELECT COUNTIF(x)",
        "clickhouse": "SELECT countIf(x)",
        "duckdb": "SELECT COUNT_IF(x)"
      }
    },
    {
      "sql": "SELECT TIMESTAMP_DIFF(TIMESTAMP_SECONDS(60), TIMESTAMP_SECONDS(0), minute)",
      "read": {},
      "write": {
        "bigquery": "SELECT TIMESTAMP_DIFF(TIMESTAMP_SECONDS(60), TIMESTAMP_SECONDS(0), MINUTE)",
        "databricks": "SELECT TIMESTAMPDIFF(MINUTE, CAST(FROM_UNIXTIME(0) AS TIMESTAMP), CAST(FROM_UNIXTIME(60) AS TIMESTAMP))",
        "duckdb": "SELECT DATE_DIFF('MINUTE', TO_TIMESTAMP(0), TO_TIMESTAMP(60))",
        "snowflake": "SELECT TIMESTAMPDIFF(MINUTE, TO_TIMESTAMP(0), TO_TIMESTAMP(60))"
      }
    },
    {
      "sql": "TIMESTAMP_DIFF(a, b, MONTH)",
      "read": {
        "bigquery": "TIMESTAMP_DIFF(a, b, month)",
        "databricks": "TIMESTAMPDIFF(month, b, a)",
        "mysql": "TIMESTAMPDIFF(month, b, a)"
      },
      "write": {
        "databricks": "TIMESTAMPDIFF(MONTH, b, a)",
        "mysql": "TIMESTAMPDIFF(MONTH, b, a)",
        "snowflake": "TIMESTAMPDIFF(MONTH, b, a)"
      }
    },
    {
      "sql": "SELECT TIMESTAMP_MICROS(x)",
      "read": {
        "duckdb": "SELECT MAKE_TIMESTAMP(x)",
        "spark": "SELECT TIMESTAMP_MICROS(x)"
      },
      "write": {
        "bigquery": "SELECT TIMESTAMP_MICROS(x)",
        "duckdb": "SELECT MAKE_TIMESTAMP(x)",
        "snowflake": "SELECT TO_TIMESTAMP(x, 6)",
        "spark": "SELECT TIMESTAMP_MICROS(x)"
      }
    },
    {
      "sql": "SELECT * FROM t WHERE EXISTS(SELECT * FROM unnest(nums) AS x WHERE x > 1)",
      "read": {},
      "write": {
        "bigquery": "SELECT * FROM t WHERE EXISTS(SELECT * FROM UNNEST(nums) AS x WHERE x > 1)",
        "duckdb": "SELECT * FROM t WHERE EXISTS(SELECT * FROM UNNEST(nums) AS _t0(x) WHERE x > 1)"
      }
    },
    {
      "sql": "NULL",
      "read": {
        "duckdb": "NULL = a",
        "postgres": "a = NULL"
      },
      "write": {}
    },
    {
      "sql": "SELECT '\\n'",
      "read": {
        "bigquery": "SELECT '''\n'''"
      },
      "write": {
        "bigquery": "SELECT '\\n'",
        "postgres": "SELECT '\n'"
      }
    },
    {
      "sql": "TRIM(item, '*')",
      "read": {
        "snowflake": "TRIM(item, '*')",
        "spark": "TRIM('*', item)"
      },
      "write": {
        "bigquery": "TRIM(item, '*')",
        "snowflake": "TRIM(item, '*')",
        "spark": "TRIM('*' FROM item)"
      }
    },
    {
      "sql": "CREATE OR REPLACE TABLE `a.b.c` COPY `a.b.d`",
      "read": {},
      "write": {
        "bigquery": "CREATE OR REPLACE TABLE `a.b.c` COPY `a.b.d`",
        "snowflake": "CREATE OR REPLACE TABLE \"a\".\"b\".\"c\" CLONE \"a\".\"b\".\"d\""
      }
    },
    {
      "sql": "LEAST(x, y)",
      "read": {
        "sqlite": "MIN(x, y)"
      },
      "write": {}
    },
    {
      "sql": "SELECT TIMESTAMP_ADD(TIMESTAMP \"2008-12-25 15:30:00+00\", INTERVAL 10 MINUTE)",
      "read": {},
      "write": {
        "bigquery": "SELECT TIMESTAMP_ADD(CAST('2008-12-25 15:30:00+00' AS TIMESTAMP), INTERVAL '10' MINUTE)",
        "databricks": "SELECT DATE_ADD(MINUTE, '10', CAST('2008-12-25 15:30:00+00' AS TIMESTAMP))",
        "duckdb": "SELECT CAST('2008-12-25 15:30:00+00' AS TIMESTAMPTZ) + INTERVAL '10' MINUTE",
        "mysql": "SELECT DATE_ADD(TIMESTAMP('2008-12-25 15:30:00+00'), INTERVAL '10' MINUTE)",
        "spark": "SELECT DATE_ADD(MINUTE, '10', CAST('2008-12-25 15:30:00+00' AS TIMESTAMP))",
        "snowflake": "SELECT TIMESTAMPADD(MINUTE, '10', CAST('2008-12-25 15:30:00+00' AS TIMESTAMPTZ))"
      }
    },
    {
      "sql": "SELECT TIMESTAMP_SUB(TIMESTAMP \"2008-12-25 15:30:00+00\", INTERVAL 10 MINUTE)",
      "read": {},
      "write": {
        "bigquery": "SELECT TIMESTAMP_SUB(CAST('2008-12-25 15:30:00+00' AS TIMESTAMP), INTERVAL '10' MINUTE)",
        "duckdb": "SELECT CAST('2008-12-25 15:30:00+00' AS TIMESTAMPTZ) - INTERVAL '10' MINUTE",
        "mysql": "SELECT DATE_SUB(TIMESTAMP('2008-12-25 15:30:00+00'), INTERVAL '10' MINUTE)",
        "snowflake": "SELECT TIMESTAMPADD(MINUTE, '10' * -1, CAST('2008-12-25 15:30:00+00' AS TIMESTAMPTZ))",
        "spark": "SELECT CAST('2008-12-25 15:30:00+00' AS TIMESTAMP) - INTERVAL '10' MINUTE"
      }
    },
    {
      "sql": "SELECT TIMESTAMP_SUB(TIMESTAMP \"2008-12-25 15:30:00+00\", INTERVAL col MINUTE)",
      "read": {},
      "write": {
        "bigquery": "SELECT TIMESTAMP_SUB(CAST('2008-12-25 15:30:00+00' AS TIMESTAMP), INTERVAL col MINUTE)",
        "snowflake": "SELECT TIMESTAMPADD(MINUTE, col * -1, CAST('2008-12-25 15:30:00+00' AS TIMESTAMPTZ))"
      }
    },
    {
      "sql": "SELECT TIME_ADD(CAST('09:05:03' AS TIME), INTERVAL 2 HOUR)",
      "read": {},
      "write": {
        "bigquery": "SELECT TIME_ADD(CAST('09:05:03' AS TIME), INTERVAL '2' HOUR)",
        "duckdb": "SELECT CAST('09:05:03' AS TIME) + INTERVAL '2' HOUR"
      }
    },
    {
      "sql": "SELECT TIME_SUB(CAST('09:05:03' AS TIME), INTERVAL 2 HOUR)",
      "read": {},
      "write": {
        "bigquery": "SELECT TIME_SUB(CAST('09:05:03' AS TIME), INTERVAL '2' HOUR)",
        "duckdb": "SELECT CAST('09:05:03' AS TIME) - INTERVAL '2' HOUR"
      }
    },
    {
      "sql": "LOWER(TO_HEX(x))",
      "read": {},
      "write": {
        "bigquery": "TO_HEX(x)",
        "clickhouse": "LOWER(HEX(x))",
        "duckdb": "LOWER(HEX(x))",
        "hive": "LOWER(HEX(x))",
        "mysql": "LOWER(HEX(x))",
        "spark": "LOWER(HEX(x))",
        "sqlite": "LOWER(HEX(x))",
        "presto": "LOWER(TO_HEX(x))",
        "trino": "LOWER(TO_HEX(x))"
      }
    },
    {
      "sql": "TO_HEX(x)",
      "read": {
        "clickhouse": "LOWER(HEX(x))",
        "duckdb": "LOWER(HEX(x))",
        "hive": "LOWER(HEX(x))",
        "mysql": "LOWER(HEX(x))",
        "spark": "LOWER(HEX(x))",
        "sqlite": "LOWER(HEX(x))",
        "presto": "LOWER(TO_HEX(x))",
        "trino": "LOWER(TO_HEX(x))"
      },
      "write": {
        "bigquery": "TO_HEX(x)",
        "clickhouse": "LOWER(HEX(x))",
        "duckdb": "LOWER(HEX(x))",
        "hive": "LOWER(HEX(x))",
        "mysql": "LOWER(HEX(x))",
        "presto": "LOWER(TO_HEX(x))",
        "spark": "LOWER(HEX(x))",
        "sqlite": "LOWER(HEX(x))",
        "trino": "LOWER(TO_HEX(x))"
      }
    },
    {
      "sql": "UPPER(TO_HEX(x))",
      "read": {
        "clickhouse": "HEX(x)",
        "duckdb": "HEX(x)",
        "hive": "HEX(x)",
        "mysql": "HEX(x)",
        "presto": "TO_HEX(x)",
        "spark": "HEX(x)",
        "sqlite": "HEX(x)",
        "trino": "TO_HEX(x)"
      },
      "write": {
        "bigquery": "UPPER(TO_HEX(x))",
        "clickhouse": "HEX(x)",
        "duckdb": "HEX(x)",
        "hive": "HEX(x)",
        "mysql": "HEX(x)",
        "presto": "TO_HEX(x)",
        "spark": "HEX(x)",
        "sqlite": "HEX(x)",
        "trino": "TO_HEX(x)"
      }
    },
    {
      "sql": "MD5(x)",
      "read": {
        "clickhouse": "MD5(x)",
        "presto": "MD5(x)",
        "trino": "MD5(x)"
      },
      "write": {
        "bigquery": "MD5(x)",
        "clickhouse": "MD5(x)",
        "hive": "UNHEX(MD5(x))",
        "presto": "MD5(x)",
        "spark": "UNHEX(MD5(x))",
        "trino": "MD5(x)"
      }
    },
    {
      "sql": "SELECT TO_HEX(MD5(some_string))",
      "read": {
        "duckdb": "SELECT MD5(some_string)",
        "spark": "SELECT MD5(some_string)",
        "clickhouse": "SELECT LOWER(HEX(MD5(some_string)))",
        "presto": "SELECT LOWER(TO_HEX(MD5(some_string)))",
        "trino": "SELECT LOWER(TO_HEX(MD5(some_string)))"
      },
      "write": {
        "bigquery": "SELECT TO_HEX(MD5(some_string))",
        "duckdb": "SELECT MD5(some_string)",
        "clickhouse": "SELECT LOWER(HEX(MD5(some_string)))",
        "presto": "SELECT LOWER(TO_HEX(MD5(some_string)))",
        "trino": "SELECT LOWER(TO_HEX(MD5(some_string)))"
      }
    },
    {
      "sql": "SHA1(x)",
      "read": {
        "bigquery": "SHA1(x)",
        "clickhouse": "SHA1(x)",
        "presto": "SHA1(x)",
        "trino": "SHA1(x)"
      },
      "write": {
        "clickhouse": "SHA1(x)",
        "bigquery": "SHA1(x)",
        "presto": "SHA1(x)",
        "trino": "SHA1(x)",
        "duckdb": "UNHEX(SHA1(x))"
      }
    },
    {
      "sql": "SHA256(x)",
      "read": {
        "clickhouse": "SHA256(x)",
        "presto": "SHA256(x)",
        "trino": "SHA256(x)",
        "postgres": "SHA256(x)",
        "duckdb": "SHA256(x)"
      },
      "write": {
        "bigquery": "SHA256(x)",
        "spark2": "SHA2(x, 256)",
        "clickhouse": "SHA256(x)",
        "postgres": "SHA256(x)",
        "presto": "SHA256(x)",
        "redshift": "SHA2(x, 256)",
        "trino": "SHA256(x)",
        "duckdb": "UNHEX(SHA256(x))",
        "snowflake": "SHA2_BINARY(x, 256)"
      }
    },
    {
      "sql": "SHA512(x)",
      "read": {
        "clickhouse": "SHA512(x)",
        "presto": "SHA512(x)",
        "trino": "SHA512(x)"
      },
      "write": {
        "clickhouse": "SHA512(x)",
        "bigquery": "SHA512(x)",
        "spark2": "SHA2(x, 512)",
        "presto": "SHA512(x)",
        "trino": "SHA512(x)"
      }
    },
    {
      "sql": "SELECT CAST('20201225' AS TIMESTAMP FORMAT 'YYYYMMDD' AT TIME ZONE 'America/New_York')",
      "read": {},
      "write": {
        "bigquery": "SELECT PARSE_TIMESTAMP('%Y%m%d', '20201225', 'America/New_York')"
      }
    },
    {
      "sql": "SELECT CAST('20201225' AS TIMESTAMP FORMAT 'YYYYMMDD')",
      "read": {},
      "write": {
        "bigquery": "SELECT PARSE_TIMESTAMP('%Y%m%d', '20201225')"
      }
    },
    {
      "sql": "SELECT CAST(TIMESTAMP '2008-12-25 00:00:00+00:00' AS STRING FORMAT 'YYYY-MM-DD HH24:MI:SS TZH:TZM') AS date_time_to_string",
      "read": {},
      "write": {
        "bigquery": "SELECT CAST(CAST('2008-12-25 00:00:00+00:00' AS TIMESTAMP) AS STRING FORMAT 'YYYY-MM-DD HH24:MI:SS TZH:TZM') AS date_time_to_string"
      }
    },
    {
      "sql": "SELECT CAST(TIMESTAMP '2008-12-25 00:00:00+00:00' AS STRING FORMAT 'YYYY-MM-DD HH24:MI:SS TZH:TZM' AT TIME ZONE 'Asia/Kolkata') AS date_time_to_string",
      "read": {},
      "write": {
        "bigquery": "SELECT CAST(CAST('2008-12-25 00:00:00+00:00' AS TIMESTAMP) AS STRING FORMAT 'YYYY-MM-DD HH24:MI:SS TZH:TZM' AT TIME ZONE 'Asia/Kolkata') AS date_time_to_string"
      }
    },
    {
      "sql": "WITH cte AS (SELECT [1, 2, 3] AS arr) SELECT IF(pos = pos_2, col, NULL) AS col FROM cte CROSS JOIN UNNEST(GENERATE_ARRAY(0, GREATEST(ARRAY_LENGTH(arr)) - 1)) AS pos CROSS JOIN UNNEST(arr) AS col WITH OFFSET AS pos_2 WHERE pos = pos_2 OR (pos > (ARRAY_LENGTH(arr) - 1) AND pos_2 = (ARRAY_LENGTH(arr) - 1))",
      "read": {
        "spark": "WITH cte AS (SELECT ARRAY(1, 2, 3) AS arr) SELECT EXPLODE(arr) FROM cte"
      },
      "write": {}
    },
    {
      "sql": "SELECT IF(pos = pos_2, col, NULL) AS col FROM UNNEST(GENERATE_ARRAY(0, GREATEST(ARRAY_LENGTH(IF(ARRAY_LENGTH(COALESCE([], [])) = 0, [[][SAFE_ORDINAL(0)]], []))) - 1)) AS pos CROSS JOIN UNNEST(IF(ARRAY_LENGTH(COALESCE([], [])) = 0, [[][SAFE_ORDINAL(0)]], [])) AS col WITH OFFSET AS pos_2 WHERE pos = pos_2 OR (pos > (ARRAY_LENGTH(IF(ARRAY_LENGTH(COALESCE([], [])) = 0, [[][SAFE_ORDINAL(0)]], [])) - 1) AND pos_2 = (ARRAY_LENGTH(IF(ARRAY_LENGTH(COALESCE([], [])) = 0, [[][SAFE_ORDINAL(0)]], [])) - 1))",
      "read": {
        "spark": "select explode_outer([])"
      },
      "write": {}
    },
    {
      "sql": "SELECT IF(pos = pos_2, col, NULL) AS col, IF(pos = pos_2, pos_2, NULL) AS pos_2 FROM UNNEST(GENERATE_ARRAY(0, GREATEST(ARRAY_LENGTH(IF(ARRAY_LENGTH(COALESCE([], [])) = 0, [[][SAFE_ORDINAL(0)]], []))) - 1)) AS pos CROSS JOIN UNNEST(IF(ARRAY_LENGTH(COALESCE([], [])) = 0, [[][SAFE_ORDINAL(0)]], [])) AS col WITH OFFSET AS pos_2 WHERE pos = pos_2 OR (pos > (ARRAY_LENGTH(IF(ARRAY_LENGTH(COALESCE([], [])) = 0, [[][SAFE_ORDINAL(0)]], [])) - 1) AND pos_2 = (ARRAY_LENGTH(IF(ARRAY_LENGTH(COALESCE([], [])) = 0, [[][SAFE_ORDINAL(0)]], [])) - 1))",
      "read": {
        "spark": "select posexplode_outer([])"
      },
      "write": {}
    },
    {
      "sql": "SELECT AS STRUCT ARRAY(SELECT AS STRUCT 1 AS b FROM x) AS y FROM z",
      "read": {},
      "write": {
        "bigquery": "SELECT AS STRUCT ARRAY(SELECT AS STRUCT 1 AS b FROM x) AS y FROM z",
        "duckdb": "SELECT {'y': ARRAY(SELECT {'b': 1} FROM x)} FROM z"
      }
    },
    {
      "sql": "SELECT CAST(STRUCT(1) AS STRUCT<INT64>)",
      "read": {},
      "write": {
        "bigquery": "SELECT CAST(STRUCT(1) AS STRUCT<INT64>)",
        "snowflake": "SELECT CAST(OBJECT_CONSTRUCT('_0', 1) AS OBJECT)"
      }
    },
    {
      "sql": "cast(x as date format 'MM/DD/YYYY')",
      "read": {},
      "write": {
        "bigquery": "PARSE_DATE('%m/%d/%Y', x)"
      }
    },
    {
      "sql": "cast(x as time format 'YYYY.MM.DD HH:MI:SSTZH')",
      "read": {},
      "write": {
        "bigquery": "PARSE_TIMESTAMP('%Y.%m.%d %I:%M:%S%z', x)"
      }
    },
    {
      "sql": "REGEXP_CONTAINS('foo', '.*')",
      "read": {
        "bigquery": "REGEXP_CONTAINS('foo', '.*')",
        "mysql": "REGEXP_LIKE('foo', '.*')",
        "starrocks": "REGEXP('foo', '.*')"
      },
      "write": {
        "mysql": "REGEXP_LIKE('foo', '.*')",
        "starrocks": "REGEXP('foo', '.*')"
      }
    },
    {
      "sql": "\"\"\"x\"\"\"",
      "read": {},
      "write": {
        "bigquery": "'x'",
        "duckdb": "'x'",
        "presto": "'x'",
        "hive": "'x'",
        "spark": "'x'"
      }
    },
    {
      "sql": "\"\"\"x'\"\"\"",
      "read": {},
      "write": {
        "bigquery": "'x\\''",
        "duckdb": "'x'''",
        "presto": "'x'''",
        "hive": "'x\\''",
        "spark": "'x\\''"
      }
    },
    {
      "sql": "r'x\\''",
      "read": {},
      "write": {
        "bigquery": "'x\\''",
        "hive": "'x\\''"
      }
    },
    {
      "sql": "r'x\\y'",
      "read": {},
      "write": {
        "bigquery": "'x\\\\y'",
        "hive": "'x\\\\y'"
      }
    },
    {
      "sql": "'\\\\'",
      "read": {},
      "write": {
        "bigquery": "'\\\\'",
        "duckdb": "'\\'",
        "presto": "'\\'",
        "hive": "'\\\\'"
      }
    },
    {
      "sql": "r\"\"\"/\\*.*\\*/\"\"\"",
      "read": {},
      "write": {
        "bigquery": "'/\\\\*.*\\\\*/'",
        "duckdb": "'/\\*.*\\*/'",
        "presto": "'/\\*.*\\*/'",
        "hive": "'/\\\\*.*\\\\*/'",
        "spark": "'/\\\\*.*\\\\*/'"
      }
    },
    {
      "sql": "R\"\"\"/\\*.*\\*/\"\"\"",
      "read": {},
      "write": {
        "bigquery": "'/\\\\*.*\\\\*/'",
        "duckdb": "'/\\*.*\\*/'",
        "presto": "'/\\*.*\\*/'",
        "hive": "'/\\\\*.*\\\\*/'",
        "spark": "'/\\\\*.*\\\\*/'"
      }
    },
    {
      "sql": "r\"\"\"a\n\"\"\"",
      "read": {},
      "write": {
        "bigquery": "'a\\n'",
        "duckdb": "'a\n'"
      }
    },
    {
      "sql": "\"\"\"a\n\"\"\"",
      "read": {},
      "write": {
        "bigquery": "'a\\n'",
        "duckdb": "'a\n'"
      }
    },
    {
      "sql": "CAST(a AS INT64)",
      "read": {},
      "write": {
        "bigquery": "CAST(a AS INT64)",
        "duckdb": "CAST(a AS BIGINT)",
        "presto": "CAST(a AS BIGINT)",
        "hive": "CAST(a AS BIGINT)",
        "spark": "CAST(a AS BIGINT)"
      }
    },
    {
      "sql": "CAST(a AS BYTES)",
      "read": {},
      "write": {
        "bigquery": "CAST(a AS BYTES)",
        "duckdb": "CAST(a AS BLOB)",
        "presto": "CAST(a AS VARBINARY)",
        "hive": "CAST(a AS BINARY)",
        "spark": "CAST(a AS BINARY)"
      }
    },
    {
      "sql": "CAST(a AS NUMERIC)",
      "read": {},
      "write": {
        "bigquery": "CAST(a AS NUMERIC)",
        "duckdb": "CAST(a AS DECIMAL)",
        "presto": "CAST(a AS DECIMAL)",
        "hive": "CAST(a AS DECIMAL)",
        "spark": "CAST(a AS DECIMAL)"
      }
    },
    {
      "sql": "[1, 2, 3]",
      "read": {
        "duckdb": "[1, 2, 3]",
        "presto": "ARRAY[1, 2, 3]",
        "hive": "ARRAY(1, 2, 3)",
        "spark": "ARRAY(1, 2, 3)"
      },
      "write": {
        "bigquery": "[1, 2, 3]",
        "duckdb": "[1, 2, 3]",
        "presto": "ARRAY[1, 2, 3]",
        "hive": "ARRAY(1, 2, 3)",
        "spark": "ARRAY(1, 2, 3)"
      }
    },
    {
      "sql": "SELECT * FROM UNNEST(['7', '14']) AS x",
      "read": {
        "spark": "SELECT * FROM UNNEST(ARRAY('7', '14')) AS (x)"
      },
      "write": {
        "bigquery": "SELECT * FROM UNNEST(['7', '14']) AS x",
        "presto": "SELECT * FROM UNNEST(ARRAY['7', '14']) AS _t0(x)",
        "spark": "SELECT * FROM EXPLODE(ARRAY('7', '14')) AS _t0(x)"
      }
    },
    {
      "sql": "SELECT ARRAY(SELECT x FROM UNNEST([0, 1]) AS x)",
      "read": {},
      "write": {
        "bigquery": "SELECT ARRAY(SELECT x FROM UNNEST([0, 1]) AS x)"
      }
    },
    {
      "sql": "SELECT ARRAY(SELECT DISTINCT x FROM UNNEST(some_numbers) AS x) AS unique_numbers",
      "read": {},
      "write": {
        "bigquery": "SELECT ARRAY(SELECT DISTINCT x FROM UNNEST(some_numbers) AS x) AS unique_numbers"
      }
    },
    {
      "sql": "SELECT ARRAY(SELECT * FROM foo JOIN bla ON x = y)",
      "read": {},
      "write": {
        "bigquery": "SELECT ARRAY(SELECT * FROM foo JOIN bla ON x = y)"
      }
    },
    {
      "sql": "CURRENT_TIMESTAMP()",
      "read": {
        "tsql": "GETDATE()"
      },
      "write": {
        "tsql": "GETDATE()"
      }
    },
    {
      "sql": "current_datetime",
      "read": {},
      "write": {
        "bigquery": "CURRENT_DATETIME()",
        "presto": "CURRENT_DATETIME()",
        "hive": "CURRENT_DATETIME()",
        "spark": "CURRENT_DATETIME()"
      }
    },
    {
      "sql": "current_time",
      "read": {},
      "write": {
        "bigquery": "CURRENT_TIME()",
        "duckdb": "CURRENT_TIME",
        "presto": "CURRENT_TIME",
        "trino": "CURRENT_TIME",
        "hive": "CURRENT_TIME()",
        "spark": "CURRENT_TIME()"
      }
    },
    {
      "sql": "CURRENT_TIMESTAMP",
      "read": {},
      "write": {
        "bigquery": "CURRENT_TIMESTAMP()",
        "duckdb": "CURRENT_TIMESTAMP",
        "postgres": "CURRENT_TIMESTAMP",
        "presto": "CURRENT_TIMESTAMP",
        "hive": "CURRENT_TIMESTAMP()",
        "spark": "CURRENT_TIMESTAMP()"
      }
    },
    {
      "sql": "CURRENT_TIMESTAMP()",
      "read": {},
      "write": {
        "bigquery": "CURRENT_TIMESTAMP()",
        "duckdb": "CURRENT_TIMESTAMP",
        "postgres": "CURRENT_TIMESTAMP",
        "presto": "CURRENT_TIMESTAMP",
        "hive": "CURRENT_TIMESTAMP()",
        "spark": "CURRENT_TIMESTAMP()"
      }
    },
    {
      "sql": "DIV(x, y)",
      "read": {},
      "write": {
        "bigquery": "DIV(x, y)",
        "duckdb": "x // y"
      }
    },
    {
      "sql": "CREATE TABLE db.example_table (col_a struct<struct_col_a:int, struct_col_b:string>)",
      "read": {},
      "write": {
        "bigquery": "CREATE TABLE db.example_table (col_a STRUCT<struct_col_a INT64, struct_col_b STRING>)",
        "duckdb": "CREATE TABLE db.example_table (col_a STRUCT(struct_col_a INT, struct_col_b TEXT))",
        "presto": "CREATE TABLE db.example_table (col_a ROW(struct_col_a INTEGER, struct_col_b VARCHAR))",
        "hive": "CREATE TABLE db.example_table (col_a STRUCT<struct_col_a: INT, struct_col_b: STRING>)",
        "spark": "CREATE TABLE db.example_table (col_a STRUCT<struct_col_a: INT, struct_col_b: STRING>)"
      }
    },
    {
      "sql": "CREATE TABLE db.example_table (col_a STRUCT<struct_col_a INT64, struct_col_b STRUCT<nested_col_a STRING, nested_col_b STRING>>)",
      "read": {},
      "write": {
        "bigquery": "CREATE TABLE db.example_table (col_a STRUCT<struct_col_a INT64, struct_col_b STRUCT<nested_col_a STRING, nested_col_b STRING>>)",
        "duckdb": "CREATE TABLE db.example_table (col_a STRUCT(struct_col_a BIGINT, struct_col_b STRUCT(nested_col_a TEXT, nested_col_b TEXT)))",
        "presto": "CREATE TABLE db.example_table (col_a ROW(struct_col_a BIGINT, struct_col_b ROW(nested_col_a VARCHAR, nested_col_b VARCHAR)))",
        "hive": "CREATE TABLE db.example_table (col_a STRUCT<struct_col_a: BIGINT, struct_col_b: STRUCT<nested_col_a: STRING, nested_col_b: STRING>>)",
        "spark": "CREATE TABLE db.example_table (col_a STRUCT<struct_col_a: BIGINT, struct_col_b: STRUCT<nested_col_a: STRING, nested_col_b: STRING>>)"
      }
    },
    {
      "sql": "CREATE TABLE db.example_table (x int) PARTITION BY x cluster by x",
      "read": {},
      "write": {
        "bigquery": "CREATE TABLE db.example_table (x INT64) PARTITION BY x CLUSTER BY x"
      }
    },
    {
      "sql": "DELETE db.example_table WHERE x = 1",
      "read": {},
      "write": {
        "bigquery": "DELETE db.example_table WHERE x = 1",
        "presto": "DELETE FROM db.example_table WHERE x = 1"
      }
    },
    {
      "sql": "DELETE db.example_table tb WHERE tb.x = 1",
      "read": {},
      "write": {
        "bigquery": "DELETE db.example_table AS tb WHERE tb.x = 1",
        "presto": "DELETE FROM db.example_table WHERE x = 1"
      }
    },
    {
      "sql": "DELETE db.example_table AS tb WHERE tb.x = 1",
      "read": {},
      "write": {
        "bigquery": "DELETE db.example_table AS tb WHERE tb.x = 1",
        "presto": "DELETE FROM db.example_table WHERE x = 1"
      }
    },
    {
      "sql": "DELETE FROM db.example_table WHERE x = 1",
      "read": {},
      "write": {
        "bigquery": "DELETE FROM db.example_table WHERE x = 1",
        "presto": "DELETE FROM db.example_table WHERE x = 1"
      }
    },
    {
      "sql": "DELETE FROM db.example_table tb WHERE tb.x = 1",
      "read": {},
      "write": {
        "bigquery": "DELETE FROM db.example_table AS tb WHERE tb.x = 1",
        "presto": "DELETE FROM db.example_table WHERE x = 1"
      }
    },
    {
      "sql": "DELETE FROM db.example_table AS tb WHERE tb.x = 1",
      "read": {},
      "write": {
        "bigquery": "DELETE FROM db.example_table AS tb WHERE tb.x = 1",
        "presto": "DELETE FROM db.example_table WHERE x = 1"
      }
    },
    {
      "sql": "DELETE FROM db.example_table AS tb WHERE example_table.x = 1",
      "read": {},
      "write": {
        "bigquery": "DELETE FROM db.example_table AS tb WHERE example_table.x = 1",
        "presto": "DELETE FROM db.example_table WHERE x = 1"
      }
    },
    {
      "sql": "DELETE FROM db.example_table WHERE example_table.x = 1",
      "read": {},
      "write": {
        "bigquery": "DELETE FROM db.example_table WHERE example_table.x = 1",
        "presto": "DELETE FROM db.example_table WHERE example_table.x = 1"
      }
    },
    {
      "sql": "DELETE FROM db.t1 AS t1 WHERE NOT t1.c IN (SELECT db.t2.c FROM db.t2)",
      "read": {},
      "write": {
        "bigquery": "DELETE FROM db.t1 AS t1 WHERE NOT t1.c IN (SELECT db.t2.c FROM db.t2)",
        "presto": "DELETE FROM db.t1 WHERE NOT c IN (SELECT c FROM db.t2)"
      }
    },
    {
      "sql": "SELECT * FROM a WHERE b IN UNNEST([1, 2, 3])",
      "read": {},
      "write": {
        "bigquery": "SELECT * FROM a WHERE b IN UNNEST([1, 2, 3])",
        "presto": "SELECT * FROM a WHERE b IN (SELECT UNNEST(ARRAY[1, 2, 3]))",
        "hive": "SELECT * FROM a WHERE b IN (SELECT EXPLODE(ARRAY(1, 2, 3)))",
        "spark": "SELECT * FROM a WHERE b IN (SELECT EXPLODE(ARRAY(1, 2, 3)))"
      }
    },
    {
      "sql": "DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY)",
      "read": {},
      "write": {
        "postgres": "CURRENT_DATE - INTERVAL '1 DAY'",
        "bigquery": "DATE_SUB(CURRENT_DATE, INTERVAL '1' DAY)"
      }
    },
    {
      "sql": "DATE_ADD(CURRENT_DATE(), INTERVAL -1 DAY)",
      "read": {},
      "write": {
        "bigquery": "DATE_ADD(CURRENT_DATE, INTERVAL '-1' DAY)",
        "duckdb": "CURRENT_DATE + INTERVAL '-1' DAY",
        "mysql": "DATE_ADD(CURRENT_DATE, INTERVAL '-1' DAY)",
        "postgres": "CURRENT_DATE + INTERVAL '-1 DAY'",
        "presto": "DATE_ADD('DAY', CAST('-1' AS BIGINT), CURRENT_DATE)",
        "hive": "DATE_ADD(CURRENT_DATE, -1)",
        "spark": "DATE_ADD(CURRENT_DATE, -1)"
      }
    },
    {
      "sql": "DATE_DIFF(DATE '2010-07-07', DATE '2008-12-25', DAY)",
      "read": {},
      "write": {
        "bigquery": "DATE_DIFF(CAST('2010-07-07' AS DATE), CAST('2008-12-25' AS DATE), DAY)",
        "mysql": "DATEDIFF(CAST('2010-07-07' AS DATE), CAST('2008-12-25' AS DATE))",
        "starrocks": "DATE_DIFF('DAY', CAST('2010-07-07' AS DATE), CAST('2008-12-25' AS DATE))"
      }
    },
    {
      "sql": "DATE_DIFF(CAST('2010-07-07' AS DATE), CAST('2008-12-25' AS DATE), DAY)",
      "read": {
        "mysql": "DATEDIFF(CAST('2010-07-07' AS DATE), CAST('2008-12-25' AS DATE))",
        "starrocks": "DATEDIFF(CAST('2010-07-07' AS DATE), CAST('2008-12-25' AS DATE))"
      },
      "write": {}
    },
    {
      "sql": "DATE_DIFF(DATE '2010-07-07', DATE '2008-12-25', MINUTE)",
      "read": {},
      "write": {
        "bigquery": "DATE_DIFF(CAST('2010-07-07' AS DATE), CAST('2008-12-25' AS DATE), MINUTE)",
        "starrocks": "DATE_DIFF('MINUTE', CAST('2010-07-07' AS DATE), CAST('2008-12-25' AS DATE))"
      }
    },
    {
      "sql": "DATE_DIFF('2021-01-01', '2020-01-01', DAY)",
      "read": {},
      "write": {
        "bigquery": "DATE_DIFF('2021-01-01', '2020-01-01', DAY)",
        "duckdb": "DATE_DIFF('DAY', CAST('2020-01-01' AS DATE), CAST('2021-01-01' AS DATE))"
      }
    },
    {
      "sql": "CURRENT_DATE('UTC')",
      "read": {},
      "write": {
        "bigquery": "CURRENT_DATE('UTC')",
        "duckdb": "CAST(CURRENT_TIMESTAMP AT TIME ZONE 'UTC' AS DATE)",
        "mysql": "CURRENT_DATE AT TIME ZONE 'UTC'",
        "postgres": "CURRENT_DATE AT TIME ZONE 'UTC'",
        "snowflake": "CAST(CONVERT_TIMEZONE('UTC', CURRENT_TIMESTAMP()) AS DATE)"
      }
    },
    {
      "sql": "SELECT a FROM test WHERE a = 1 GROUP BY a HAVING a = 2 QUALIFY z ORDER BY a LIMIT 10",
      "read": {},
      "write": {
        "bigquery": "SELECT a FROM test WHERE a = 1 GROUP BY a HAVING a = 2 QUALIFY z ORDER BY a LIMIT 10",
        "snowflake": "SELECT a FROM test WHERE a = 1 GROUP BY a HAVING a = 2 QUALIFY z ORDER BY a NULLS FIRST LIMIT 10"
      }
    },
    {
      "sql": "SELECT cola, colb FROM UNNEST([STRUCT(1 AS cola, 'test' AS colb)]) AS tab",
      "read": {
        "bigquery": "SELECT cola, colb FROM UNNEST([STRUCT(1 AS cola, 'test' AS colb)]) as tab",
        "snowflake": "SELECT cola, colb FROM (VALUES (1, 'test')) AS tab(cola, colb)",
        "spark": "SELECT cola, colb FROM VALUES (1, 'test') AS tab(cola, colb)"
      },
      "write": {}
    },
    {
      "sql": "SELECT * FROM UNNEST([STRUCT(1 AS _c0)]) AS t1",
      "read": {
        "bigquery": "SELECT * FROM UNNEST([STRUCT(1 AS _c0)]) AS t1",
        "postgres": "SELECT * FROM (VALUES (1)) AS t1"
      },
      "write": {}
    },
    {
      "sql": "SELECT * FROM UNNEST([STRUCT(1 AS id)]) AS t1 CROSS JOIN UNNEST([STRUCT(1 AS id)]) AS t2",
      "read": {
        "bigquery": "SELECT * FROM UNNEST([STRUCT(1 AS id)]) AS t1 CROSS JOIN UNNEST([STRUCT(1 AS id)]) AS t2",
        "postgres": "SELECT * FROM (VALUES (1)) AS t1(id) CROSS JOIN (VALUES (1)) AS t2(id)"
      },
      "write": {}
    },
    {
      "sql": "SELECT * FROM UNNEST([1]) WITH OFFSET",
      "read": {},
      "write": {
        "bigquery": "SELECT * FROM UNNEST([1]) WITH OFFSET AS offset"
      }
    },
    {
      "sql": "SELECT * FROM UNNEST([1]) WITH OFFSET y",
      "read": {},
      "write": {
        "bigquery": "SELECT * FROM UNNEST([1]) WITH OFFSET AS y"
      }
    },
    {
      "sql": "GENERATE_ARRAY(1, 4)",
      "read": {
        "bigquery": "GENERATE_ARRAY(1, 4)"
      },
      "write": {
        "duckdb": "GENERATE_SERIES(1, 4)"
      }
    },
    {
      "sql": "TO_JSON_STRING(x)",
      "read": {
        "bigquery": "TO_JSON_STRING(x)"
      },
      "write": {
        "bigquery": "TO_JSON_STRING(x)",
        "duckdb": "CAST(TO_JSON(x) AS TEXT)",
        "presto": "JSON_FORMAT(CAST(x AS JSON))",
        "spark": "TO_JSON(x)"
      }
    },
    {
      "sql": "SELECT MOD(x, 10)",
      "read": {
        "postgres": "SELECT x % 10"
      },
      "write": {
        "bigquery": "SELECT MOD(x, 10)",
        "postgres": "SELECT x % 10"
      }
    },
    {
      "sql": "SELECT CAST(x AS DATETIME)",
      "read": {},
      "write": {
        "bigquery": "SELECT CAST(x AS DATETIME)"
      }
    },
    {
      "sql": "SELECT TIME(foo, 'America/Los_Angeles')",
      "read": {},
      "write": {
        "duckdb": "SELECT CAST(CAST(foo AS TIMESTAMPTZ) AT TIME ZONE 'America/Los_Angeles' AS TIME)",
        "bigquery": "SELECT TIME(foo, 'America/Los_Angeles')"
      }
    },
    {
      "sql": "SELECT DATETIME('2020-01-01')",
      "read": {},
      "write": {
        "duckdb": "SELECT CAST('2020-01-01' AS TIMESTAMP)",
        "bigquery": "SELECT DATETIME('2020-01-01')"
      }
    },
    {
      "sql": "SELECT DATETIME('2020-01-01', TIME '23:59:59')",
      "read": {},
      "write": {
        "duckdb": "SELECT CAST(CAST('2020-01-01' AS DATE) + CAST('23:59:59' AS TIME) AS TIMESTAMP)",
        "bigquery": "SELECT DATETIME('2020-01-01', CAST('23:59:59' AS TIME))"
      }
    },
    {
      "sql": "SELECT DATETIME('2020-01-01', 'America/Los_Angeles')",
      "read": {},
      "write": {
        "duckdb": "SELECT CAST(CAST('2020-01-01' AS TIMESTAMPTZ) AT TIME ZONE 'America/Los_Angeles' AS TIMESTAMP)",
        "bigquery": "SELECT DATETIME('2020-01-01', 'America/Los_Angeles')"
      }
    },
    {
      "sql": "SELECT LENGTH(foo)",
      "read": {
        "bigquery": "SELECT LENGTH(foo)",
        "snowflake": "SELECT LENGTH(foo)"
      },
      "write": {
        "duckdb": "SELECT CASE TYPEOF(foo) WHEN 'BLOB' THEN OCTET_LENGTH(CAST(foo AS BLOB)) ELSE LENGTH(CAST(foo AS TEXT)) END",
        "snowflake": "SELECT LENGTH(foo)"
      }
    },
    {
      "sql": "SELECT TIME_DIFF('12:00:00', '12:30:00', MINUTE)",
      "read": {},
      "write": {
        "duckdb": "SELECT DATE_DIFF('MINUTE', CAST('12:30:00' AS TIME), CAST('12:00:00' AS TIME))",
        "bigquery": "SELECT TIME_DIFF('12:00:00', '12:30:00', MINUTE)"
      }
    },
    {
      "sql": "ARRAY_CONCAT([1, 2], [3, 4], [5, 6])",
      "read": {},
      "write": {
        "bigquery": "ARRAY_CONCAT([1, 2], [3, 4], [5, 6])",
        "duckdb": "ARRAY_CONCAT([1, 2], ARRAY_CONCAT([3, 4], [5, 6]))",
        "postgres": "ARRAY_CAT(ARRAY[1, 2], ARRAY_CAT(ARRAY[3, 4], ARRAY[5, 6]))",
        "redshift": "ARRAY_CONCAT(ARRAY(1, 2), ARRAY_CONCAT(ARRAY(3, 4), ARRAY(5, 6)))",
        "snowflake": "ARRAY_CAT([1, 2], ARRAY_CAT([3, 4], [5, 6]))",
        "hive": "CONCAT(ARRAY(1, 2), ARRAY(3, 4), ARRAY(5, 6))",
        "spark2": "CONCAT(ARRAY(1, 2), ARRAY(3, 4), ARRAY(5, 6))",
        "spark": "CONCAT(ARRAY(1, 2), ARRAY(3, 4), ARRAY(5, 6))",
        "databricks": "CONCAT(ARRAY(1, 2), ARRAY(3, 4), ARRAY(5, 6))",
        "presto": "CONCAT(ARRAY[1, 2], ARRAY[3, 4], ARRAY[5, 6])",
        "trino": "CONCAT(ARRAY[1, 2], ARRAY[3, 4], ARRAY[5, 6])"
      }
    },
    {
      "sql": "SELECT GENERATE_TIMESTAMP_ARRAY('2016-10-05 00:00:00', '2016-10-07 00:00:00', INTERVAL '1' DAY)",
      "read": {},
      "write": {
        "duckdb": "SELECT GENERATE_SERIES(CAST('2016-10-05 00:00:00' AS TIMESTAMP), CAST('2016-10-07 00:00:00' AS TIMESTAMP), INTERVAL '1' DAY)",
        "bigquery": "SELECT GENERATE_TIMESTAMP_ARRAY('2016-10-05 00:00:00', '2016-10-07 00:00:00', INTERVAL '1' DAY)"
      }
    },
    {
      "sql": "SELECT PARSE_DATE('%A %b %e %Y', 'Thursday Dec 25 2008')",
      "read": {},
      "write": {
        "bigquery": "SELECT PARSE_DATE('%A %b %e %Y', 'Thursday Dec 25 2008')",
        "duckdb": "SELECT CAST(STRPTIME('Thursday Dec 25 2008', '%A %b %-d %Y') AS DATE)"
      }
    },
    {
      "sql": "SELECT PARSE_DATE('%Y%m%d', '20081225')",
      "read": {},
      "write": {
        "bigquery": "SELECT PARSE_DATE('%Y%m%d', '20081225')",
        "duckdb": "SELECT CAST(STRPTIME('20081225', '%Y%m%d') AS DATE)",
        "snowflake": "SELECT DATE('20081225', 'yyyymmDD')"
      }
    },
    {
      "sql": "SELECT ARRAY_TO_STRING(['cake', 'pie', NULL], '--') AS text",
      "read": {},
      "write": {
        "bigquery": "SELECT ARRAY_TO_STRING(['cake', 'pie', NULL], '--') AS text",
        "duckdb": "SELECT ARRAY_TO_STRING(['cake', 'pie', NULL], '--') AS text"
      }
    },
    {
      "sql": "SELECT ARRAY_TO_STRING(['cake', 'pie', NULL], '--', 'MISSING') AS text",
      "read": {},
      "write": {
        "bigquery": "SELECT ARRAY_TO_STRING(['cake', 'pie', NULL], '--', 'MISSING') AS text",
        "duckdb": "SELECT ARRAY_TO_STRING(LIST_TRANSFORM(['cake', 'pie', NULL], x -> COALESCE(x, 'MISSING')), '--') AS text"
      }
    },
    {
      "sql": "STRING(a)",
      "read": {},
      "write": {
        "bigquery": "STRING(a)",
        "snowflake": "CAST(a AS VARCHAR)",
        "duckdb": "CAST(a AS TEXT)"
      }
    },
    {
      "sql": "STRING('2008-12-25 15:30:00', 'America/New_York')",
      "read": {},
      "write": {
        "bigquery": "STRING('2008-12-25 15:30:00', 'America/New_York')",
        "snowflake": "CAST(CONVERT_TIMEZONE('UTC', 'America/New_York', '2008-12-25 15:30:00') AS VARCHAR)",
        "duckdb": "CAST(CAST('2008-12-25 15:30:00' AS TIMESTAMP) AT TIME ZONE 'UTC' AT TIME ZONE 'America/New_York' AS TEXT)"
      }
    },
    {
      "sql": "SAFE_DIVIDE(x, y)",
      "read": {},
      "write": {
        "bigquery": "SAFE_DIVIDE(x, y)",
        "duckdb": "CASE WHEN y <> 0 THEN x / y ELSE NULL END",
        "presto": "IF(y <> 0, CAST(x AS DOUBLE) / y, NULL)",
        "trino": "IF(y <> 0, CAST(x AS DOUBLE) / y, NULL)",
        "hive": "IF(y <> 0, x / y, NULL)",
        "spark2": "IF(y <> 0, x / y, NULL)",
        "spark": "IF(y <> 0, x / y, NULL)",
        "databricks": "IF(y <> 0, x / y, NULL)",
        "snowflake": "IFF(y <> 0, x / y, NULL)",
        "postgres": "CASE WHEN y <> 0 THEN CAST(x AS DOUBLE PRECISION) / y ELSE NULL END"
      }
    },
    {
      "sql": "SAFE_DIVIDE(x + 1, 2 * y)",
      "read": {},
      "write": {
        "bigquery": "SAFE_DIVIDE(x + 1, 2 * y)",
        "duckdb": "CASE WHEN (2 * y) <> 0 THEN (x + 1) / (2 * y) ELSE NULL END",
        "presto": "IF((2 * y) <> 0, CAST((x + 1) AS DOUBLE) / (2 * y), NULL)",
        "trino": "IF((2 * y) <> 0, CAST((x + 1) AS DOUBLE) / (2 * y), NULL)",
        "hive": "IF((2 * y) <> 0, (x + 1) / (2 * y), NULL)",
        "spark2": "IF((2 * y) <> 0, (x + 1) / (2 * y), NULL)",
        "spark": "IF((2 * y) <> 0, (x + 1) / (2 * y), NULL)",
        "databricks": "IF((2 * y) <> 0, (x + 1) / (2 * y), NULL)",
        "snowflake": "IFF((2 * y) <> 0, (x + 1) / (2 * y), NULL)",
        "postgres": "CASE WHEN (2 * y) <> 0 THEN CAST((x + 1) AS DOUBLE PRECISION) / (2 * y) ELSE NULL END"
      }
    },
    {
      "sql": "SELECT JSON_VALUE_ARRAY('{\"arr\": [1, \"a\"]}', '$.arr')",
      "read": {},
      "write": {
        "bigquery": "SELECT JSON_VALUE_ARRAY('{\"arr\": [1, \"a\"]}', '$.arr')",
        "duckdb": "SELECT CAST('{\"arr\": [1, \"a\"]}' -> '$.arr' AS TEXT[])",
        "snowflake": "SELECT TRANSFORM(GET_PATH(PARSE_JSON('{\"arr\": [1, \"a\"]}'), 'arr'), x -> CAST(x AS VARCHAR))"
      }
    },
    {
      "sql": "SELECT INSTR('foo@example.com', '@')",
      "read": {},
      "write": {
        "bigquery": "SELECT INSTR('foo@example.com', '@')",
        "duckdb": "SELECT STRPOS('foo@example.com', '@')",
        "snowflake": "SELECT CHARINDEX('@', 'foo@example.com')"
      }
    },
    {
      "sql": "SELECT ts + MAKE_INTERVAL(1, 2, minute => 5, day => 3)",
      "read": {},
      "write": {
        "bigquery": "SELECT ts + MAKE_INTERVAL(1, 2, day => 3, minute => 5)",
        "duckdb": "SELECT ts + INTERVAL '1 year 2 month 5 minute 3 day'",
        "snowflake": "SELECT ts + INTERVAL '1 year, 2 month, 5 minute, 3 day'"
      }
    },
    {
      "sql": "SELECT INT64(JSON_QUERY(JSON '{\"key\": 2000}', '$.key'))",
      "read": {},
      "write": {
        "bigquery": "SELECT INT64(JSON_QUERY(PARSE_JSON('{\"key\": 2000}'), '$.key'))",
        "duckdb": "SELECT CAST(JSON('{\"key\": 2000}') -> '$.key' AS BIGINT)",
        "snowflake": "SELECT CAST(GET_PATH(PARSE_JSON('{\"key\": 2000}'), 'key') AS BIGINT)"
      }
    },
    {
      "sql": "CONTAINS_SUBSTR(a, b)",
      "read": {
        "spark": "CONTAINS(a, b)",
        "databricks": "CONTAINS(a, b)",
        "snowflake": "CONTAINS(a, b)",
        "duckdb": "CONTAINS(a, b)",
        "oracle": "CONTAINS(a, b)"
      },
      "write": {
        "spark": "CONTAINS(LOWER(a), LOWER(b))",
        "databricks": "CONTAINS(LOWER(a), LOWER(b))",
        "snowflake": "CONTAINS(LOWER(a), LOWER(b))",
        "duckdb": "CONTAINS(LOWER(a), LOWER(b))",
        "oracle": "CONTAINS(LOWER(a), LOWER(b))",
        "bigquery": "CONTAINS_SUBSTR(a, b)"
      }
    },
    {
      "sql": "SELECT * FROM t1, UNNEST(`t1`) AS `col`",
      "read": {
        "duckdb": "SELECT * FROM t1, UNNEST(\"t1\") \"t1\" (\"col\")"
      },
      "write": {
        "bigquery": "SELECT * FROM t1 CROSS JOIN UNNEST(`t1`) AS `col`",
        "redshift": "SELECT * FROM t1 CROSS JOIN \"t1\" AS \"col\""
      }
    },
    {
      "sql": "SELECT * FROM t, UNNEST(`t2`.`t3`) AS `col`",
      "read": {
        "duckdb": "SELECT * FROM t, UNNEST(\"t1\".\"t2\".\"t3\") \"t1\" (\"col\")"
      },
      "write": {
        "bigquery": "SELECT * FROM t CROSS JOIN UNNEST(`t2`.`t3`) AS `col`",
        "redshift": "SELECT * FROM t CROSS JOIN \"t2\".\"t3\" AS \"col\""
      }
    },
    {
      "sql": "SELECT * FROM t1, UNNEST(`t1`.`t2`.`t3`.`t4`) AS `col`",
      "read": {
        "duckdb": "SELECT * FROM t1, UNNEST(\"t1\".\"t2\".\"t3\".\"t4\") \"t3\" (\"col\")"
      },
      "write": {
        "bigquery": "SELECT * FROM t1 CROSS JOIN UNNEST(`t1`.`t2`.`t3`.`t4`) AS `col`",
        "redshift": "SELECT * FROM t1 CROSS JOIN \"t1\".\"t2\".\"t3\".\"t4\" AS \"col\""
      }
    },
    {
      "sql": "SELECT CAST(col AS STRUCT<fld1 STRUCT<fld2 INT>>).fld1.fld2",
      "read": {},
      "write": {
        "bigquery": "SELECT CAST(col AS STRUCT<fld1 STRUCT<fld2 INT64>>).fld1.fld2",
        "snowflake": "SELECT CAST(col AS OBJECT(fld1 OBJECT(fld2 INT))):fld1.fld2"
      }
    },
    {
      "sql": "SELECT 0xA",
      "read": {},
      "write": {
        "bigquery": "SELECT 0xA",
        "duckdb": "SELECT 10"
      }
    },
    {
      "sql": "SELECT ARRAY_CONCAT_AGG(1)",
      "read": {},
      "write": {
        "snowflake": "SELECT ARRAY_FLATTEN(ARRAY_AGG(1))",
        "bigquery": "SELECT ARRAY_CONCAT_AGG(1)"
      }
    },
    {
      "sql": "SELECT b'a'",
      "read": {},
      "write": {
        "bigquery": "SELECT b'a'",
        "duckdb": "SELECT CAST(e'a' AS BLOB)",
        "postgres": "SELECT CAST(e'a' AS BYTEA)"
      }
    },
    {
      "sql": "SELECT b'a'",
      "read": {},
      "write": {
        "bigquery": "SELECT b'a'",
        "duckdb": "SELECT CAST(e'a' AS BLOB)",
        "postgres": "SELECT CAST(e'a' AS BYTEA)"
      }
    },
    {
      "sql": "SELECT GENERATE_UUID()",
      "read": {},
      "write": {
        "bigquery": "SELECT GENERATE_UUID()",
        "duckdb": "SELECT CAST(UUID() AS TEXT)",
        "spark2": "SELECT CAST(UUID() AS STRING)",
        "spark": "SELECT CAST(UUID() AS STRING)",
        "presto": "SELECT CAST(UUID() AS VARCHAR)",
        "trino": "SELECT CAST(UUID() AS VARCHAR)",
        "snowflake": "SELECT UUID_STRING()"
      }
    },
    {
      "sql": "SELECT REPLACE('apple pie', 'pie', 'cobbler') AS result",
      "read": {},
      "write": {
        "bigquery": "SELECT REPLACE('apple pie', 'pie', 'cobbler') AS result",
        "duckdb": "SELECT REPLACE('apple pie', 'pie', 'cobbler') AS result"
      }
    },
    {
      "sql": "TIMESTAMP_TRUNC(TIMESTAMP '2024-03-15 14:35:47.123456', DAY, 'America/New_York')",
      "read": {},
      "write": {
        "bigquery": "TIMESTAMP_TRUNC(CAST('2024-03-15 14:35:47.123456' AS TIMESTAMP), DAY, 'America/New_York')",
        "duckdb": "DATE_TRUNC('DAY', CAST('2024-03-15 14:35:47.123456' AS TIMESTAMPTZ) AT TIME ZONE 'America/New_York') AT TIME ZONE 'America/New_York'"
      }
    },
    {
      "sql": "TIMESTAMP_TRUNC(TIMESTAMP '2024-03-15 14:35:00', MINUTE, 'America/New_York')",
      "read": {},
      "write": {
        "bigquery": "TIMESTAMP_TRUNC(CAST('2024-03-15 14:35:00' AS TIMESTAMP), MINUTE, 'America/New_York')",
        "duckdb": "DATE_TRUNC('MINUTE', CAST('2024-03-15 14:35:00' AS TIMESTAMPTZ))"
      }
    },
    {
      "sql": "TIMESTAMP_TRUNC(TIMESTAMP '2024-03-15 14:35:47.123456', DAY)",
      "read": {},
      "write": {
        "bigquery": "TIMESTAMP_TRUNC(CAST('2024-03-15 14:35:47.123456' AS TIMESTAMP), DAY)",
        "duckdb": "DATE_TRUNC('DAY', CAST('2024-03-15 14:35:47.123456' AS TIMESTAMPTZ))"
      }
    },
    {
      "sql": "TIMESTAMP_TRUNC(TIMESTAMP '2025-01-01 14:35:47.123456', MINUTE)",
      "read": {},
      "write": {
        "bigquery": "TIMESTAMP_TRUNC(CAST('2025-01-01 14:35:47.123456' AS TIMESTAMP), MINUTE)",
        "duckdb": "DATE_TRUNC('MINUTE', CAST('2025-01-01 14:35:47.123456' AS TIMESTAMPTZ))"
      }
    },
    {
      "sql": "WITH sample AS (SELECT * FROM UNNEST([TIMESTAMP '2024-03-15 14:35:46', TIMESTAMP '2024-03-16 01:12:03']) AS ts) SELECT ts, TIMESTAMP_TRUNC(ts, DAY, 'America/New_York') AS truncated_ts FROM sample",
      "read": {},
      "write": {
        "bigquery": "WITH sample AS (SELECT * FROM UNNEST([CAST('2024-03-15 14:35:46' AS TIMESTAMP), CAST('2024-03-16 01:12:03' AS TIMESTAMP)]) AS ts) SELECT ts, TIMESTAMP_TRUNC(ts, DAY, 'America/New_York') AS truncated_ts FROM sample",
        "duckdb": "WITH sample AS (SELECT * FROM UNNEST([CAST('2024-03-15 14:35:46' AS TIMESTAMPTZ), CAST('2024-03-16 01:12:03' AS TIMESTAMPTZ)]) AS _t0(ts)) SELECT ts, DATE_TRUNC('DAY', ts AT TIME ZONE 'America/New_York') AT TIME ZONE 'America/New_York' AS truncated_ts FROM sample"
      }
    },
    {
      "sql": "WITH sample AS (SELECT ts FROM UNNEST([TIMESTAMP '2024-03-15 14:35:46', TIMESTAMP '2024-03-16 01:12:03']) AS ts) SELECT ts, TIMESTAMP_TRUNC(ts, DAY) AS truncated_ts FROM sample",
      "read": {},
      "write": {
        "bigquery": "WITH sample AS (SELECT ts FROM UNNEST([CAST('2024-03-15 14:35:46' AS TIMESTAMP), CAST('2024-03-16 01:12:03' AS TIMESTAMP)]) AS ts) SELECT ts, TIMESTAMP_TRUNC(ts, DAY) AS truncated_ts FROM sample",
        "duckdb": "WITH sample AS (SELECT ts FROM UNNEST([CAST('2024-03-15 14:35:46' AS TIMESTAMPTZ), CAST('2024-03-16 01:12:03' AS TIMESTAMPTZ)]) AS _t0(ts)) SELECT ts, DATE_TRUNC('DAY', ts) AS truncated_ts FROM sample"
      }
    },
    {
      "sql": "WITH sample AS (SELECT * FROM UNNEST([TIMESTAMP '2024-03-15 14:35:46', TIMESTAMP '2024-03-16 01:12:03']) AS ts) SELECT ts, TIMESTAMP_TRUNC(ts, MINUTE, 'America/New_York') AS truncated_ts FROM sample",
      "read": {},
      "write": {
        "bigquery": "WITH sample AS (SELECT * FROM UNNEST([CAST('2024-03-15 14:35:46' AS TIMESTAMP), CAST('2024-03-16 01:12:03' AS TIMESTAMP)]) AS ts) SELECT ts, TIMESTAMP_TRUNC(ts, MINUTE, 'America/New_York') AS truncated_ts FROM sample",
        "duckdb": "WITH sample AS (SELECT * FROM UNNEST([CAST('2024-03-15 14:35:46' AS TIMESTAMPTZ), CAST('2024-03-16 01:12:03' AS TIMESTAMPTZ)]) AS _t0(ts)) SELECT ts, DATE_TRUNC('MINUTE', ts) AS truncated_ts FROM sample"
      }
    },
    {
      "sql": "WITH sample AS (SELECT * FROM UNNEST([TIMESTAMP '2024-03-15 14:35:46', TIMESTAMP '2024-03-16 01:12:03']) AS ts) SELECT ts, TIMESTAMP_TRUNC(ts, MINUTE) AS truncated_ts FROM sample",
      "read": {},
      "write": {
        "bigquery": "WITH sample AS (SELECT * FROM UNNEST([CAST('2024-03-15 14:35:46' AS TIMESTAMP), CAST('2024-03-16 01:12:03' AS TIMESTAMP)]) AS ts) SELECT ts, TIMESTAMP_TRUNC(ts, MINUTE) AS truncated_ts FROM sample",
        "duckdb": "WITH sample AS (SELECT * FROM UNNEST([CAST('2024-03-15 14:35:46' AS TIMESTAMPTZ), CAST('2024-03-16 01:12:03' AS TIMESTAMPTZ)]) AS _t0(ts)) SELECT ts, DATE_TRUNC('MINUTE', ts) AS truncated_ts FROM sample"
      }
    },
    {
      "sql": "SELECT GREATEST(1, NULL, 3)",
      "read": {},
      "write": {
        "duckdb": "SELECT CASE WHEN 1 IS NULL OR NULL IS NULL OR 3 IS NULL THEN NULL ELSE GREATEST(1, NULL, 3) END",
        "bigquery": "SELECT GREATEST(1, NULL, 3)"
      }
    },
    {
      "sql": "SELECT LEAST(1, NULL, 3)",
      "read": {},
      "write": {
        "duckdb": "SELECT CASE WHEN 1 IS NULL OR NULL IS NULL OR 3 IS NULL THEN NULL ELSE LEAST(1, NULL, 3) END",
        "bigquery": "SELECT LEAST(1, NULL, 3)"
      }
    },
    {
      "sql": "\n            MERGE dataset.Inventory T\n            USING dataset.NewArrivals S ON FALSE\n            WHEN NOT MATCHED BY TARGET AND product LIKE '%a%'\n            THEN DELETE\n            WHEN NOT MATCHED BY SOURCE AND product LIKE '%b%'\n            THEN DELETE",
      "read": {},
      "write": {
        "bigquery": "MERGE INTO dataset.Inventory AS T USING dataset.NewArrivals AS S ON FALSE WHEN NOT MATCHED AND product LIKE '%a%' THEN DELETE WHEN NOT MATCHED BY SOURCE AND product LIKE '%b%' THEN DELETE",
        "snowflake": "MERGE INTO dataset.Inventory AS T USING dataset.NewArrivals AS S ON FALSE WHEN NOT MATCHED AND product LIKE '%a%' THEN DELETE WHEN NOT MATCHED AND product LIKE '%b%' THEN DELETE"
      }
    },
    {
      "sql": "ALTER TABLE db.t1 RENAME TO db.t2",
      "read": {},
      "write": {
        "snowflake": "ALTER TABLE db.t1 RENAME TO db.t2",
        "bigquery": "ALTER TABLE db.t1 RENAME TO t2"
      }
    },
    {
      "sql": "WITH cte AS (SELECT 1 AS foo) SELECT foo FROM cte",
      "read": {
        "spark": "WITH cte(foo) AS (SELECT 1) SELECT foo FROM cte"
      },
      "write": {}
    },
    {
      "sql": "WITH cte AS (SELECT 1 AS foo) SELECT foo FROM cte",
      "read": {
        "spark": "WITH cte(foo) AS (SELECT 1 AS bar) SELECT foo FROM cte"
      },
      "write": {}
    },
    {
      "sql": "WITH cte AS (SELECT 1 AS bar) SELECT bar FROM cte",
      "read": {
        "spark": "WITH cte AS (SELECT 1 AS bar) SELECT bar FROM cte"
      },
      "write": {}
    },
    {
      "sql": "WITH cte AS (SELECT 1 AS foo, 2) SELECT foo FROM cte",
      "read": {
        "postgres": "WITH cte(foo) AS (SELECT 1, 2) SELECT foo FROM cte"
      },
      "write": {}
    },
    {
      "sql": "WITH cte AS (SELECT 1 AS foo UNION ALL SELECT 2) SELECT foo FROM cte",
      "read": {
        "postgres": "WITH cte(foo) AS (SELECT 1 UNION ALL SELECT 2) SELECT foo FROM cte"
      },
      "write": {}
    },
    {
      "sql": "SELECT ARRAY<FLOAT64>[1, 2, 3]",
      "read": {},
      "write": {
        "bigquery": "SELECT ARRAY<FLOAT64>[1, 2, 3]",
        "duckdb": "SELECT CAST([1, 2, 3] AS DOUBLE[])"
      }
    },
    {
      "sql": "CAST(STRUCT<a INT64>(1) AS STRUCT<a INT64>)",
      "read": {},
      "write": {
        "bigquery": "CAST(CAST(STRUCT(1) AS STRUCT<a INT64>) AS STRUCT<a INT64>)",
        "duckdb": "CAST(CAST(ROW(1) AS STRUCT(a BIGINT)) AS STRUCT(a BIGINT))"
      }
    },
    {
      "sql": "SELECT * FROM UNNEST(ARRAY<STRUCT<x INT64>>[])",
      "read": {},
      "write": {
        "bigquery": "SELECT * FROM UNNEST(ARRAY<STRUCT<x INT64>>[])",
        "duckdb": "SELECT * FROM (SELECT UNNEST(CAST([] AS STRUCT(x BIGINT)[]), max_depth => 2))"
      }
    },
    {
      "sql": "SELECT * FROM UNNEST(ARRAY<STRUCT<device_id INT64, time DATETIME, signal INT64, state STRING>>[STRUCT(1, DATETIME '2023-11-01 09:34:01', 74, 'INACTIVE'),STRUCT(4, DATETIME '2023-11-01 09:38:01', 80, 'ACTIVE')])",
      "read": {},
      "write": {
        "bigquery": "SELECT * FROM UNNEST(ARRAY<STRUCT<device_id INT64, time DATETIME, signal INT64, state STRING>>[STRUCT(1, CAST('2023-11-01 09:34:01' AS DATETIME), 74, 'INACTIVE'), STRUCT(4, CAST('2023-11-01 09:38:01' AS DATETIME), 80, 'ACTIVE')])",
        "duckdb": "SELECT * FROM (SELECT UNNEST(CAST([ROW(1, CAST('2023-11-01 09:34:01' AS TIMESTAMP), 74, 'INACTIVE'), ROW(4, CAST('2023-11-01 09:38:01' AS TIMESTAMP), 80, 'ACTIVE')] AS STRUCT(device_id BIGINT, time TIMESTAMP, signal BIGINT, state TEXT)[]), max_depth => 2))"
      }
    },
    {
      "sql": "SELECT STRUCT<a INT64, b STRUCT<c STRING>>(1, STRUCT('c_str'))",
      "read": {},
      "write": {
        "bigquery": "SELECT CAST(STRUCT(1, STRUCT('c_str')) AS STRUCT<a INT64, b STRUCT<c STRING>>)",
        "duckdb": "SELECT CAST(ROW(1, ROW('c_str')) AS STRUCT(a BIGINT, b STRUCT(c TEXT)))"
      }
    },
    {
      "sql": "SELECT MAX_BY(name, score) FROM table1",
      "read": {},
      "write": {
        "bigquery": "SELECT MAX_BY(name, score) FROM table1",
        "duckdb": "SELECT ARG_MAX(name, score) FROM table1"
      }
    },
    {
      "sql": "SELECT MIN_BY(product, price) FROM table1",
      "read": {},
      "write": {
        "bigquery": "SELECT MIN_BY(product, price) FROM table1",
        "duckdb": "SELECT ARG_MIN(product, price) FROM table1"
      }
    },
    {
      "sql": "SELECT name, laps FROM UNNEST([STRUCT('Rudisha' AS name, [23.4, 26.3, 26.4, 26.1] AS laps), STRUCT('Makhloufi' AS name, [24.5, 25.4, 26.6, 26.1] AS laps)])",
      "read": {},
      "write": {
        "bigquery": "SELECT name, laps FROM UNNEST([STRUCT('Rudisha' AS name, [23.4, 26.3, 26.4, 26.1] AS laps), STRUCT('Makhloufi' AS name, [24.5, 25.4, 26.6, 26.1] AS laps)])",
        "duckdb": "SELECT name, laps FROM (SELECT UNNEST([{'name': 'Rudisha', 'laps': [23.4, 26.3, 26.4, 26.1]}, {'name': 'Makhloufi', 'laps': [24.5, 25.4, 26.6, 26.1]}], max_depth => 2))"
      }
    },
    {
      "sql": "WITH Races AS (SELECT '800M' AS race) SELECT race, name, laps FROM Races AS r CROSS JOIN UNNEST([STRUCT('Rudisha' AS name, [23.4, 26.3, 26.4, 26.1] AS laps)])",
      "read": {},
      "write": {
        "bigquery": "WITH Races AS (SELECT '800M' AS race) SELECT race, name, laps FROM Races AS r CROSS JOIN UNNEST([STRUCT('Rudisha' AS name, [23.4, 26.3, 26.4, 26.1] AS laps)])",
        "duckdb": "WITH Races AS (SELECT '800M' AS race) SELECT race, name, laps FROM Races AS r CROSS JOIN (SELECT UNNEST([{'name': 'Rudisha', 'laps': [23.4, 26.3, 26.4, 26.1]}], max_depth => 2))"
      }
    },
    {
      "sql": "SELECT participant FROM UNNEST([STRUCT('Rudisha' AS name, [23.4, 26.3, 26.4, 26.1] AS laps)]) AS participant",
      "read": {},
      "write": {
        "bigquery": "SELECT participant FROM UNNEST([STRUCT('Rudisha' AS name, [23.4, 26.3, 26.4, 26.1] AS laps)]) AS participant",
        "duckdb": "SELECT participant FROM (SELECT UNNEST([{'name': 'Rudisha', 'laps': [23.4, 26.3, 26.4, 26.1]}], max_depth => 2)) AS participant"
      }
    },
    {
      "sql": "WITH Races AS (SELECT '800M' AS race) SELECT race, participant FROM Races AS r CROSS JOIN UNNEST([STRUCT('Rudisha' AS name, [23.4, 26.3, 26.4, 26.1] AS laps)]) AS participant",
      "read": {},
      "write": {
        "bigquery": "WITH Races AS (SELECT '800M' AS race) SELECT race, participant FROM Races AS r CROSS JOIN UNNEST([STRUCT('Rudisha' AS name, [23.4, 26.3, 26.4, 26.1] AS laps)]) AS participant",
        "duckdb": "WITH Races AS (SELECT '800M' AS race) SELECT race, participant FROM Races AS r CROSS JOIN (SELECT UNNEST([{'name': 'Rudisha', 'laps': [23.4, 26.3, 26.4, 26.1]}], max_depth => 2)) AS participant"
      }
    },
    {
      "sql": "SELECT * FROM UNNEST([STRUCT('Alice' AS name, STRUCT(85 AS math, 90 AS english) AS scores), STRUCT('Bob' AS name, STRUCT(92 AS math, 88 AS english) AS scores)])",
      "read": {},
      "write": {
        "bigquery": "SELECT * FROM UNNEST([STRUCT('Alice' AS name, STRUCT(85 AS math, 90 AS english) AS scores), STRUCT('Bob' AS name, STRUCT(92 AS math, 88 AS english) AS scores)])",
        "duckdb": "SELECT * FROM (SELECT UNNEST([{'name': 'Alice', 'scores': {'math': 85, 'english': 90}}, {'name': 'Bob', 'scores': {'math': 92, 'english': 88}}], max_depth => 2))",
        "snowflake": "SELECT * FROM TABLE(FLATTEN(INPUT => [OBJECT_CONSTRUCT('name', 'Alice', 'scores', OBJECT_CONSTRUCT('math', 85, 'english', 90)), OBJECT_CONSTRUCT('name', 'Bob', 'scores', OBJECT_CONSTRUCT('math', 92, 'english', 88))])) AS _t0(seq, key, path, index, value, this)",
        "presto": "SELECT * FROM UNNEST(ARRAY[CAST(ROW('Alice', CAST(ROW(85, 90) AS ROW(math INTEGER, english INTEGER))) AS ROW(name VARCHAR, scores ROW(math INTEGER, english INTEGER))), CAST(ROW('Bob', CAST(ROW(92, 88) AS ROW(math INTEGER, english INTEGER))) AS ROW(name VARCHAR, scores ROW(math INTEGER, english INTEGER)))])",
        "trino": "SELECT * FROM UNNEST(ARRAY[CAST(ROW('Alice', CAST(ROW(85, 90) AS ROW(math INTEGER, english INTEGER))) AS ROW(name VARCHAR, scores ROW(math INTEGER, english INTEGER))), CAST(ROW('Bob', CAST(ROW(92, 88) AS ROW(math INTEGER, english INTEGER))) AS ROW(name VARCHAR, scores ROW(math INTEGER, english INTEGER)))])",
        "spark2": "SELECT * FROM EXPLODE(ARRAY(STRUCT('Alice' AS name, STRUCT(85 AS math, 90 AS english) AS scores), STRUCT('Bob' AS name, STRUCT(92 AS math, 88 AS english) AS scores)))",
        "databricks": "SELECT * FROM EXPLODE(ARRAY(STRUCT('Alice' AS name, STRUCT(85 AS math, 90 AS english) AS scores), STRUCT('Bob' AS name, STRUCT(92 AS math, 88 AS english) AS scores)))",
        "hive": "SELECT * FROM EXPLODE(ARRAY(STRUCT('Alice', STRUCT(85, 90)), STRUCT('Bob', STRUCT(92, 88))))"
      }
    },
    {
      "sql": "SELECT * FROM UNNEST([STRUCT('Alice' AS name, 85 AS score), STRUCT('Bob', 92), STRUCT('Diana', 95)])",
      "read": {},
      "write": {
        "bigquery": "SELECT * FROM UNNEST([STRUCT('Alice' AS name, 85 AS score), STRUCT('Bob', 92), STRUCT('Diana', 95)])",
        "duckdb": "SELECT * FROM (SELECT UNNEST([{'name': 'Alice', 'score': 85}, {'name': 'Bob', 'score': 92}, {'name': 'Diana', 'score': 95}], max_depth => 2))",
        "snowflake": "SELECT * FROM TABLE(FLATTEN(INPUT => [OBJECT_CONSTRUCT('name', 'Alice', 'score', 85), OBJECT_CONSTRUCT('name', 'Bob', 'score', 92), OBJECT_CONSTRUCT('name', 'Diana', 'score', 95)])) AS _t0(seq, key, path, index, value, this)",
        "presto": "SELECT * FROM UNNEST(ARRAY[CAST(ROW('Alice', 85) AS ROW(name VARCHAR, score INTEGER)), CAST(ROW('Bob', 92) AS ROW(name VARCHAR, score INTEGER)), CAST(ROW('Diana', 95) AS ROW(name VARCHAR, score INTEGER))])",
        "trino": "SELECT * FROM UNNEST(ARRAY[CAST(ROW('Alice', 85) AS ROW(name VARCHAR, score INTEGER)), CAST(ROW('Bob', 92) AS ROW(name VARCHAR, score INTEGER)), CAST(ROW('Diana', 95) AS ROW(name VARCHAR, score INTEGER))])",
        "spark2": "SELECT * FROM EXPLODE(ARRAY(STRUCT('Alice' AS name, 85 AS score), STRUCT('Bob' AS name, 92 AS score), STRUCT('Diana' AS name, 95 AS score)))",
        "databricks": "SELECT * FROM EXPLODE(ARRAY(STRUCT('Alice' AS name, 85 AS score), STRUCT('Bob' AS name, 92 AS score), STRUCT('Diana' AS name, 95 AS score)))",
        "hive": "SELECT * FROM EXPLODE(ARRAY(STRUCT('Alice', 85), STRUCT('Bob', 92), STRUCT('Diana', 95)))"
      }
    },
    {
      "sql": "SELECT JSON_QUERY('{\"class\": {\"students\": []}}', '$.class')",
      "read": {},
      "write": {
        "bigquery": "SELECT JSON_QUERY('{\"class\": {\"students\": []}}', '$.class')",
        "duckdb": "SELECT '{\"class\": {\"students\": []}}' -> '$.class'",
        "snowflake": "SELECT GET_PATH(PARSE_JSON('{\"class\": {\"students\": []}}'), 'class')"
      }
    },
    {
      "sql": "SELECT JSON_QUERY(foo, '$.class')",
      "read": {},
      "write": {
        "bigquery": "SELECT JSON_QUERY(foo, '$.class')",
        "snowflake": "SELECT GET_PATH(PARSE_JSON(foo), 'class')"
      }
    },
    {
      "sql": "SELECT UNIX_SECONDS('2008-12-25 15:30:00+00')",
      "read": {
        "bigquery": "SELECT UNIX_SECONDS('2008-12-25 15:30:00+00')",
        "spark": "SELECT UNIX_SECONDS('2008-12-25 15:30:00+00')",
        "databricks": "SELECT UNIX_SECONDS('2008-12-25 15:30:00+00')"
      },
      "write": {
        "spark": "SELECT UNIX_SECONDS('2008-12-25 15:30:00+00')",
        "databricks": "SELECT UNIX_SECONDS('2008-12-25 15:30:00+00')",
        "duckdb": "SELECT CAST(EPOCH(CAST('2008-12-25 15:30:00+00' AS TIMESTAMPTZ)) AS BIGINT)",
        "snowflake": "SELECT TIMESTAMPDIFF(SECONDS, CAST('1970-01-01 00:00:00+00' AS TIMESTAMPTZ), '2008-12-25 15:30:00+00')"
      }
    },
    {
      "sql": "SELECT UNIX_MICROS('2008-12-25 15:30:00+00')",
      "read": {},
      "write": {
        "bigquery": "SELECT UNIX_MICROS('2008-12-25 15:30:00+00')",
        "duckdb": "SELECT EPOCH_US(CAST('2008-12-25 15:30:00+00' AS TIMESTAMPTZ))"
      }
    },
    {
      "sql": "SELECT UNIX_MICROS(TIMESTAMP '2008-12-25 15:30:00+00')",
      "read": {},
      "write": {
        "bigquery": "SELECT UNIX_MICROS(CAST('2008-12-25 15:30:00+00' AS TIMESTAMP))",
        "duckdb": "SELECT EPOCH_US(CAST('2008-12-25 15:30:00+00' AS TIMESTAMPTZ))"
      }
    },
    {
      "sql": "SELECT UNIX_MILLIS('2008-12-25 15:30:00+00')",
      "read": {},
      "write": {
        "bigquery": "SELECT UNIX_MILLIS('2008-12-25 15:30:00+00')",
        "duckdb": "SELECT EPOCH_MS(CAST('2008-12-25 15:30:00+00' AS TIMESTAMPTZ))"
      }
    },
    {
      "sql": "SELECT UNIX_MILLIS(TIMESTAMP '2008-12-25 15:30:00+00')",
      "read": {},
      "write": {
        "bigquery": "SELECT UNIX_MILLIS(CAST('2008-12-25 15:30:00+00' AS TIMESTAMP))",
        "duckdb": "SELECT EPOCH_MS(CAST('2008-12-25 15:30:00+00' AS TIMESTAMPTZ))"
      }
    },
    {
      "sql": "SELECT REGEXP_EXTRACT(abc, 'pattern(group)') FROM table",
      "read": {},
      "write": {
        "bigquery": "SELECT REGEXP_EXTRACT(abc, 'pattern(group)') FROM table",
        "duckdb": "SELECT REGEXP_EXTRACT(abc, 'pattern(group)', 1) FROM \"table\""
      }
    },
    {
      "sql": "SELECT REGEXP_EXTRACT(abc, 'pattern(group)', 1) FROM table",
      "read": {},
      "write": {
        "bigquery": "SELECT REGEXP_EXTRACT(abc, 'pattern(group)', 1) FROM table",
        "duckdb": "SELECT REGEXP_EXTRACT(abc, 'pattern(group)', 1) FROM \"table\""
      }
    },
    {
      "sql": "SELECT REGEXP_EXTRACT(abc, 'pattern(group)', 2) FROM table",
      "read": {},
      "write": {
        "bigquery": "SELECT REGEXP_EXTRACT(abc, 'pattern(group)', 2) FROM table",
        "duckdb": "SELECT REGEXP_EXTRACT(NULLIF(SUBSTRING(abc, 2), ''), 'pattern(group)', 1) FROM \"table\""
      }
    },
    {
      "sql": "SELECT REGEXP_EXTRACT(abc, 'pattern(group)', 1, 1) FROM table",
      "read": {},
      "write": {
        "bigquery": "SELECT REGEXP_EXTRACT(abc, 'pattern(group)', 1, 1) FROM table",
        "duckdb": "SELECT REGEXP_EXTRACT(abc, 'pattern(group)', 1) FROM \"table\""
      }
    },
    {
      "sql": "SELECT REGEXP_EXTRACT(abc, 'pattern(group)', 2, 3) FROM table",
      "read": {},
      "write": {
        "bigquery": "SELECT REGEXP_EXTRACT(abc, 'pattern(group)', 2, 3) FROM table",
        "duckdb": "SELECT ARRAY_EXTRACT(REGEXP_EXTRACT_ALL(NULLIF(SUBSTRING(abc, 2), ''), 'pattern(group)', 1), 3) FROM \"table\""
      }
    },
    {
      "sql": "REGEXP_EXTRACT_ALL('a1_a2a3_a4A5a6', 'a[0-9]')",
      "read": {
        "bigquery": "REGEXP_EXTRACT_ALL('a1_a2a3_a4A5a6', 'a[0-9]')",
        "trino": "REGEXP_EXTRACT_ALL('a1_a2a3_a4A5a6', 'a[0-9]')",
        "presto": "REGEXP_EXTRACT_ALL('a1_a2a3_a4A5a6', 'a[0-9]')",
        "snowflake": "REGEXP_EXTRACT_ALL('a1_a2a3_a4A5a6', 'a[0-9]')",
        "duckdb": "REGEXP_EXTRACT_ALL('a1_a2a3_a4A5a6', 'a[0-9]', 0)",
        "spark": "REGEXP_EXTRACT_ALL('a1_a2a3_a4A5a6', 'a[0-9]', 0)",
        "databricks": "REGEXP_EXTRACT_ALL('a1_a2a3_a4A5a6', 'a[0-9]', 0)"
      },
      "write": {
        "bigquery": "REGEXP_EXTRACT_ALL('a1_a2a3_a4A5a6', 'a[0-9]')",
        "trino": "REGEXP_EXTRACT_ALL('a1_a2a3_a4A5a6', 'a[0-9]')",
        "presto": "REGEXP_EXTRACT_ALL('a1_a2a3_a4A5a6', 'a[0-9]')",
        "snowflake": "REGEXP_EXTRACT_ALL('a1_a2a3_a4A5a6', 'a[0-9]')",
        "duckdb": "REGEXP_EXTRACT_ALL('a1_a2a3_a4A5a6', 'a[0-9]', 0)",
        "spark": "REGEXP_EXTRACT_ALL('a1_a2a3_a4A5a6', 'a[0-9]', 0)",
        "databricks": "REGEXP_EXTRACT_ALL('a1_a2a3_a4A5a6', 'a[0-9]', 0)"
      }
    },
    {
      "sql": "REGEXP_EXTRACT_ALL('a1_a2a3_a4A5a6', '(a)[0-9]')",
      "read": {},
      "write": {
        "bigquery": "REGEXP_EXTRACT_ALL('a1_a2a3_a4A5a6', '(a)[0-9]')",
        "trino": "REGEXP_EXTRACT_ALL('a1_a2a3_a4A5a6', '(a)[0-9]', 1)",
        "presto": "REGEXP_EXTRACT_ALL('a1_a2a3_a4A5a6', '(a)[0-9]', 1)",
        "snowflake": "REGEXP_EXTRACT_ALL('a1_a2a3_a4A5a6', '(a)[0-9]', 1, 1, 'c', 1)",
        "duckdb": "REGEXP_EXTRACT_ALL('a1_a2a3_a4A5a6', '(a)[0-9]', 1)",
        "spark": "REGEXP_EXTRACT_ALL('a1_a2a3_a4A5a6', '(a)[0-9]')",
        "databricks": "REGEXP_EXTRACT_ALL('a1_a2a3_a4A5a6', '(a)[0-9]')"
      }
    },
    {
      "sql": "SELECT FORMAT_DATE('%Y%m%d', '2023-12-25')",
      "read": {},
      "write": {
        "bigquery": "SELECT FORMAT_DATE('%Y%m%d', '2023-12-25')",
        "duckdb": "SELECT STRFTIME(CAST('2023-12-25' AS DATE), '%Y%m%d')"
      }
    },
    {
      "sql": "SELECT FORMAT_DATETIME('%Y%m%d %H:%M:%S', DATETIME '2023-12-25 15:30:00')",
      "read": {},
      "write": {
        "bigquery": "SELECT FORMAT_DATETIME('%Y%m%d %T', CAST('2023-12-25 15:30:00' AS DATETIME))",
        "duckdb": "SELECT STRFTIME(CAST('2023-12-25 15:30:00' AS TIMESTAMP), '%Y%m%d %H:%M:%S')"
      }
    },
    {
      "sql": "SELECT FORMAT_DATETIME('%x', '2023-12-25 15:30:00')",
      "read": {},
      "write": {
        "bigquery": "SELECT FORMAT_DATETIME('%D', '2023-12-25 15:30:00')",
        "duckdb": "SELECT STRFTIME(CAST('2023-12-25 15:30:00' AS TIMESTAMP), '%m/%d/%y')"
      }
    },
    {
      "sql": "SELECT FORMAT_DATETIME('%F %T', DATETIME '2023-10-15 14:30:45')",
      "read": {},
      "write": {
        "bigquery": "SELECT FORMAT_DATETIME('%F %T', CAST('2023-10-15 14:30:45' AS DATETIME))",
        "duckdb": "SELECT STRFTIME(CAST('2023-10-15 14:30:45' AS TIMESTAMP), '%Y-%m-%d %H:%M:%S')"
      }
    },
    {
      "sql": "SELECT FORMAT_DATETIME('%c', DATETIME '2008-12-25 15:30:00')",
      "read": {},
      "write": {
        "bigquery": "SELECT FORMAT_DATETIME('%c', CAST('2008-12-25 15:30:00' AS DATETIME))",
        "duckdb": "SELECT STRFTIME(CAST('2008-12-25 15:30:00' AS TIMESTAMP), '%a %b %-d %H:%M:%S %Y')"
      }
    },
    {
      "sql": "SELECT FORMAT_DATETIME('%Y-%m-%e', DATETIME '2020-09-09 10:15:30')",
      "read": {},
      "write": {
        "bigquery": "SELECT FORMAT_DATETIME('%Y-%m-%e', CAST('2020-09-09 10:15:30' AS DATETIME))",
        "duckdb": "SELECT STRFTIME(CAST('2020-09-09 10:15:30' AS TIMESTAMP), '%Y-%m-%-d')"
      }
    },
    {
      "sql": "SELECT FORMAT_TIMESTAMP(\"%b-%d-%Y\", TIMESTAMP \"2050-12-25 15:30:55+00\")",
      "read": {},
      "write": {
        "bigquery": "SELECT FORMAT_TIMESTAMP('%b-%d-%Y', CAST('2050-12-25 15:30:55+00' AS TIMESTAMP))",
        "duckdb": "SELECT STRFTIME(CAST(CAST('2050-12-25 15:30:55+00' AS TIMESTAMPTZ) AS TIMESTAMP), '%b-%d-%Y')",
        "snowflake": "SELECT TO_CHAR(CAST(CAST('2050-12-25 15:30:55+00' AS TIMESTAMPTZ) AS TIMESTAMP), 'mon-DD-yyyy')"
      }
    },
    {
      "sql": "WITH x AS ( SELECT 1 AS id), test_cte AS ( SELECT ARRAY_CONCAT(( SELECT id FROM x WHERE FALSE)) AS result ) SELECT * FROM test_cte;",
      "read": {},
      "write": {
        "snowflake": "WITH x AS (SELECT 1 AS id), test_cte AS (SELECT ARRAY_CAT((SELECT id FROM x WHERE FALSE), []) AS result) SELECT * FROM test_cte"
      }
    },
    {
      "sql": "SELECT ARRAY(SELECT AS STRUCT x1 AS x1, x2 AS x2 FROM t) AS array_col",
      "read": {},
      "write": {
        "bigquery": "SELECT ARRAY(SELECT AS STRUCT x1 AS x1, x2 AS x2 FROM t) AS array_col",
        "snowflake": "SELECT (SELECT ARRAY_AGG(OBJECT_CONSTRUCT('x1', x1, 'x2', x2)) FROM t) AS array_col"
      }
    },
    {
      "sql": "WITH t1 AS (SELECT ARRAY(SELECT AS STRUCT x1 AS alias_x1, x2 /* test */ FROM t2) AS array_col) SELECT array_col[0].alias_x1, array_col[0].x2 FROM t1",
      "read": {},
      "write": {
        "bigquery": "WITH t1 AS (SELECT ARRAY(SELECT AS STRUCT x1 AS alias_x1, x2 /* test */ FROM t2) AS array_col) SELECT array_col[0].alias_x1, array_col[0].x2 FROM t1",
        "snowflake": "WITH t1 AS (SELECT (SELECT ARRAY_AGG(OBJECT_CONSTRUCT('alias_x1', x1, 'x2', x2 /* test */)) FROM t2) AS array_col) SELECT array_col[0].alias_x1, array_col[0].x2 FROM t1"
      }
    },
    {
      "sql": "WITH t1 AS (SELECT ARRAY(SELECT AS STRUCT 1 AS a, 2 AS b) AS array_col) SELECT array_col[0].a, array_col[0].b FROM t1",
      "read": {},
      "write": {
        "bigquery": "WITH t1 AS (SELECT ARRAY(SELECT AS STRUCT 1 AS a, 2 AS b) AS array_col) SELECT array_col[0].a, array_col[0].b FROM t1",
        "snowflake": "WITH t1 AS (SELECT (SELECT ARRAY_AGG(OBJECT_CONSTRUCT('a', 1, 'b', 2))) AS array_col) SELECT array_col[0].a, array_col[0].b FROM t1"
      }
    },
    {
      "sql": "WITH t1 AS (SELECT ARRAY(SELECT AS STRUCT x1 AS alias_x1, x2 /* test */ FROM t2 WHERE x2 = 4) AS array_col) SELECT array_col[0].alias_x1, array_col[0].x2 FROM t1",
      "read": {},
      "write": {
        "bigquery": "WITH t1 AS (SELECT ARRAY(SELECT AS STRUCT x1 AS alias_x1, x2 /* test */ FROM t2 WHERE x2 = 4) AS array_col) SELECT array_col[0].alias_x1, array_col[0].x2 FROM t1",
        "snowflake": "WITH t1 AS (SELECT (SELECT ARRAY_AGG(OBJECT_CONSTRUCT('alias_x1', x1, 'x2', x2 /* test */)) FROM t2 WHERE x2 = 4) AS array_col) SELECT array_col[0].alias_x1, array_col[0].x2 FROM t1"
      }
    },
    {
      "sql": "SELECT GENERATE_DATE_ARRAY('2016-10-05', '2016-10-08')",
      "read": {},
      "write": {
        "bigquery": "SELECT GENERATE_DATE_ARRAY('2016-10-05', '2016-10-08', INTERVAL '1' DAY)",
        "duckdb": "SELECT CAST(GENERATE_SERIES(CAST('2016-10-05' AS DATE), CAST('2016-10-08' AS DATE), INTERVAL '1' DAY) AS DATE[])"
      }
    },
    {
      "sql": "SELECT GENERATE_DATE_ARRAY('2016-10-05', '2016-10-08', INTERVAL '1' MONTH)",
      "read": {},
      "write": {
        "bigquery": "SELECT GENERATE_DATE_ARRAY('2016-10-05', '2016-10-08', INTERVAL '1' MONTH)",
        "duckdb": "SELECT CAST(GENERATE_SERIES(CAST('2016-10-05' AS DATE), CAST('2016-10-08' AS DATE), INTERVAL '1' MONTH) AS DATE[])"
      }
    },
    {
      "sql": "SELECT id, mnth FROM t CROSS JOIN UNNEST(GENERATE_DATE_ARRAY(start_month, DATE_TRUNC(CURRENT_DATE, MONTH), INTERVAL '1' MONTH)) AS mnth",
      "read": {},
      "write": {
        "bigquery": "SELECT id, mnth FROM t CROSS JOIN UNNEST(GENERATE_DATE_ARRAY(start_month, DATE_TRUNC(CURRENT_DATE, MONTH), INTERVAL '1' MONTH)) AS mnth",
        "duckdb": "SELECT id, mnth FROM t CROSS JOIN UNNEST(CAST(GENERATE_SERIES(start_month, DATE_TRUNC('MONTH', CURRENT_DATE), INTERVAL '1' MONTH) AS DATE[])) AS _t0(mnth)",
        "snowflake": "SELECT id, DATEADD(MONTH, CAST(mnth AS INT), CAST(start_month AS DATE)) AS mnth FROM t, LATERAL FLATTEN(INPUT => ARRAY_GENERATE_RANGE(0, (DATEDIFF(MONTH, start_month, DATE_TRUNC('MONTH', CURRENT_DATE)) + 1 - 1) + 1)) AS _t0(seq, key, path, index, mnth, this)"
      }
    },
    {
      "sql": "SELECT id, mnth AS a_mnth FROM t CROSS JOIN UNNEST(GENERATE_DATE_ARRAY(start_month, DATE_TRUNC(CURRENT_DATE, MONTH), INTERVAL '1' MONTH)) AS mnth",
      "read": {},
      "write": {
        "bigquery": "SELECT id, mnth AS a_mnth FROM t CROSS JOIN UNNEST(GENERATE_DATE_ARRAY(start_month, DATE_TRUNC(CURRENT_DATE, MONTH), INTERVAL '1' MONTH)) AS mnth",
        "duckdb": "SELECT id, mnth AS a_mnth FROM t CROSS JOIN UNNEST(CAST(GENERATE_SERIES(start_month, DATE_TRUNC('MONTH', CURRENT_DATE), INTERVAL '1' MONTH) AS DATE[])) AS _t0(mnth)",
        "snowflake": "SELECT id, DATEADD(MONTH, CAST(mnth AS INT), CAST(start_month AS DATE)) AS a_mnth FROM t, LATERAL FLATTEN(INPUT => ARRAY_GENERATE_RANGE(0, (DATEDIFF(MONTH, start_month, DATE_TRUNC('MONTH', CURRENT_DATE)) + 1 - 1) + 1)) AS _t0(seq, key, path, index, mnth, this)"
      }
    },
    {
      "sql": "SELECT id, mnth + 1 AS a_mnth FROM t CROSS JOIN UNNEST(GENERATE_DATE_ARRAY(start_month, DATE_TRUNC(CURRENT_DATE, MONTH), INTERVAL '1' MONTH)) AS mnth",
      "read": {},
      "write": {
        "bigquery": "SELECT id, mnth + 1 AS a_mnth FROM t CROSS JOIN UNNEST(GENERATE_DATE_ARRAY(start_month, DATE_TRUNC(CURRENT_DATE, MONTH), INTERVAL '1' MONTH)) AS mnth",
        "duckdb": "SELECT id, mnth + 1 AS a_mnth FROM t CROSS JOIN UNNEST(CAST(GENERATE_SERIES(start_month, DATE_TRUNC('MONTH', CURRENT_DATE), INTERVAL '1' MONTH) AS DATE[])) AS _t0(mnth)",
        "snowflake": "SELECT id, DATEADD(MONTH, CAST(mnth AS INT), CAST(start_month AS DATE)) + 1 AS a_mnth FROM t, LATERAL FLATTEN(INPUT => ARRAY_GENERATE_RANGE(0, (DATEDIFF(MONTH, start_month, DATE_TRUNC('MONTH', CURRENT_DATE)) + 1 - 1) + 1)) AS _t0(seq, key, path, index, mnth, this)"
      }
    },
    {
      "sql": "SELECT DATE_DIFF('2024-06-15', '2024-01-08', WEEK(MONDAY))",
      "read": {},
      "write": {
        "bigquery": "SELECT DATE_DIFF('2024-06-15', '2024-01-08', WEEK(MONDAY))",
        "duckdb": "SELECT DATE_DIFF('WEEK', DATE_TRUNC('WEEK', CAST('2024-01-08' AS DATE)), DATE_TRUNC('WEEK', CAST('2024-06-15' AS DATE)))"
      }
    },
    {
      "sql": "SELECT DATE_DIFF('2026-01-15', '2024-01-08', WEEK(SUNDAY))",
      "read": {},
      "write": {
        "bigquery": "SELECT DATE_DIFF('2026-01-15', '2024-01-08', WEEK)",
        "duckdb": "SELECT DATE_DIFF('WEEK', DATE_TRUNC('WEEK', CAST('2024-01-08' AS DATE) + INTERVAL '1' DAY), DATE_TRUNC('WEEK', CAST('2026-01-15' AS DATE) + INTERVAL '1' DAY))"
      }
    },
    {
      "sql": "SELECT DATE_DIFF('2024-01-15', '2022-04-28', WEEK(SATURDAY))",
      "read": {},
      "write": {
        "bigquery": "SELECT DATE_DIFF('2024-01-15', '2022-04-28', WEEK(SATURDAY))",
        "duckdb": "SELECT DATE_DIFF('WEEK', DATE_TRUNC('WEEK', CAST('2022-04-28' AS DATE) + INTERVAL '-5' DAY), DATE_TRUNC('WEEK', CAST('2024-01-15' AS DATE) + INTERVAL '-5' DAY))"
      }
    },
    {
      "sql": "SELECT DATE_DIFF('2024-01-15', '2024-01-08', WEEK)",
      "read": {},
      "write": {
        "bigquery": "SELECT DATE_DIFF('2024-01-15', '2024-01-08', WEEK)",
        "duckdb": "SELECT DATE_DIFF('WEEK', DATE_TRUNC('WEEK', CAST('2024-01-08' AS DATE) + INTERVAL '1' DAY), DATE_TRUNC('WEEK', CAST('2024-01-15' AS DATE) + INTERVAL '1' DAY))"
      }
    },
    {
      "sql": "SELECT DATE_DIFF('2024-01-07', '2024-01-06', WEEK)",
      "read": {},
      "write": {
        "bigquery": "SELECT DATE_DIFF('2024-01-07', '2024-01-06', WEEK)",
        "duckdb": "SELECT DATE_DIFF('WEEK', DATE_TRUNC('WEEK', CAST('2024-01-06' AS DATE) + INTERVAL '1' DAY), DATE_TRUNC('WEEK', CAST('2024-01-07' AS DATE) + INTERVAL '1' DAY))"
      }
    },
    {
      "sql": "SELECT DATE_DIFF('2024-01-15', '2024-01-08', ISOWEEK)",
      "read": {},
      "write": {
        "bigquery": "SELECT DATE_DIFF('2024-01-15', '2024-01-08', ISOWEEK)",
        "duckdb": "SELECT DATE_DIFF('WEEK', DATE_TRUNC('WEEK', CAST('2024-01-08' AS DATE)), DATE_TRUNC('WEEK', CAST('2024-01-15' AS DATE)))"
      }
    },
    {
      "sql": "SELECT DATE_DIFF(DATE '2024-09-15', DATE '2024-01-08', WEEK(MONDAY))",
      "read": {},
      "write": {
        "bigquery": "SELECT DATE_DIFF(CAST('2024-09-15' AS DATE), CAST('2024-01-08' AS DATE), WEEK(MONDAY))",
        "duckdb": "SELECT DATE_DIFF('WEEK', DATE_TRUNC('WEEK', CAST('2024-01-08' AS DATE)), DATE_TRUNC('WEEK', CAST('2024-09-15' AS DATE)))"
      }
    },
    {
      "sql": "SELECT DATE_DIFF(DATE '2024-01-01', DATE '2024-01-15', WEEK(SUNDAY))",
      "read": {},
      "write": {
        "bigquery": "SELECT DATE_DIFF(CAST('2024-01-01' AS DATE), CAST('2024-01-15' AS DATE), WEEK)",
        "duckdb": "SELECT DATE_DIFF('WEEK', DATE_TRUNC('WEEK', CAST('2024-01-15' AS DATE) + INTERVAL '1' DAY), DATE_TRUNC('WEEK', CAST('2024-01-01' AS DATE) + INTERVAL '1' DAY))"
      }
    },
    {
      "sql": "SELECT DATE_DIFF(DATE '2023-05-01', DATE '2024-01-15', ISOWEEK)",
      "read": {},
      "write": {
        "bigquery": "SELECT DATE_DIFF(CAST('2023-05-01' AS DATE), CAST('2024-01-15' AS DATE), ISOWEEK)",
        "duckdb": "SELECT DATE_DIFF('WEEK', DATE_TRUNC('WEEK', CAST('2024-01-15' AS DATE)), DATE_TRUNC('WEEK', CAST('2023-05-01' AS DATE)))"
      }
    },
    {
      "sql": "SELECT DATE_DIFF(DATE '2024-01-01', DATE '2024-01-15', DAY)",
      "read": {},
      "write": {
        "bigquery": "SELECT DATE_DIFF(CAST('2024-01-01' AS DATE), CAST('2024-01-15' AS DATE), DAY)",
        "duckdb": "SELECT DATE_DIFF('DAY', CAST('2024-01-15' AS DATE), CAST('2024-01-01' AS DATE))"
      }
    },
    {
      "sql": "SAFE_ADD(x, y)",
      "read": {
        "bigquery": "SAFE_ADD(x, y)",
        "spark": "TRY_ADD(x, y)",
        "databricks": "TRY_ADD(x, y)"
      },
      "write": {
        "spark": "TRY_ADD(x, y)",
        "databricks": "TRY_ADD(x, y)"
      }
    },
    {
      "sql": "SAFE_MULTIPLY(x, y)",
      "read": {
        "bigquery": "SAFE_MULTIPLY(x, y)",
        "spark": "TRY_MULTIPLY(x, y)",
        "databricks": "TRY_MULTIPLY(x, y)"
      },
      "write": {
        "spark": "TRY_MULTIPLY(x, y)",
        "databricks": "TRY_MULTIPLY(x, y)"
      }
    },
    {
      "sql": "SAFE_SUBTRACT(x, y)",
      "read": {
        "bigquery": "SAFE_SUBTRACT(x, y)",
        "spark": "TRY_SUBTRACT(x, y)",
        "databricks": "TRY_SUBTRACT(x, y)"
      },
      "write": {
        "spark": "TRY_SUBTRACT(x, y)",
        "databricks": "TRY_SUBTRACT(x, y)"
      }
    },
    {
      "sql": "SELECT 1 & 1",
      "read": {},
      "write": {
        "bigquery": "SELECT 1 & 1",
        "snowflake": "SELECT BITAND(1, 1)"
      }
    },
    {
      "sql": "SELECT ~1",
      "read": {},
      "write": {
        "bigquery": "SELECT ~1",
        "snowflake": "SELECT BITNOT(1)"
      }
    },
    {
      "sql": "BIT_AND(x)",
      "read": {
        "bigquery": "BIT_AND(x)",
        "databricks": "BIT_AND(x)",
        "dremio": "BIT_AND(x)",
        "duckdb": "BIT_AND(x)",
        "mysql": "BIT_AND(x)",
        "postgres": "BIT_AND(x)",
        "spark": "BIT_AND(x)"
      },
      "write": {
        "databricks": "BIT_AND(x)",
        "dremio": "BIT_AND(x)",
        "duckdb": "BIT_AND(x)",
        "mysql": "BIT_AND(x)",
        "postgres": "BIT_AND(x)",
        "spark": "BIT_AND(x)"
      }
    },
    {
      "sql": "BIT_OR(x)",
      "read": {
        "bigquery": "BIT_OR(x)",
        "databricks": "BIT_OR(x)",
        "dremio": "BIT_OR(x)",
        "duckdb": "BIT_OR(x)",
        "mysql": "BIT_OR(x)",
        "postgres": "BIT_OR(x)",
        "spark": "BIT_OR(x)"
      },
      "write": {
        "databricks": "BIT_OR(x)",
        "dremio": "BIT_OR(x)",
        "duckdb": "BIT_OR(x)",
        "mysql": "BIT_OR(x)",
        "postgres": "BIT_OR(x)",
        "spark": "BIT_OR(x)"
      }
    },
    {
      "sql": "BIT_XOR(x)",
      "read": {
        "bigquery": "BIT_XOR(x)",
        "databricks": "BIT_XOR(x)",
        "duckdb": "BIT_XOR(x)",
        "mysql": "BIT_XOR(x)",
        "postgres": "BIT_XOR(x)",
        "spark": "BIT_XOR(x)"
      },
      "write": {
        "databricks": "BIT_XOR(x)",
        "duckdb": "BIT_XOR(x)",
        "mysql": "BIT_XOR(x)",
        "postgres": "BIT_XOR(x)",
        "spark": "BIT_XOR(x)"
      }
    },
    {
      "sql": "BIT_COUNT(x)",
      "read": {
        "bigquery": "BIT_COUNT(x)",
        "spark": "BIT_COUNT(x)",
        "databricks": "BIT_COUNT(x)",
        "mysql": "BIT_COUNT(x)"
      },
      "write": {
        "spark": "BIT_COUNT(x)",
        "databricks": "BIT_COUNT(x)",
        "mysql": "BIT_COUNT(x)"
      }
    },
    {
      "sql": "SELECT TO_HEX(SHA1('abc'))",
      "read": {},
      "write": {
        "bigquery": "SELECT TO_HEX(SHA1('abc'))",
        "snowflake": "SELECT TO_CHAR(SHA1_BINARY('abc'))"
      }
    },
    {
      "sql": "SELECT MD5('abc')",
      "read": {},
      "write": {
        "bigquery": "SELECT MD5('abc')",
        "snowflake": "SELECT MD5_BINARY('abc')"
      }
    },
    {
      "sql": "SELECT TO_JSON_STRING(STRUCT('Alice' AS name)) AS json_data",
      "read": {},
      "write": {
        "bigquery": "SELECT TO_JSON_STRING(STRUCT('Alice' AS name)) AS json_data",
        "snowflake": "SELECT TO_JSON(OBJECT_CONSTRUCT('name', 'Alice')) AS json_data"
      }
    },
    {
      "sql": "SELECT CONCAT('T.P.', ' ', 'Bar') AS author",
      "read": {},
      "write": {
        "bigquery": "SELECT CONCAT('T.P.', ' ', 'Bar') AS author",
        "duckdb": "SELECT 'T.P.' || ' ' || 'Bar' AS author"
      }
    },
    {
      "sql": "SELECT ROUND(2.25) AS value",
      "read": {},
      "write": {
        "bigquery": "SELECT ROUND(2.25) AS value",
        "duckdb": "SELECT ROUND(2.25) AS value"
      }
    },
    {
      "sql": "SELECT ROUND(2.25, 1) AS value",
      "read": {},
      "write": {
        "bigquery": "SELECT ROUND(2.25, 1) AS value",
        "duckdb": "SELECT ROUND(2.25, 1) AS value"
      }
    },
    {
      "sql": "SELECT ROUND(NUMERIC '2.25', 1, 'ROUND_HALF_AWAY_FROM_ZERO') AS value",
      "read": {},
      "write": {
        "bigquery": "SELECT ROUND(CAST('2.25' AS NUMERIC), 1, 'ROUND_HALF_AWAY_FROM_ZERO') AS value",
        "duckdb": "SELECT ROUND(CAST('2.25' AS DECIMAL), 1) AS value"
      }
    },
    {
      "sql": "SELECT ROUND(NUMERIC '2.25', 1, 'ROUND_HALF_EVEN') AS value",
      "read": {},
      "write": {
        "bigquery": "SELECT ROUND(CAST('2.25' AS NUMERIC), 1, 'ROUND_HALF_EVEN') AS value",
        "duckdb": "SELECT ROUND_EVEN(CAST('2.25' AS DECIMAL), 1) AS value"
      }
    },
    {
      "sql": "APPROX_QUANTILES(x, 1)",
      "read": {},
      "write": {
        "duckdb": "APPROX_QUANTILE(x, [0, 1])"
      }
    },
    {
      "sql": "APPROX_QUANTILES(x, 2)",
      "read": {},
      "write": {
        "duckdb": "APPROX_QUANTILE(x, [0, 0.5, 1])"
      }
    },
    {
      "sql": "APPROX_QUANTILES(x, 4)",
      "read": {},
      "write": {
        "duckdb": "APPROX_QUANTILE(x, [0, 0.25, 0.5, 0.75, 1])"
      }
    },
    {
      "sql": "APPROX_QUANTILES(DISTINCT x, 2)",
      "read": {},
      "write": {
        "duckdb": "APPROX_QUANTILE(DISTINCT x, [0, 0.5, 1])"
      }
    },
    {
      "sql": "SELECT DATETIME_DIFF('2023-01-01T00:00:00', '2023-01-01T05:00:00', MILLISECOND)",
      "read": {},
      "write": {
        "bigquery": "SELECT DATETIME_DIFF('2023-01-01T00:00:00', '2023-01-01T05:00:00', MILLISECOND)",
        "databricks": "SELECT TIMESTAMPDIFF(MILLISECOND, '2023-01-01T05:00:00', '2023-01-01T00:00:00')",
        "snowflake": "SELECT TIMESTAMPDIFF(MILLISECOND, '2023-01-01T05:00:00', '2023-01-01T00:00:00')",
        "duckdb": "SELECT DATE_DIFF('MILLISECOND', CAST('2023-01-01T05:00:00' AS TIMESTAMP), CAST('2023-01-01T00:00:00' AS TIMESTAMP))"
      }
    },
    {
      "sql": "SELECT DATETIME_ADD('2023-01-01T00:00:00', INTERVAL 1 MILLISECOND)",
      "read": {},
      "write": {
        "bigquery": "SELECT DATETIME_ADD('2023-01-01T00:00:00', INTERVAL '1' MILLISECOND)",
        "databricks": "SELECT TIMESTAMPADD(MILLISECOND, '1', '2023-01-01T00:00:00')",
        "duckdb": "SELECT CAST('2023-01-01T00:00:00' AS TIMESTAMP) + INTERVAL '1' MILLISECOND",
        "snowflake": "SELECT TIMESTAMPADD(MILLISECOND, '1', '2023-01-01T00:00:00')",
        "spark": "SELECT '2023-01-01T00:00:00' + INTERVAL '1' MILLISECOND"
      }
    },
    {
      "sql": "SELECT DATETIME_SUB('2023-01-01T00:00:00', INTERVAL 1 MILLISECOND)",
      "read": {},
      "write": {
        "bigquery": "SELECT DATETIME_SUB('2023-01-01T00:00:00', INTERVAL '1' MILLISECOND)",
        "databricks": "SELECT TIMESTAMPADD(MILLISECOND, '1' * -1, '2023-01-01T00:00:00')",
        "duckdb": "SELECT CAST('2023-01-01T00:00:00' AS TIMESTAMP) - INTERVAL '1' MILLISECOND",
        "spark": "SELECT '2023-01-01T00:00:00' - INTERVAL '1' MILLISECOND"
      }
    },
    {
      "sql": "SELECT DATETIME_TRUNC('2023-01-01T01:01:01', HOUR)",
      "read": {},
      "write": {
        "bigquery": "SELECT DATETIME_TRUNC('2023-01-01T01:01:01', HOUR)",
        "databricks": "SELECT DATE_TRUNC('HOUR', '2023-01-01T01:01:01')",
        "duckdb": "SELECT DATE_TRUNC('HOUR', CAST('2023-01-01T01:01:01' AS TIMESTAMP))"
      }
    },
    {
      "sql": "SELECT a[1], b[OFFSET(1)], c[ORDINAL(1)], d[SAFE_OFFSET(1)], e[SAFE_ORDINAL(1)]",
      "read": {},
      "write": {
        "duckdb": "SELECT a[2], b[2], c[1], d[2], e[1]",
        "bigquery": "SELECT a[1], b[OFFSET(1)], c[ORDINAL(1)], d[SAFE_OFFSET(1)], e[SAFE_ORDINAL(1)]",
        "presto": "SELECT a[2], b[2], c[1], ELEMENT_AT(d, 2), ELEMENT_AT(e, 1)"
      }
    },
    {
      "sql": "a[0]",
      "read": {
        "bigquery": "a[0]",
        "duckdb": "a[1]",
        "presto": "a[1]"
      },
      "write": {}
    },
    {
      "sql": "APPROX_QUANTILES(x, 2 IGNORE NULLS)",
      "read": {},
      "write": {
        "duckdb": "APPROX_QUANTILE(x, [0, 0.5, 1])"
      }
    }
  ]
}