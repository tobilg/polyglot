{
  "dialect": "",
  "identity": [
    {
      "sql": "DECODE(bin, charset)",
      "expected": null
    },
    {
      "sql": "SELECT ARRAY_INTERSECT(x, y, z)",
      "expected": null
    },
    {
      "sql": "SELECT c FROM t ORDER BY a, b,",
      "expected": "SELECT c FROM t ORDER BY a, b"
    },
    {
      "sql": "SELECT art FROM tbl1 INNER JOIN LATERAL (SELECT art FROM tbl2) AS tbl2 ON tbl1.art = tbl2.art",
      "expected": null
    },
    {
      "sql": "SELECT * FROM tbl AS t LEFT JOIN LATERAL (SELECT * FROM b WHERE b.t_id = t.t_id) AS t ON TRUE",
      "expected": null
    },
    {
      "sql": "some.column LIKE 'foo' || another.column || 'bar' || LOWER(x)",
      "expected": null
    },
    {
      "sql": "some.column LIKE 'foo' + another.column + 'bar'",
      "expected": null
    },
    {
      "sql": "COUNT_IF(DISTINCT cond)",
      "expected": null
    },
    {
      "sql": "CAST(x AS some_udt(1234))",
      "expected": null
    },
    {
      "sql": "TRUNCATE TABLE table",
      "expected": null
    },
    {
      "sql": "TRUNCATE TABLE db.schema.test",
      "expected": null
    },
    {
      "sql": "TRUNCATE TABLE IF EXISTS db.schema.test",
      "expected": null
    },
    {
      "sql": "TRUNCATE TABLE t1, t2, t3",
      "expected": null
    },
    {
      "sql": "CREATE SEQUENCE seq",
      "expected": null
    },
    {
      "sql": "CREATE TEMPORARY SEQUENCE seq AS SMALLINT START WITH 3 INCREMENT BY 2 MINVALUE 1 MAXVALUE 10 CACHE 1 NO CYCLE OWNED BY table.col",
      "expected": null
    },
    {
      "sql": "CREATE SEQUENCE seq START WITH 1 NO MINVALUE NO MAXVALUE CYCLE NO CACHE",
      "expected": null
    },
    {
      "sql": "CREATE OR REPLACE TEMPORARY SEQUENCE seq INCREMENT BY 1 NO CYCLE",
      "expected": null
    },
    {
      "sql": "CREATE OR REPLACE SEQUENCE IF NOT EXISTS seq COMMENT='test comment' ORDER",
      "expected": null
    },
    {
      "sql": "CREATE SEQUENCE schema.seq SHARING=METADATA NOORDER NOKEEP SCALE EXTEND SHARD EXTEND SESSION",
      "expected": null
    },
    {
      "sql": "CREATE SEQUENCE schema.seq SHARING=DATA ORDER KEEP NOSCALE NOSHARD GLOBAL",
      "expected": null
    },
    {
      "sql": "CREATE SEQUENCE schema.seq SHARING=DATA NOCACHE NOCYCLE SCALE NOEXTEND",
      "expected": null
    },
    {
      "sql": "CREATE TEMPORARY SEQUENCE seq AS BIGINT INCREMENT BY 2 MINVALUE 1 CACHE 1 NOMAXVALUE NO CYCLE OWNED BY NONE",
      "expected": "CREATE TEMPORARY SEQUENCE seq AS BIGINT INCREMENT BY 2 MINVALUE 1 CACHE 1 NOMAXVALUE NO CYCLE"
    },
    {
      "sql": "CREATE TEMPORARY SEQUENCE seq START 1",
      "expected": "CREATE TEMPORARY SEQUENCE seq START WITH 1"
    },
    {
      "sql": "CREATE TEMPORARY SEQUENCE seq START WITH = 1 INCREMENT BY = 2",
      "expected": "CREATE TEMPORARY SEQUENCE seq START WITH 1 INCREMENT BY 2"
    },
    {
      "sql": "SELECT partition.d FROM t PARTITION (d)",
      "expected": "SELECT partition.d FROM t AS PARTITION(d)"
    },
    {
      "sql": "WITH sub_query AS (SELECT a FROM table) (SELECT a FROM sub_query)",
      "expected": "WITH sub_query AS (SELECT a FROM table) SELECT a FROM sub_query"
    },
    {
      "sql": "WITH sub_query AS (SELECT a FROM table) ((((SELECT a FROM sub_query))))",
      "expected": "WITH sub_query AS (SELECT a FROM table) SELECT a FROM sub_query"
    },
    {
      "sql": "1 OPERATOR(+) 2 OPERATOR(*) 3",
      "expected": null
    },
    {
      "sql": "SELECT operator FROM t",
      "expected": null
    },
    {
      "sql": "SELECT 1 OPERATOR(+) 2",
      "expected": null
    },
    {
      "sql": "SELECT 1 OPERATOR(+) /* foo */ 2",
      "expected": null
    },
    {
      "sql": "SELECT 1 OPERATOR(pg_catalog.+) 2",
      "expected": null
    },
    {
      "sql": "SELECT CAST('2001-02-17 08:38:40' AS TIMESTAMP) AT TIME ZONE 'UTC' AT TIME ZONE 'Asia/Tokyo'",
      "expected": null
    },
    {
      "sql": "SELECT CAST('2001-02-17 08:38:40' AS TIMESTAMP) AT TIME ZONE INTERVAL '3' HOURS AT TIME ZONE 'Asia/Tokyo'",
      "expected": null
    },
    {
      "sql": "CREATE TABLE foo (bar INT AS (foo))",
      "expected": null
    },
    {
      "sql": "CREATE TABLE foo (t1 INT, t2 INT, bar INT AS (t1 * t2 * 2))",
      "expected": null
    }
  ],
  "transpilation": [
    {
      "sql": "CAST(a AS TEXT)",
      "read": {},
      "write": {
        "bigquery": "CAST(a AS STRING)",
        "clickhouse": "CAST(a AS Nullable(String))",
        "drill": "CAST(a AS VARCHAR)",
        "duckdb": "CAST(a AS TEXT)",
        "materialize": "CAST(a AS TEXT)",
        "mysql": "CAST(a AS CHAR)",
        "hive": "CAST(a AS STRING)",
        "oracle": "CAST(a AS CLOB)",
        "postgres": "CAST(a AS TEXT)",
        "presto": "CAST(a AS VARCHAR)",
        "redshift": "CAST(a AS VARCHAR(MAX))",
        "snowflake": "CAST(a AS VARCHAR)",
        "spark": "CAST(a AS STRING)",
        "starrocks": "CAST(a AS STRING)",
        "tsql": "CAST(a AS VARCHAR(MAX))",
        "doris": "CAST(a AS STRING)"
      }
    },
    {
      "sql": "CAST(a AS BINARY(4))",
      "read": {},
      "write": {
        "bigquery": "CAST(a AS BYTES)",
        "clickhouse": "CAST(a AS Nullable(BINARY(4)))",
        "drill": "CAST(a AS VARBINARY(4))",
        "duckdb": "CAST(a AS BLOB(4))",
        "materialize": "CAST(a AS BYTEA(4))",
        "mysql": "CAST(a AS BINARY(4))",
        "hive": "CAST(a AS BINARY(4))",
        "oracle": "CAST(a AS BLOB(4))",
        "postgres": "CAST(a AS BYTEA(4))",
        "presto": "CAST(a AS VARBINARY(4))",
        "redshift": "CAST(a AS VARBYTE(4))",
        "snowflake": "CAST(a AS BINARY(4))",
        "sqlite": "CAST(a AS BLOB(4))",
        "spark": "CAST(a AS BINARY(4))",
        "starrocks": "CAST(a AS BINARY(4))"
      }
    },
    {
      "sql": "CAST(a AS VARBINARY(4))",
      "read": {},
      "write": {
        "bigquery": "CAST(a AS BYTES)",
        "clickhouse": "CAST(a AS Nullable(String))",
        "duckdb": "CAST(a AS BLOB(4))",
        "materialize": "CAST(a AS BYTEA(4))",
        "mysql": "CAST(a AS VARBINARY(4))",
        "hive": "CAST(a AS BINARY(4))",
        "oracle": "CAST(a AS BLOB(4))",
        "postgres": "CAST(a AS BYTEA(4))",
        "presto": "CAST(a AS VARBINARY(4))",
        "redshift": "CAST(a AS VARBYTE(4))",
        "snowflake": "CAST(a AS VARBINARY(4))",
        "sqlite": "CAST(a AS BLOB(4))",
        "spark": "CAST(a AS BINARY(4))",
        "starrocks": "CAST(a AS VARBINARY(4))"
      }
    },
    {
      "sql": "CAST(MAP('a', '1') AS MAP(TEXT, TEXT))",
      "read": {},
      "write": {
        "clickhouse": "CAST(map('a', '1') AS Map(String, Nullable(String)))"
      }
    },
    {
      "sql": "CAST(ARRAY(1, 2) AS ARRAY<TINYINT>)",
      "read": {},
      "write": {
        "clickhouse": "CAST([1, 2] AS Array(Nullable(Int8)))"
      }
    },
    {
      "sql": "CAST((1, 2, 3, 4) AS STRUCT<a: TINYINT, b: SMALLINT, c: INT, d: BIGINT>)",
      "read": {},
      "write": {
        "clickhouse": "CAST((1, 2, 3, 4) AS Tuple(a Nullable(Int8), b Nullable(Int16), c Nullable(Int32), d Nullable(Int64)))"
      }
    },
    {
      "sql": "CAST(a AS DATETIME)",
      "read": {},
      "write": {
        "postgres": "CAST(a AS TIMESTAMP)",
        "sqlite": "CAST(a AS DATETIME)"
      }
    },
    {
      "sql": "CAST(a AS STRING)",
      "read": {},
      "write": {
        "bigquery": "CAST(a AS STRING)",
        "drill": "CAST(a AS VARCHAR)",
        "duckdb": "CAST(a AS TEXT)",
        "materialize": "CAST(a AS TEXT)",
        "mysql": "CAST(a AS CHAR)",
        "hive": "CAST(a AS STRING)",
        "oracle": "CAST(a AS CLOB)",
        "postgres": "CAST(a AS TEXT)",
        "presto": "CAST(a AS VARCHAR)",
        "redshift": "CAST(a AS VARCHAR(MAX))",
        "snowflake": "CAST(a AS VARCHAR)",
        "spark": "CAST(a AS STRING)",
        "starrocks": "CAST(a AS STRING)",
        "tsql": "CAST(a AS VARCHAR(MAX))",
        "doris": "CAST(a AS STRING)"
      }
    },
    {
      "sql": "CAST(a AS VARCHAR)",
      "read": {},
      "write": {
        "bigquery": "CAST(a AS STRING)",
        "drill": "CAST(a AS VARCHAR)",
        "duckdb": "CAST(a AS TEXT)",
        "materialize": "CAST(a AS VARCHAR)",
        "mysql": "CAST(a AS CHAR)",
        "hive": "CAST(a AS STRING)",
        "oracle": "CAST(a AS VARCHAR2)",
        "postgres": "CAST(a AS VARCHAR)",
        "presto": "CAST(a AS VARCHAR)",
        "redshift": "CAST(a AS VARCHAR)",
        "snowflake": "CAST(a AS VARCHAR)",
        "spark": "CAST(a AS STRING)",
        "starrocks": "CAST(a AS VARCHAR)",
        "tsql": "CAST(a AS VARCHAR)",
        "doris": "CAST(a AS VARCHAR)"
      }
    },
    {
      "sql": "CAST(a AS VARCHAR(3))",
      "read": {},
      "write": {
        "bigquery": "CAST(a AS STRING)",
        "drill": "CAST(a AS VARCHAR(3))",
        "duckdb": "CAST(a AS TEXT(3))",
        "materialize": "CAST(a AS VARCHAR(3))",
        "mysql": "CAST(a AS CHAR(3))",
        "hive": "CAST(a AS VARCHAR(3))",
        "oracle": "CAST(a AS VARCHAR2(3))",
        "postgres": "CAST(a AS VARCHAR(3))",
        "presto": "CAST(a AS VARCHAR(3))",
        "redshift": "CAST(a AS VARCHAR(3))",
        "snowflake": "CAST(a AS VARCHAR(3))",
        "spark": "CAST(a AS VARCHAR(3))",
        "starrocks": "CAST(a AS VARCHAR(3))",
        "tsql": "CAST(a AS VARCHAR(3))",
        "doris": "CAST(a AS VARCHAR(3))"
      }
    },
    {
      "sql": "CAST(a AS CHARACTER VARYING)",
      "read": {},
      "write": {
        "bigquery": "CAST(a AS STRING)",
        "drill": "CAST(a AS VARCHAR)",
        "duckdb": "CAST(a AS TEXT)",
        "materialize": "CAST(a AS VARCHAR)",
        "mysql": "CAST(a AS CHAR)",
        "hive": "CAST(a AS STRING)",
        "oracle": "CAST(a AS VARCHAR2)",
        "postgres": "CAST(a AS VARCHAR)",
        "presto": "CAST(a AS VARCHAR)",
        "redshift": "CAST(a AS VARCHAR)",
        "snowflake": "CAST(a AS VARCHAR)",
        "spark": "CAST(a AS STRING)",
        "starrocks": "CAST(a AS VARCHAR)",
        "tsql": "CAST(a AS VARCHAR)",
        "doris": "CAST(a AS VARCHAR)"
      }
    },
    {
      "sql": "CAST(a AS CHARACTER VARYING(3))",
      "read": {},
      "write": {
        "bigquery": "CAST(a AS STRING)",
        "drill": "CAST(a AS VARCHAR(3))",
        "duckdb": "CAST(a AS TEXT(3))",
        "materialize": "CAST(a AS VARCHAR(3))",
        "mysql": "CAST(a AS CHAR(3))",
        "hive": "CAST(a AS VARCHAR(3))",
        "oracle": "CAST(a AS VARCHAR2(3))",
        "postgres": "CAST(a AS VARCHAR(3))",
        "presto": "CAST(a AS VARCHAR(3))",
        "redshift": "CAST(a AS VARCHAR(3))",
        "snowflake": "CAST(a AS VARCHAR(3))",
        "spark": "CAST(a AS VARCHAR(3))",
        "starrocks": "CAST(a AS VARCHAR(3))",
        "tsql": "CAST(a AS VARCHAR(3))",
        "doris": "CAST(a AS VARCHAR(3))"
      }
    },
    {
      "sql": "CAST(a AS SMALLINT)",
      "read": {},
      "write": {
        "bigquery": "CAST(a AS INT64)",
        "drill": "CAST(a AS INTEGER)",
        "duckdb": "CAST(a AS SMALLINT)",
        "materialize": "CAST(a AS SMALLINT)",
        "mysql": "CAST(a AS SIGNED)",
        "hive": "CAST(a AS SMALLINT)",
        "oracle": "CAST(a AS SMALLINT)",
        "postgres": "CAST(a AS SMALLINT)",
        "presto": "CAST(a AS SMALLINT)",
        "redshift": "CAST(a AS SMALLINT)",
        "snowflake": "CAST(a AS SMALLINT)",
        "spark": "CAST(a AS SMALLINT)",
        "sqlite": "CAST(a AS INTEGER)",
        "starrocks": "CAST(a AS SMALLINT)",
        "doris": "CAST(a AS SMALLINT)"
      }
    },
    {
      "sql": "CAST(a AS DOUBLE)",
      "read": {
        "postgres": "CAST(a AS DOUBLE PRECISION)",
        "redshift": "CAST(a AS DOUBLE PRECISION)"
      },
      "write": {
        "bigquery": "CAST(a AS FLOAT64)",
        "clickhouse": "CAST(a AS Nullable(Float64))",
        "doris": "CAST(a AS DOUBLE)",
        "drill": "CAST(a AS DOUBLE)",
        "duckdb": "CAST(a AS DOUBLE)",
        "materialize": "CAST(a AS DOUBLE PRECISION)",
        "mysql": "CAST(a AS DOUBLE)",
        "hive": "CAST(a AS DOUBLE)",
        "oracle": "CAST(a AS DOUBLE PRECISION)",
        "postgres": "CAST(a AS DOUBLE PRECISION)",
        "presto": "CAST(a AS DOUBLE)",
        "redshift": "CAST(a AS DOUBLE PRECISION)",
        "snowflake": "CAST(a AS DOUBLE)",
        "spark": "CAST(a AS DOUBLE)",
        "starrocks": "CAST(a AS DOUBLE)"
      }
    },
    {
      "sql": "CAST('1 DAY' AS INTERVAL)",
      "read": {},
      "write": {
        "postgres": "CAST('1 DAY' AS INTERVAL)",
        "redshift": "CAST('1 DAY' AS INTERVAL)"
      }
    },
    {
      "sql": "CAST(a AS TIMESTAMP)",
      "read": {},
      "write": {
        "starrocks": "CAST(a AS DATETIME)",
        "redshift": "CAST(a AS TIMESTAMP)",
        "doris": "CAST(a AS DATETIME)",
        "mysql": "CAST(a AS DATETIME)"
      }
    },
    {
      "sql": "CAST(a AS TIMESTAMPTZ)",
      "read": {},
      "write": {
        "starrocks": "TIMESTAMP(a)",
        "redshift": "CAST(a AS TIMESTAMP WITH TIME ZONE)",
        "doris": "CAST(a AS DATETIME)",
        "mysql": "TIMESTAMP(a)"
      }
    },
    {
      "sql": "CAST(a AS TINYINT)",
      "read": {},
      "write": {
        "oracle": "CAST(a AS SMALLINT)"
      }
    },
    {
      "sql": "CAST(a AS SMALLINT)",
      "read": {},
      "write": {
        "oracle": "CAST(a AS SMALLINT)"
      }
    },
    {
      "sql": "CAST(a AS BIGINT)",
      "read": {},
      "write": {
        "oracle": "CAST(a AS INT)"
      }
    },
    {
      "sql": "CAST(a AS INT)",
      "read": {},
      "write": {
        "oracle": "CAST(a AS INT)"
      }
    },
    {
      "sql": "CAST(a AS DECIMAL)",
      "read": {
        "oracle": "CAST(a AS NUMBER)"
      },
      "write": {
        "oracle": "CAST(a AS NUMBER)"
      }
    },
    {
      "sql": "CAST('127.0.0.1/32' AS INET)",
      "read": {
        "postgres": "INET '127.0.0.1/32'"
      },
      "write": {}
    },
    {
      "sql": "CREATE TABLE a LIKE b",
      "read": {},
      "write": {
        "bigquery": "CREATE TABLE a LIKE b",
        "clickhouse": "CREATE TABLE a AS b",
        "databricks": "CREATE TABLE a LIKE b",
        "doris": "CREATE TABLE a LIKE b",
        "drill": "CREATE TABLE a AS SELECT * FROM b LIMIT 0",
        "duckdb": "CREATE TABLE a AS SELECT * FROM b LIMIT 0",
        "hive": "CREATE TABLE a LIKE b",
        "mysql": "CREATE TABLE a LIKE b",
        "oracle": "CREATE TABLE a LIKE b",
        "postgres": "CREATE TABLE a (LIKE b)",
        "presto": "CREATE TABLE a (LIKE b)",
        "redshift": "CREATE TABLE a (LIKE b)",
        "snowflake": "CREATE TABLE a LIKE b",
        "spark": "CREATE TABLE a LIKE b",
        "sqlite": "CREATE TABLE a AS SELECT * FROM b LIMIT 0",
        "trino": "CREATE TABLE a (LIKE b)",
        "tsql": "SELECT TOP 0 * INTO a FROM b AS temp"
      }
    },
    {
      "sql": "SELECT DECODE(a, 1, 'one')",
      "read": {},
      "write": {
        "duckdb": "SELECT CASE WHEN a = 1 THEN 'one' END",
        "oracle": "SELECT DECODE(a, 1, 'one')",
        "redshift": "SELECT DECODE(a, 1, 'one')",
        "snowflake": "SELECT DECODE(a, 1, 'one')",
        "spark": "SELECT DECODE(a, 1, 'one')"
      }
    },
    {
      "sql": "SELECT DECODE(a, 1, 'one', 'default')",
      "read": {},
      "write": {
        "duckdb": "SELECT CASE WHEN a = 1 THEN 'one' ELSE 'default' END",
        "oracle": "SELECT DECODE(a, 1, 'one', 'default')",
        "redshift": "SELECT DECODE(a, 1, 'one', 'default')",
        "snowflake": "SELECT DECODE(a, 1, 'one', 'default')",
        "spark": "SELECT DECODE(a, 1, 'one', 'default')"
      }
    },
    {
      "sql": "SELECT DECODE(a, NULL, 'null')",
      "read": {},
      "write": {
        "duckdb": "SELECT CASE WHEN a IS NULL THEN 'null' END",
        "oracle": "SELECT DECODE(a, NULL, 'null')",
        "redshift": "SELECT DECODE(a, NULL, 'null')",
        "snowflake": "SELECT DECODE(a, NULL, 'null')",
        "spark": "SELECT DECODE(a, NULL, 'null')"
      }
    },
    {
      "sql": "SELECT DECODE(a, b, c)",
      "read": {},
      "write": {
        "duckdb": "SELECT CASE WHEN a = b OR (a IS NULL AND b IS NULL) THEN c END",
        "oracle": "SELECT DECODE(a, b, c)",
        "redshift": "SELECT DECODE(a, b, c)",
        "snowflake": "SELECT DECODE(a, b, c)",
        "spark": "SELECT DECODE(a, b, c)"
      }
    },
    {
      "sql": "SELECT DECODE(tbl.col, 'some_string', 'foo')",
      "read": {},
      "write": {
        "duckdb": "SELECT CASE WHEN tbl.col = 'some_string' THEN 'foo' END",
        "oracle": "SELECT DECODE(tbl.col, 'some_string', 'foo')",
        "redshift": "SELECT DECODE(tbl.col, 'some_string', 'foo')",
        "snowflake": "SELECT DECODE(tbl.col, 'some_string', 'foo')",
        "spark": "SELECT DECODE(tbl.col, 'some_string', 'foo')"
      }
    },
    {
      "sql": "TO_BINARY('1C')",
      "read": {
        "snowflake": "TO_BINARY('1C')",
        "starrocks": "TO_BINARY('1C')",
        "duckdb": "TO_BINARY('1C')",
        "spark": "TO_BINARY('1C')",
        "databricks": "TO_BINARY('1C')"
      },
      "write": {
        "snowflake": "TO_BINARY('1C')",
        "starrocks": "TO_BINARY('1C')",
        "duckdb": "TO_BINARY('1C')",
        "spark": "TO_BINARY('1C')",
        "databricks": "TO_BINARY('1C')"
      }
    },
    {
      "sql": "TO_BINARY('1C', 'HEX')",
      "read": {
        "snowflake": "TO_BINARY('1C', 'HEX')",
        "starrocks": "TO_BINARY('1C', 'HEX')",
        "spark": "TO_BINARY('1C', 'HEX')",
        "databricks": "TO_BINARY('1C', 'HEX')"
      },
      "write": {
        "snowflake": "TO_BINARY('1C', 'HEX')",
        "starrocks": "TO_BINARY('1C', 'HEX')",
        "spark": "TO_BINARY('1C', 'HEX')",
        "databricks": "TO_BINARY('1C', 'HEX')"
      }
    },
    {
      "sql": "SELECT IFNULL(1, NULL) FROM foo",
      "read": {},
      "write": {
        "redshift": "SELECT COALESCE(1, NULL) FROM foo",
        "postgres": "SELECT COALESCE(1, NULL) FROM foo",
        "mysql": "SELECT COALESCE(1, NULL) FROM foo",
        "duckdb": "SELECT COALESCE(1, NULL) FROM foo",
        "spark": "SELECT COALESCE(1, NULL) FROM foo",
        "bigquery": "SELECT COALESCE(1, NULL) FROM foo",
        "presto": "SELECT COALESCE(1, NULL) FROM foo"
      }
    },
    {
      "sql": "SELECT IS_ASCII(x)",
      "read": {},
      "write": {
        "sqlite": "SELECT (NOT x GLOB CAST(x'2a5b5e012d7f5d2a' AS TEXT))",
        "mysql": "SELECT REGEXP_LIKE(x, '^[[:ascii:]]*$')",
        "postgres": "SELECT (x ~ '^[[:ascii:]]*$')",
        "tsql": "SELECT (PATINDEX(CONVERT(VARCHAR(MAX), 0x255b5e002d7f5d25) COLLATE Latin1_General_BIN, x) = 0)",
        "oracle": "SELECT NVL(REGEXP_LIKE(x, '^[' || CHR(1) || '-' || CHR(127) || ']*$'), TRUE)"
      }
    },
    {
      "sql": "SELECT NVL2(a, b, c)",
      "read": {},
      "write": {
        "bigquery": "SELECT CASE WHEN NOT a IS NULL THEN b ELSE c END",
        "clickhouse": "SELECT CASE WHEN NOT (a IS NULL) THEN b ELSE c END",
        "databricks": "SELECT NVL2(a, b, c)",
        "doris": "SELECT CASE WHEN NOT a IS NULL THEN b ELSE c END",
        "dremio": "SELECT CASE WHEN NOT a IS NULL THEN b ELSE c END",
        "drill": "SELECT CASE WHEN NOT a IS NULL THEN b ELSE c END",
        "duckdb": "SELECT CASE WHEN NOT a IS NULL THEN b ELSE c END",
        "hive": "SELECT CASE WHEN NOT a IS NULL THEN b ELSE c END",
        "mysql": "SELECT CASE WHEN NOT a IS NULL THEN b ELSE c END",
        "oracle": "SELECT NVL2(a, b, c)",
        "postgres": "SELECT CASE WHEN NOT a IS NULL THEN b ELSE c END",
        "presto": "SELECT CASE WHEN NOT a IS NULL THEN b ELSE c END",
        "redshift": "SELECT NVL2(a, b, c)",
        "snowflake": "SELECT NVL2(a, b, c)",
        "spark": "SELECT NVL2(a, b, c)",
        "spark2": "SELECT NVL2(a, b, c)",
        "sqlite": "SELECT CASE WHEN NOT a IS NULL THEN b ELSE c END",
        "starrocks": "SELECT CASE WHEN NOT a IS NULL THEN b ELSE c END",
        "teradata": "SELECT NVL2(a, b, c)",
        "trino": "SELECT CASE WHEN NOT a IS NULL THEN b ELSE c END",
        "tsql": "SELECT CASE WHEN NOT a IS NULL THEN b ELSE c END"
      }
    },
    {
      "sql": "SELECT NVL2(a, b)",
      "read": {},
      "write": {
        "bigquery": "SELECT CASE WHEN NOT a IS NULL THEN b END",
        "clickhouse": "SELECT CASE WHEN NOT (a IS NULL) THEN b END",
        "databricks": "SELECT NVL2(a, b)",
        "doris": "SELECT CASE WHEN NOT a IS NULL THEN b END",
        "dremio": "SELECT CASE WHEN NOT a IS NULL THEN b END",
        "drill": "SELECT CASE WHEN NOT a IS NULL THEN b END",
        "duckdb": "SELECT CASE WHEN NOT a IS NULL THEN b END",
        "hive": "SELECT CASE WHEN NOT a IS NULL THEN b END",
        "mysql": "SELECT CASE WHEN NOT a IS NULL THEN b END",
        "oracle": "SELECT NVL2(a, b)",
        "postgres": "SELECT CASE WHEN NOT a IS NULL THEN b END",
        "presto": "SELECT CASE WHEN NOT a IS NULL THEN b END",
        "redshift": "SELECT NVL2(a, b)",
        "snowflake": "SELECT NVL2(a, b)",
        "spark": "SELECT NVL2(a, b)",
        "spark2": "SELECT NVL2(a, b)",
        "sqlite": "SELECT CASE WHEN NOT a IS NULL THEN b END",
        "starrocks": "SELECT CASE WHEN NOT a IS NULL THEN b END",
        "teradata": "SELECT NVL2(a, b)",
        "trino": "SELECT CASE WHEN NOT a IS NULL THEN b END",
        "tsql": "SELECT CASE WHEN NOT a IS NULL THEN b END"
      }
    },
    {
      "sql": "STR_TO_TIME(x, '%Y-%m-%dT%H:%M:%S')",
      "read": {
        "duckdb": "STRPTIME(x, '%Y-%m-%dT%H:%M:%S')"
      },
      "write": {
        "mysql": "STR_TO_DATE(x, '%Y-%m-%dT%T')",
        "duckdb": "STRPTIME(x, '%Y-%m-%dT%H:%M:%S')",
        "hive": "CAST(FROM_UNIXTIME(UNIX_TIMESTAMP(x, 'yyyy-MM-ddTHH:mm:ss')) AS TIMESTAMP)",
        "presto": "DATE_PARSE(x, '%Y-%m-%dT%T')",
        "drill": "TO_TIMESTAMP(x, 'yyyy-MM-dd''T''HH:mm:ss')",
        "redshift": "TO_TIMESTAMP(x, 'YYYY-MM-DDTHH24:MI:SS')",
        "spark": "TO_TIMESTAMP(x, 'yyyy-MM-ddTHH:mm:ss')"
      }
    },
    {
      "sql": "STR_TO_TIME('2020-01-01', '%Y-%m-%d')",
      "read": {},
      "write": {
        "drill": "TO_TIMESTAMP('2020-01-01', 'yyyy-MM-dd')",
        "duckdb": "STRPTIME('2020-01-01', '%Y-%m-%d')",
        "hive": "CAST('2020-01-01' AS TIMESTAMP)",
        "oracle": "TO_TIMESTAMP('2020-01-01', 'YYYY-MM-DD')",
        "postgres": "TO_TIMESTAMP('2020-01-01', 'YYYY-MM-DD')",
        "presto": "DATE_PARSE('2020-01-01', '%Y-%m-%d')",
        "redshift": "TO_TIMESTAMP('2020-01-01', 'YYYY-MM-DD')",
        "spark": "TO_TIMESTAMP('2020-01-01', 'yyyy-MM-dd')"
      }
    },
    {
      "sql": "STR_TO_TIME(x, '%y')",
      "read": {},
      "write": {
        "drill": "TO_TIMESTAMP(x, 'yy')",
        "duckdb": "STRPTIME(x, '%y')",
        "hive": "CAST(FROM_UNIXTIME(UNIX_TIMESTAMP(x, 'yy')) AS TIMESTAMP)",
        "materialize": "TO_TIMESTAMP(x, 'YY')",
        "presto": "DATE_PARSE(x, '%y')",
        "oracle": "TO_TIMESTAMP(x, 'YY')",
        "postgres": "TO_TIMESTAMP(x, 'YY')",
        "redshift": "TO_TIMESTAMP(x, 'YY')",
        "spark": "TO_TIMESTAMP(x, 'yy')"
      }
    },
    {
      "sql": "STR_TO_UNIX('2020-01-01', '%Y-%m-%d')",
      "read": {},
      "write": {
        "duckdb": "EPOCH(STRPTIME('2020-01-01', '%Y-%m-%d'))",
        "hive": "UNIX_TIMESTAMP('2020-01-01', 'yyyy-MM-dd')",
        "presto": "TO_UNIXTIME(COALESCE(TRY(DATE_PARSE(CAST('2020-01-01' AS VARCHAR), '%Y-%m-%d')), PARSE_DATETIME(DATE_FORMAT(CAST('2020-01-01' AS TIMESTAMP), '%Y-%m-%d'), 'yyyy-MM-dd')))",
        "starrocks": "UNIX_TIMESTAMP('2020-01-01', '%Y-%m-%d')",
        "doris": "UNIX_TIMESTAMP('2020-01-01', '%Y-%m-%d')"
      }
    },
    {
      "sql": "TIME_STR_TO_DATE('2020-01-01')",
      "read": {},
      "write": {
        "drill": "CAST('2020-01-01' AS DATE)",
        "duckdb": "CAST('2020-01-01' AS DATE)",
        "hive": "TO_DATE('2020-01-01')",
        "presto": "CAST('2020-01-01' AS TIMESTAMP)",
        "starrocks": "TO_DATE('2020-01-01')",
        "doris": "TO_DATE('2020-01-01')"
      }
    },
    {
      "sql": "TIME_STR_TO_TIME('2020-01-01')",
      "read": {},
      "write": {
        "bigquery": "CAST('2020-01-01' AS DATETIME)",
        "databricks": "CAST('2020-01-01' AS TIMESTAMP)",
        "duckdb": "CAST('2020-01-01' AS TIMESTAMP)",
        "tsql": "CAST('2020-01-01' AS DATETIME2)",
        "mysql": "CAST('2020-01-01' AS DATETIME)",
        "postgres": "CAST('2020-01-01' AS TIMESTAMP)",
        "redshift": "CAST('2020-01-01' AS TIMESTAMP)",
        "snowflake": "CAST('2020-01-01' AS TIMESTAMP)",
        "spark": "CAST('2020-01-01' AS TIMESTAMP)",
        "trino": "CAST('2020-01-01' AS TIMESTAMP)",
        "clickhouse": "CAST('2020-01-01' AS DateTime64(6))",
        "drill": "CAST('2020-01-01' AS TIMESTAMP)",
        "hive": "CAST('2020-01-01' AS TIMESTAMP)",
        "presto": "CAST('2020-01-01' AS TIMESTAMP)",
        "sqlite": "'2020-01-01'",
        "doris": "CAST('2020-01-01' AS DATETIME)"
      }
    },
    {
      "sql": "TIME_STR_TO_TIME('2020-01-01 12:13:14.123456+00:00')",
      "read": {},
      "write": {
        "mysql": "CAST('2020-01-01 12:13:14.123456+00:00' AS DATETIME(6))",
        "trino": "CAST('2020-01-01 12:13:14.123456+00:00' AS TIMESTAMP(6))",
        "presto": "CAST('2020-01-01 12:13:14.123456+00:00' AS TIMESTAMP)"
      }
    },
    {
      "sql": "TIME_STR_TO_TIME('2020-01-01 12:13:14.123-08:00', 'America/Los_Angeles')",
      "read": {},
      "write": {
        "mysql": "TIMESTAMP('2020-01-01 12:13:14.123-08:00')",
        "trino": "CAST('2020-01-01 12:13:14.123-08:00' AS TIMESTAMP(3) WITH TIME ZONE)",
        "presto": "CAST('2020-01-01 12:13:14.123-08:00' AS TIMESTAMP WITH TIME ZONE)"
      }
    },
    {
      "sql": "TIME_STR_TO_TIME('2020-01-01 12:13:14-08:00', 'America/Los_Angeles')",
      "read": {},
      "write": {
        "bigquery": "CAST('2020-01-01 12:13:14-08:00' AS TIMESTAMP)",
        "databricks": "CAST('2020-01-01 12:13:14-08:00' AS TIMESTAMP)",
        "duckdb": "CAST('2020-01-01 12:13:14-08:00' AS TIMESTAMPTZ)",
        "tsql": "CAST('2020-01-01 12:13:14-08:00' AS DATETIMEOFFSET) AT TIME ZONE 'UTC'",
        "mysql": "TIMESTAMP('2020-01-01 12:13:14-08:00')",
        "postgres": "CAST('2020-01-01 12:13:14-08:00' AS TIMESTAMPTZ)",
        "redshift": "CAST('2020-01-01 12:13:14-08:00' AS TIMESTAMP WITH TIME ZONE)",
        "snowflake": "CAST('2020-01-01 12:13:14-08:00' AS TIMESTAMPTZ)",
        "spark": "CAST('2020-01-01 12:13:14-08:00' AS TIMESTAMP)",
        "trino": "CAST('2020-01-01 12:13:14-08:00' AS TIMESTAMP WITH TIME ZONE)",
        "clickhouse": "CAST('2020-01-01 12:13:14' AS DateTime64(6, 'America/Los_Angeles'))",
        "drill": "CAST('2020-01-01 12:13:14-08:00' AS TIMESTAMP)",
        "hive": "CAST('2020-01-01 12:13:14-08:00' AS TIMESTAMP)",
        "presto": "CAST('2020-01-01 12:13:14-08:00' AS TIMESTAMP WITH TIME ZONE)",
        "sqlite": "'2020-01-01 12:13:14-08:00'",
        "doris": "CAST('2020-01-01 12:13:14-08:00' AS DATETIME)"
      }
    },
    {
      "sql": "TIME_STR_TO_TIME(col, 'America/Los_Angeles')",
      "read": {},
      "write": {
        "bigquery": "CAST(col AS TIMESTAMP)",
        "databricks": "CAST(col AS TIMESTAMP)",
        "duckdb": "CAST(col AS TIMESTAMPTZ)",
        "tsql": "CAST(col AS DATETIMEOFFSET) AT TIME ZONE 'UTC'",
        "mysql": "TIMESTAMP(col)",
        "postgres": "CAST(col AS TIMESTAMPTZ)",
        "redshift": "CAST(col AS TIMESTAMP WITH TIME ZONE)",
        "snowflake": "CAST(col AS TIMESTAMPTZ)",
        "spark": "CAST(col AS TIMESTAMP)",
        "trino": "CAST(col AS TIMESTAMP WITH TIME ZONE)",
        "clickhouse": "CAST(col AS DateTime64(6, 'America/Los_Angeles'))",
        "drill": "CAST(col AS TIMESTAMP)",
        "hive": "CAST(col AS TIMESTAMP)",
        "presto": "CAST(col AS TIMESTAMP WITH TIME ZONE)",
        "sqlite": "col",
        "doris": "CAST(col AS DATETIME)"
      }
    },
    {
      "sql": "TIME_STR_TO_UNIX('2020-01-01')",
      "read": {},
      "write": {
        "duckdb": "EPOCH(CAST('2020-01-01' AS TIMESTAMP))",
        "hive": "UNIX_TIMESTAMP('2020-01-01')",
        "mysql": "UNIX_TIMESTAMP('2020-01-01')",
        "presto": "TO_UNIXTIME(DATE_PARSE('2020-01-01', '%Y-%m-%d %T'))",
        "doris": "UNIX_TIMESTAMP('2020-01-01')"
      }
    },
    {
      "sql": "TIME_TO_STR(x, '%Y-%m-%d')",
      "read": {},
      "write": {
        "bigquery": "FORMAT_DATE('%F', x)",
        "drill": "TO_CHAR(x, 'yyyy-MM-dd')",
        "duckdb": "STRFTIME(x, '%Y-%m-%d')",
        "hive": "DATE_FORMAT(x, 'yyyy-MM-dd')",
        "materialize": "TO_CHAR(x, 'YYYY-MM-DD')",
        "oracle": "TO_CHAR(x, 'YYYY-MM-DD')",
        "postgres": "TO_CHAR(x, 'YYYY-MM-DD')",
        "presto": "DATE_FORMAT(x, '%Y-%m-%d')",
        "redshift": "TO_CHAR(x, 'YYYY-MM-DD')",
        "doris": "DATE_FORMAT(x, '%Y-%m-%d')"
      }
    },
    {
      "sql": "TIME_TO_STR(a, '%Y-%m-%d %H:%M:%S.%f')",
      "read": {},
      "write": {
        "redshift": "TO_CHAR(a, 'YYYY-MM-DD HH24:MI:SS.US')",
        "tsql": "FORMAT(a, 'yyyy-MM-dd HH:mm:ss.ffffff')"
      }
    },
    {
      "sql": "TIME_TO_TIME_STR(x)",
      "read": {},
      "write": {
        "drill": "CAST(x AS VARCHAR)",
        "duckdb": "CAST(x AS TEXT)",
        "hive": "CAST(x AS STRING)",
        "presto": "CAST(x AS VARCHAR)",
        "redshift": "CAST(x AS VARCHAR(MAX))",
        "doris": "CAST(x AS STRING)"
      }
    },
    {
      "sql": "TIME_TO_UNIX(x)",
      "read": {},
      "write": {
        "drill": "UNIX_TIMESTAMP(x)",
        "duckdb": "EPOCH(x)",
        "hive": "UNIX_TIMESTAMP(x)",
        "presto": "TO_UNIXTIME(x)",
        "doris": "UNIX_TIMESTAMP(x)"
      }
    },
    {
      "sql": "TS_OR_DS_TO_DATE_STR(x)",
      "read": {},
      "write": {
        "duckdb": "SUBSTRING(CAST(x AS TEXT), 1, 10)",
        "hive": "SUBSTRING(CAST(x AS STRING), 1, 10)",
        "presto": "SUBSTRING(CAST(x AS VARCHAR), 1, 10)",
        "doris": "SUBSTRING(CAST(x AS STRING), 1, 10)"
      }
    },
    {
      "sql": "TS_OR_DS_TO_DATE(x)",
      "read": {},
      "write": {
        "bigquery": "CAST(x AS DATE)",
        "duckdb": "CAST(x AS DATE)",
        "hive": "TO_DATE(x)",
        "materialize": "CAST(x AS DATE)",
        "postgres": "CAST(x AS DATE)",
        "presto": "CAST(CAST(x AS TIMESTAMP) AS DATE)",
        "snowflake": "TO_DATE(x)",
        "doris": "TO_DATE(x)",
        "mysql": "DATE(x)"
      }
    },
    {
      "sql": "TS_OR_DS_TO_DATE(x, '%-d')",
      "read": {},
      "write": {
        "duckdb": "CAST(STRPTIME(x, '%-d') AS DATE)",
        "hive": "TO_DATE(x, 'd')",
        "presto": "CAST(DATE_PARSE(x, '%e') AS DATE)",
        "spark": "TO_DATE(x, 'd')"
      }
    },
    {
      "sql": "UNIX_TO_STR(x, y)",
      "read": {},
      "write": {
        "duckdb": "STRFTIME(TO_TIMESTAMP(x), y)",
        "hive": "FROM_UNIXTIME(x, y)",
        "presto": "DATE_FORMAT(FROM_UNIXTIME(x), y)",
        "starrocks": "FROM_UNIXTIME(x, y)",
        "doris": "FROM_UNIXTIME(x, y)"
      }
    },
    {
      "sql": "UNIX_TO_TIME(x)",
      "read": {},
      "write": {
        "duckdb": "TO_TIMESTAMP(x)",
        "hive": "FROM_UNIXTIME(x)",
        "oracle": "TO_DATE('1970-01-01', 'YYYY-MM-DD') + (x / 86400)",
        "materialize": "TO_TIMESTAMP(x)",
        "postgres": "TO_TIMESTAMP(x)",
        "presto": "FROM_UNIXTIME(x)",
        "starrocks": "FROM_UNIXTIME(x)",
        "doris": "FROM_UNIXTIME(x)"
      }
    },
    {
      "sql": "UNIX_TO_TIME_STR(x)",
      "read": {},
      "write": {
        "duckdb": "CAST(TO_TIMESTAMP(x) AS TEXT)",
        "hive": "FROM_UNIXTIME(x)",
        "presto": "CAST(FROM_UNIXTIME(x) AS VARCHAR)"
      }
    },
    {
      "sql": "DATE_TO_DATE_STR(x)",
      "read": {},
      "write": {
        "drill": "CAST(x AS VARCHAR)",
        "duckdb": "CAST(x AS TEXT)",
        "hive": "CAST(x AS STRING)",
        "presto": "CAST(x AS VARCHAR)"
      }
    },
    {
      "sql": "DATE_TO_DI(x)",
      "read": {},
      "write": {
        "drill": "CAST(TO_DATE(x, 'yyyyMMdd') AS INT)",
        "duckdb": "CAST(STRFTIME(x, '%Y%m%d') AS INT)",
        "hive": "CAST(DATE_FORMAT(x, 'yyyyMMdd') AS INT)",
        "presto": "CAST(DATE_FORMAT(x, '%Y%m%d') AS INT)"
      }
    },
    {
      "sql": "DI_TO_DATE(x)",
      "read": {},
      "write": {
        "drill": "TO_DATE(CAST(x AS VARCHAR), 'yyyyMMdd')",
        "duckdb": "CAST(STRPTIME(CAST(x AS TEXT), '%Y%m%d') AS DATE)",
        "hive": "TO_DATE(CAST(x AS STRING), 'yyyyMMdd')",
        "presto": "CAST(DATE_PARSE(CAST(x AS VARCHAR), '%Y%m%d') AS DATE)"
      }
    },
    {
      "sql": "TS_OR_DI_TO_DI(x)",
      "read": {},
      "write": {
        "duckdb": "CAST(SUBSTR(REPLACE(CAST(x AS TEXT), '-', ''), 1, 8) AS INT)",
        "hive": "CAST(SUBSTR(REPLACE(CAST(x AS STRING), '-', ''), 1, 8) AS INT)",
        "presto": "CAST(SUBSTR(REPLACE(CAST(x AS VARCHAR), '-', ''), 1, 8) AS INT)",
        "spark": "CAST(SUBSTR(REPLACE(CAST(x AS STRING), '-', ''), 1, 8) AS INT)"
      }
    },
    {
      "sql": "DATE_ADD(x, 1, 'DAY')",
      "read": {
        "snowflake": "DATEADD('DAY', 1, x)",
        "dremio": "DATE_ADD(x, 1)"
      },
      "write": {
        "bigquery": "DATE_ADD(x, INTERVAL 1 DAY)",
        "drill": "DATE_ADD(x, INTERVAL 1 DAY)",
        "duckdb": "x + INTERVAL 1 DAY",
        "hive": "DATE_ADD(x, 1)",
        "materialize": "x + INTERVAL '1 DAY'",
        "mysql": "DATE_ADD(x, INTERVAL 1 DAY)",
        "postgres": "x + INTERVAL '1 DAY'",
        "presto": "DATE_ADD('DAY', 1, x)",
        "snowflake": "DATEADD(DAY, 1, x)",
        "spark": "DATE_ADD(x, 1)",
        "sqlite": "DATE(x, '1 DAY')",
        "starrocks": "DATE_ADD(x, INTERVAL 1 DAY)",
        "tsql": "DATEADD(DAY, 1, x)",
        "doris": "DATE_ADD(x, INTERVAL 1 DAY)",
        "dremio": "DATE_ADD(x, 1)"
      }
    },
    {
      "sql": "DATE_ADD(x, 1)",
      "read": {},
      "write": {
        "bigquery": "DATE_ADD(x, INTERVAL 1 DAY)",
        "drill": "DATE_ADD(x, INTERVAL 1 DAY)",
        "duckdb": "x + INTERVAL 1 DAY",
        "hive": "DATE_ADD(x, 1)",
        "mysql": "DATE_ADD(x, INTERVAL 1 DAY)",
        "presto": "DATE_ADD('DAY', 1, x)",
        "spark": "DATE_ADD(x, 1)",
        "starrocks": "DATE_ADD(x, INTERVAL 1 DAY)",
        "doris": "DATE_ADD(x, INTERVAL 1 DAY)",
        "dremio": "DATE_ADD(x, 1)"
      }
    },
    {
      "sql": "DATE_TRUNC('DAY', x)",
      "read": {
        "bigquery": "DATE_TRUNC(x, day)",
        "spark": "TRUNC(x, 'day')"
      },
      "write": {
        "bigquery": "DATE_TRUNC(x, DAY)",
        "duckdb": "DATE_TRUNC('DAY', x)",
        "mysql": "DATE(x)",
        "presto": "DATE_TRUNC('DAY', x)",
        "materialize": "DATE_TRUNC('DAY', x)",
        "postgres": "DATE_TRUNC('DAY', x)",
        "snowflake": "DATE_TRUNC('DAY', x)",
        "starrocks": "DATE_TRUNC('DAY', x)",
        "spark": "TRUNC(x, 'DAY')",
        "doris": "DATE_TRUNC(x, 'DAY')"
      }
    },
    {
      "sql": "TIMESTAMP_TRUNC(x, DAY)",
      "read": {
        "bigquery": "TIMESTAMP_TRUNC(x, day)",
        "duckdb": "DATE_TRUNC('day', x)",
        "materialize": "DATE_TRUNC('day', x)",
        "presto": "DATE_TRUNC('day', x)",
        "postgres": "DATE_TRUNC('day', x)",
        "snowflake": "DATE_TRUNC('day', x)",
        "starrocks": "DATE_TRUNC('day', x)",
        "spark": "DATE_TRUNC('day', x)",
        "doris": "DATE_TRUNC('day', x)"
      },
      "write": {}
    },
    {
      "sql": "DATE_TRUNC('DAY', CAST(x AS DATE))",
      "read": {
        "presto": "DATE_TRUNC('DAY', x::DATE)",
        "snowflake": "DATE_TRUNC('DAY', x::DATE)"
      },
      "write": {}
    },
    {
      "sql": "TIMESTAMP_TRUNC(CAST(x AS DATE), DAY)",
      "read": {
        "postgres": "DATE_TRUNC('day', x::DATE)"
      },
      "write": {}
    },
    {
      "sql": "TIMESTAMP_TRUNC(CAST(x AS DATE), DAY)",
      "read": {
        "starrocks": "DATE_TRUNC('day', x::DATE)"
      },
      "write": {}
    },
    {
      "sql": "DATE_TRUNC('week', x)",
      "read": {},
      "write": {
        "mysql": "STR_TO_DATE(CONCAT(YEAR(x), ' ', WEEK(x, 1), ' 1'), '%Y %u %w')"
      }
    },
    {
      "sql": "DATE_TRUNC('month', x)",
      "read": {},
      "write": {
        "mysql": "STR_TO_DATE(CONCAT(YEAR(x), ' ', MONTH(x), ' 1'), '%Y %c %e')"
      }
    },
    {
      "sql": "DATE_TRUNC('quarter', x)",
      "read": {},
      "write": {
        "mysql": "STR_TO_DATE(CONCAT(YEAR(x), ' ', QUARTER(x) * 3 - 2, ' 1'), '%Y %c %e')"
      }
    },
    {
      "sql": "DATE_TRUNC('year', x)",
      "read": {},
      "write": {
        "mysql": "STR_TO_DATE(CONCAT(YEAR(x), ' 1 1'), '%Y %c %e')"
      }
    },
    {
      "sql": "DATE_TRUNC('YEAR', x)",
      "read": {
        "bigquery": "DATE_TRUNC(x, year)",
        "spark": "TRUNC(x, 'year')"
      },
      "write": {
        "bigquery": "DATE_TRUNC(x, YEAR)",
        "materialize": "DATE_TRUNC('YEAR', x)",
        "mysql": "STR_TO_DATE(CONCAT(YEAR(x), ' 1 1'), '%Y %c %e')",
        "postgres": "DATE_TRUNC('YEAR', x)",
        "snowflake": "DATE_TRUNC('YEAR', x)",
        "starrocks": "DATE_TRUNC('YEAR', x)",
        "spark": "TRUNC(x, 'YEAR')",
        "doris": "DATE_TRUNC(x, 'YEAR')"
      }
    },
    {
      "sql": "TIMESTAMP_TRUNC(x, YEAR)",
      "read": {
        "bigquery": "TIMESTAMP_TRUNC(x, year)",
        "materialize": "DATE_TRUNC('YEAR', x)",
        "postgres": "DATE_TRUNC(year, x)",
        "spark": "DATE_TRUNC('year', x)",
        "snowflake": "DATE_TRUNC(year, x)",
        "starrocks": "DATE_TRUNC('year', x)"
      },
      "write": {
        "bigquery": "TIMESTAMP_TRUNC(x, YEAR)",
        "spark": "DATE_TRUNC('YEAR', x)",
        "doris": "DATE_TRUNC(x, 'YEAR')"
      }
    },
    {
      "sql": "NEXT_DAY(x, y)",
      "read": {},
      "write": {
        "snowflake": "NEXT_DAY(x, y)",
        "databricks": "NEXT_DAY(x, y)",
        "oracle": "NEXT_DAY(x, y)",
        "redshift": "NEXT_DAY(x, y)"
      }
    },
    {
      "sql": "STR_TO_DATE(x, '%Y-%m-%dT%H:%M:%S')",
      "read": {},
      "write": {
        "drill": "TO_DATE(x, 'yyyy-MM-dd''T''HH:mm:ss')",
        "mysql": "STR_TO_DATE(x, '%Y-%m-%dT%T')",
        "starrocks": "STR_TO_DATE(x, '%Y-%m-%dT%T')",
        "hive": "CAST(FROM_UNIXTIME(UNIX_TIMESTAMP(x, 'yyyy-MM-ddTHH:mm:ss')) AS DATE)",
        "presto": "CAST(DATE_PARSE(x, '%Y-%m-%dT%T') AS DATE)",
        "spark": "TO_DATE(x, 'yyyy-MM-ddTHH:mm:ss')",
        "doris": "STR_TO_DATE(x, '%Y-%m-%dT%T')"
      }
    },
    {
      "sql": "STR_TO_DATE(x, '%Y-%m-%d')",
      "read": {},
      "write": {
        "drill": "CAST(x AS DATE)",
        "mysql": "STR_TO_DATE(x, '%Y-%m-%d')",
        "starrocks": "STR_TO_DATE(x, '%Y-%m-%d')",
        "hive": "CAST(x AS DATE)",
        "presto": "CAST(DATE_PARSE(x, '%Y-%m-%d') AS DATE)",
        "spark": "TO_DATE(x)",
        "doris": "STR_TO_DATE(x, '%Y-%m-%d')"
      }
    },
    {
      "sql": "DATE_STR_TO_DATE(x)",
      "read": {},
      "write": {
        "drill": "CAST(x AS DATE)",
        "duckdb": "CAST(x AS DATE)",
        "hive": "CAST(x AS DATE)",
        "presto": "CAST(x AS DATE)",
        "spark": "CAST(x AS DATE)",
        "sqlite": "x",
        "tsql": "CAST(x AS DATE)"
      }
    },
    {
      "sql": "TS_OR_DS_ADD('2021-02-01', 1, 'DAY')",
      "read": {},
      "write": {
        "drill": "DATE_ADD(CAST('2021-02-01' AS DATE), INTERVAL 1 DAY)",
        "duckdb": "CAST('2021-02-01' AS DATE) + INTERVAL 1 DAY",
        "hive": "DATE_ADD('2021-02-01', 1)",
        "presto": "DATE_ADD('DAY', 1, CAST(CAST('2021-02-01' AS TIMESTAMP) AS DATE))",
        "spark": "DATE_ADD('2021-02-01', 1)",
        "mysql": "DATE_ADD('2021-02-01', INTERVAL 1 DAY)"
      }
    },
    {
      "sql": "TS_OR_DS_ADD(x, 1, 'DAY')",
      "read": {},
      "write": {
        "presto": "DATE_ADD('DAY', 1, CAST(CAST(x AS TIMESTAMP) AS DATE))",
        "hive": "DATE_ADD(x, 1)"
      }
    },
    {
      "sql": "TS_OR_DS_ADD(CURRENT_DATE, 1, 'DAY')",
      "read": {},
      "write": {
        "presto": "DATE_ADD('DAY', 1, CAST(CAST(CURRENT_DATE AS TIMESTAMP) AS DATE))",
        "hive": "DATE_ADD(CURRENT_DATE, 1)"
      }
    },
    {
      "sql": "DATE_ADD(CAST('2020-01-01' AS DATE), 1)",
      "read": {},
      "write": {
        "drill": "DATE_ADD(CAST('2020-01-01' AS DATE), INTERVAL 1 DAY)",
        "duckdb": "CAST('2020-01-01' AS DATE) + INTERVAL 1 DAY",
        "hive": "DATE_ADD(CAST('2020-01-01' AS DATE), 1)",
        "presto": "DATE_ADD('DAY', 1, CAST('2020-01-01' AS DATE))",
        "spark": "DATE_ADD(CAST('2020-01-01' AS DATE), 1)",
        "dremio": "DATE_ADD(CAST('2020-01-01' AS DATE), 1)"
      }
    },
    {
      "sql": "TIMESTAMP '2022-01-01'",
      "read": {},
      "write": {
        "drill": "CAST('2022-01-01' AS TIMESTAMP)",
        "mysql": "CAST('2022-01-01' AS DATETIME)",
        "starrocks": "CAST('2022-01-01' AS DATETIME)",
        "hive": "CAST('2022-01-01' AS TIMESTAMP)",
        "doris": "CAST('2022-01-01' AS DATETIME)"
      }
    },
    {
      "sql": "TIMESTAMP('2022-01-01')",
      "read": {},
      "write": {
        "mysql": "TIMESTAMP('2022-01-01')",
        "starrocks": "TIMESTAMP('2022-01-01')",
        "hive": "TIMESTAMP('2022-01-01')",
        "doris": "TIMESTAMP('2022-01-01')"
      }
    },
    {
      "sql": "TIMESTAMP_TRUNC(x, DAY, 'UTC')",
      "read": {},
      "write": {
        "duckdb": "DATE_TRUNC('DAY', x AT TIME ZONE 'UTC') AT TIME ZONE 'UTC'",
        "materialize": "DATE_TRUNC('DAY', x, 'UTC')",
        "presto": "DATE_TRUNC('DAY', x)",
        "postgres": "DATE_TRUNC('DAY', x, 'UTC')",
        "snowflake": "DATE_TRUNC('DAY', x)",
        "databricks": "DATE_TRUNC('DAY', x)",
        "clickhouse": "dateTrunc('DAY', x, 'UTC')"
      }
    },
    {
      "sql": "ARRAY(0, 1, 2)",
      "read": {},
      "write": {
        "bigquery": "[0, 1, 2]",
        "duckdb": "[0, 1, 2]",
        "presto": "ARRAY[0, 1, 2]",
        "spark": "ARRAY(0, 1, 2)"
      }
    },
    {
      "sql": "ARRAY_SIZE(x)",
      "read": {},
      "write": {
        "bigquery": "ARRAY_LENGTH(x)",
        "duckdb": "ARRAY_LENGTH(x)",
        "drill": "REPEATED_COUNT(x)",
        "presto": "CARDINALITY(x)",
        "spark": "SIZE(x)"
      }
    },
    {
      "sql": "ARRAY_SUM(ARRAY(1, 2))",
      "read": {},
      "write": {
        "trino": "REDUCE(ARRAY[1, 2], 0, (acc, x) -> acc + x, acc -> acc)",
        "duckdb": "LIST_SUM([1, 2])",
        "hive": "ARRAY_SUM(ARRAY(1, 2))",
        "presto": "ARRAY_SUM(ARRAY[1, 2])",
        "spark": "AGGREGATE(ARRAY(1, 2), 0, (acc, x) -> acc + x, acc -> acc)"
      }
    },
    {
      "sql": "REDUCE(x, 0, (acc, x) -> acc + x, acc -> acc)",
      "read": {},
      "write": {
        "trino": "REDUCE(x, 0, (acc, x) -> acc + x, acc -> acc)",
        "duckdb": "REDUCE(x, 0, (acc, x) -> acc + x, acc -> acc)",
        "hive": "REDUCE(x, 0, (acc, x) -> acc + x, acc -> acc)",
        "spark": "AGGREGATE(x, 0, (acc, x) -> acc + x, acc -> acc)",
        "presto": "REDUCE(x, 0, (acc, x) -> acc + x, acc -> acc)"
      }
    },
    {
      "sql": "ARRAY_INTERSECT(x, y)",
      "read": {
        "hive": "ARRAY_INTERSECT(x, y)",
        "spark2": "ARRAY_INTERSECT(x, y)",
        "spark": "ARRAY_INTERSECT(x, y)",
        "databricks": "ARRAY_INTERSECT(x, y)",
        "presto": "ARRAY_INTERSECT(x, y)",
        "trino": "ARRAY_INTERSECT(x, y)",
        "snowflake": "ARRAY_INTERSECTION(x, y)",
        "starrocks": "ARRAY_INTERSECT(x, y)"
      },
      "write": {
        "hive": "ARRAY_INTERSECT(x, y)",
        "spark2": "ARRAY_INTERSECT(x, y)",
        "spark": "ARRAY_INTERSECT(x, y)",
        "databricks": "ARRAY_INTERSECT(x, y)",
        "presto": "ARRAY_INTERSECT(x, y)",
        "trino": "ARRAY_INTERSECT(x, y)",
        "snowflake": "ARRAY_INTERSECTION(x, y)",
        "starrocks": "ARRAY_INTERSECT(x, y)"
      }
    },
    {
      "sql": "ARRAY_REVERSE(x)",
      "read": {
        "clickhouse": "arrayReverse(x)",
        "bigquery": "ARRAY_REVERSE(x)",
        "snowflake": "ARRAY_REVERSE(x)",
        "duckdb": "ARRAY_REVERSE(x)"
      },
      "write": {
        "clickhouse": "arrayReverse(x)",
        "bigquery": "ARRAY_REVERSE(x)",
        "snowflake": "ARRAY_REVERSE(x)",
        "duckdb": "ARRAY_REVERSE(x)"
      }
    },
    {
      "sql": "ARRAY_SLICE(x, 1, 3)",
      "read": {
        "clickhouse": "arraySlice(x, 1, 3)",
        "bigquery": "ARRAY_SLICE(x, 1, 3)",
        "snowflake": "ARRAY_SLICE(x, 1, 3)",
        "duckdb": "ARRAY_SLICE(x, 1, 3)",
        "spark2": "SLICE(x, 1, 3)",
        "spark": "SLICE(x, 1, 3)",
        "databricks": "SLICE(x, 1, 3)",
        "presto": "SLICE(x, 1, 3)",
        "trino": "SLICE(x, 1, 3)"
      },
      "write": {
        "clickhouse": "arraySlice(x, 1, 3)",
        "bigquery": "ARRAY_SLICE(x, 1, 3)",
        "snowflake": "ARRAY_SLICE(x, 1, 3)",
        "duckdb": "ARRAY_SLICE(x, 1, 3)",
        "spark2": "SLICE(x, 1, 3)",
        "spark": "SLICE(x, 1, 3)",
        "databricks": "SLICE(x, 1, 3)",
        "presto": "SLICE(x, 1, 3)",
        "trino": "SLICE(x, 1, 3)"
      }
    },
    {
      "sql": "SORT_ARRAY(x)",
      "read": {},
      "write": {
        "duckdb": "ARRAY_SORT(x)",
        "hive": "SORT_ARRAY(x)",
        "presto": "ARRAY_SORT(x)",
        "snowflake": "ARRAY_SORT(x)",
        "spark": "SORT_ARRAY(x)"
      }
    },
    {
      "sql": "ARRAY_PREPEND(arr, x)",
      "read": {
        "duckdb": "LIST_PREPEND(x, arr)",
        "postgres": "ARRAY_PREPEND(x, arr)"
      },
      "write": {
        "duckdb": "LIST_PREPEND(x, arr)",
        "postgres": "ARRAY_PREPEND(x, arr)"
      }
    },
    {
      "sql": "ARRAY_APPEND(arr, x)",
      "read": {
        "duckdb": "LIST_APPEND(arr, x)",
        "postgres": "ARRAY_APPEND(arr, x)"
      },
      "write": {
        "duckdb": "LIST_APPEND(arr, x)",
        "postgres": "ARRAY_APPEND(arr, x)"
      }
    },
    {
      "sql": "SELECT fname, lname, age FROM person ORDER BY age DESC NULLS FIRST, fname ASC NULLS LAST, lname",
      "read": {},
      "write": {
        "bigquery": "SELECT fname, lname, age FROM person ORDER BY age DESC NULLS FIRST, fname ASC NULLS LAST, lname",
        "duckdb": "SELECT fname, lname, age FROM person ORDER BY age DESC NULLS FIRST, fname ASC, lname NULLS FIRST",
        "presto": "SELECT fname, lname, age FROM person ORDER BY age DESC NULLS FIRST, fname ASC, lname NULLS FIRST",
        "hive": "SELECT fname, lname, age FROM person ORDER BY age DESC NULLS FIRST, fname ASC NULLS LAST, lname",
        "spark": "SELECT fname, lname, age FROM person ORDER BY age DESC NULLS FIRST, fname ASC NULLS LAST, lname"
      }
    },
    {
      "sql": "JSON_EXTRACT(x, '$[\"a b\"]')",
      "read": {},
      "write": {
        "bigquery": "JSON_EXTRACT(x, '$[\\'a b\\']')",
        "clickhouse": "JSONExtractString(x, 'a b')",
        "duckdb": "x -> '$.\"a b\"'",
        "mysql": "JSON_EXTRACT(x, '$.\"a b\"')",
        "postgres": "JSON_EXTRACT_PATH(x, 'a b')",
        "presto": "JSON_EXTRACT(x, '$[\"a b\"]')",
        "redshift": "JSON_EXTRACT_PATH_TEXT(x, 'a b')",
        "snowflake": "GET_PATH(PARSE_JSON(x), '[\"a b\"]')",
        "spark": "GET_JSON_OBJECT(x, '$[\\'a b\\']')",
        "sqlite": "x -> '$.\"a b\"'",
        "trino": "JSON_EXTRACT(x, '$[\"a b\"]')",
        "tsql": "ISNULL(JSON_QUERY(x, '$.\"a b\"'), JSON_VALUE(x, '$.\"a b\"'))"
      }
    },
    {
      "sql": "JSON_EXTRACT(x, '$.y')",
      "read": {
        "bigquery": "JSON_EXTRACT(x, '$.y')",
        "duckdb": "x -> 'y'",
        "doris": "JSON_EXTRACT(x, '$.y')",
        "mysql": "JSON_EXTRACT(x, '$.y')",
        "postgres": "x->'y'",
        "presto": "JSON_EXTRACT(x, '$.y')",
        "snowflake": "GET_PATH(x, 'y')",
        "sqlite": "x -> '$.y'",
        "starrocks": "x -> '$.y'"
      },
      "write": {
        "bigquery": "JSON_EXTRACT(x, '$.y')",
        "clickhouse": "JSONExtractString(x, 'y')",
        "doris": "JSON_EXTRACT(x, '$.y')",
        "duckdb": "x -> '$.y'",
        "mysql": "JSON_EXTRACT(x, '$.y')",
        "oracle": "JSON_EXTRACT(x, '$.y')",
        "postgres": "JSON_EXTRACT_PATH(x, 'y')",
        "presto": "JSON_EXTRACT(x, '$.y')",
        "snowflake": "GET_PATH(PARSE_JSON(x), 'y')",
        "spark": "GET_JSON_OBJECT(x, '$.y')",
        "sqlite": "x -> '$.y'",
        "starrocks": "x -> '$.y'",
        "tsql": "ISNULL(JSON_QUERY(x, '$.y'), JSON_VALUE(x, '$.y'))"
      }
    },
    {
      "sql": "JSON_EXTRACT_SCALAR(x, '$.y')",
      "read": {
        "bigquery": "JSON_EXTRACT_SCALAR(x, '$.y')",
        "clickhouse": "JSONExtractString(x, 'y')",
        "duckdb": "x ->> 'y'",
        "postgres": "x ->> 'y'",
        "presto": "JSON_EXTRACT_SCALAR(x, '$.y')",
        "redshift": "JSON_EXTRACT_PATH_TEXT(x, 'y')",
        "spark": "GET_JSON_OBJECT(x, '$.y')",
        "snowflake": "JSON_EXTRACT_PATH_TEXT(x, 'y')",
        "sqlite": "x ->> '$.y'"
      },
      "write": {
        "bigquery": "JSON_EXTRACT_SCALAR(x, '$.y')",
        "clickhouse": "JSONExtractString(x, 'y')",
        "duckdb": "x ->> '$.y'",
        "postgres": "JSON_EXTRACT_PATH_TEXT(x, 'y')",
        "presto": "JSON_EXTRACT_SCALAR(x, '$.y')",
        "redshift": "JSON_EXTRACT_PATH_TEXT(x, 'y')",
        "snowflake": "JSON_EXTRACT_PATH_TEXT(x, 'y')",
        "spark": "GET_JSON_OBJECT(x, '$.y')",
        "sqlite": "x ->> '$.y'",
        "tsql": "ISNULL(JSON_QUERY(x, '$.y'), JSON_VALUE(x, '$.y'))"
      }
    },
    {
      "sql": "JSON_EXTRACT(x, '$.y[0].z')",
      "read": {
        "bigquery": "JSON_EXTRACT(x, '$.y[0].z')",
        "duckdb": "x -> '$.y[0].z'",
        "doris": "JSON_EXTRACT(x, '$.y[0].z')",
        "mysql": "JSON_EXTRACT(x, '$.y[0].z')",
        "presto": "JSON_EXTRACT(x, '$.y[0].z')",
        "snowflake": "GET_PATH(x, 'y[0].z')",
        "sqlite": "x -> '$.y[0].z'",
        "starrocks": "x -> '$.y[0].z'"
      },
      "write": {
        "bigquery": "JSON_EXTRACT(x, '$.y[0].z')",
        "clickhouse": "JSONExtractString(x, 'y', 1, 'z')",
        "doris": "JSON_EXTRACT(x, '$.y[0].z')",
        "duckdb": "x -> '$.y[0].z'",
        "mysql": "JSON_EXTRACT(x, '$.y[0].z')",
        "oracle": "JSON_EXTRACT(x, '$.y[0].z')",
        "postgres": "JSON_EXTRACT_PATH(x, 'y', '0', 'z')",
        "presto": "JSON_EXTRACT(x, '$.y[0].z')",
        "redshift": "JSON_EXTRACT_PATH_TEXT(x, 'y', '0', 'z')",
        "snowflake": "GET_PATH(PARSE_JSON(x), 'y[0].z')",
        "spark": "GET_JSON_OBJECT(x, '$.y[0].z')",
        "sqlite": "x -> '$.y[0].z'",
        "starrocks": "x -> '$.y[0].z'",
        "tsql": "ISNULL(JSON_QUERY(x, '$.y[0].z'), JSON_VALUE(x, '$.y[0].z'))"
      }
    },
    {
      "sql": "JSON_EXTRACT_SCALAR(x, '$.y[0].z')",
      "read": {
        "bigquery": "JSON_EXTRACT_SCALAR(x, '$.y[0].z')",
        "clickhouse": "JSONExtractString(x, 'y', 1, 'z')",
        "duckdb": "x ->> '$.y[0].z'",
        "presto": "JSON_EXTRACT_SCALAR(x, '$.y[0].z')",
        "snowflake": "JSON_EXTRACT_PATH_TEXT(x, 'y[0].z')",
        "spark": "GET_JSON_OBJECT(x, \"$.y[0].z\")",
        "sqlite": "x ->> '$.y[0].z'"
      },
      "write": {
        "bigquery": "JSON_EXTRACT_SCALAR(x, '$.y[0].z')",
        "clickhouse": "JSONExtractString(x, 'y', 1, 'z')",
        "duckdb": "x ->> '$.y[0].z'",
        "postgres": "JSON_EXTRACT_PATH_TEXT(x, 'y', '0', 'z')",
        "presto": "JSON_EXTRACT_SCALAR(x, '$.y[0].z')",
        "redshift": "JSON_EXTRACT_PATH_TEXT(x, 'y', '0', 'z')",
        "snowflake": "JSON_EXTRACT_PATH_TEXT(x, 'y[0].z')",
        "spark": "GET_JSON_OBJECT(x, '$.y[0].z')",
        "sqlite": "x ->> '$.y[0].z'",
        "tsql": "ISNULL(JSON_QUERY(x, '$.y[0].z'), JSON_VALUE(x, '$.y[0].z'))"
      }
    },
    {
      "sql": "JSON_EXTRACT(x, '$.y[*]')",
      "read": {},
      "write": {
        "duckdb": "x -> '$.y[*]'",
        "mysql": "JSON_EXTRACT(x, '$.y[*]')",
        "presto": "JSON_EXTRACT(x, '$.y[*]')",
        "spark": "GET_JSON_OBJECT(x, '$.y[*]')"
      }
    },
    {
      "sql": "JSON_EXTRACT(x, '$.y[*]')",
      "read": {},
      "write": {
        "bigquery": "JSON_EXTRACT(x, '$.y')",
        "clickhouse": "JSONExtractString(x, 'y')",
        "postgres": "JSON_EXTRACT_PATH(x, 'y')",
        "redshift": "JSON_EXTRACT_PATH_TEXT(x, 'y')",
        "snowflake": "GET_PATH(PARSE_JSON(x), 'y')",
        "sqlite": "x -> '$.y'",
        "tsql": "ISNULL(JSON_QUERY(x, '$.y'), JSON_VALUE(x, '$.y'))"
      }
    },
    {
      "sql": "JSON_EXTRACT(x, '$.y.*')",
      "read": {},
      "write": {
        "duckdb": "x -> '$.y.*'",
        "mysql": "JSON_EXTRACT(x, '$.y.*')",
        "presto": "JSON_EXTRACT(x, '$.y.*')"
      }
    },
    {
      "sql": "SELECT a FROM x CROSS JOIN UNNEST(y) AS t (a)",
      "read": {},
      "write": {
        "drill": "SELECT a FROM x CROSS JOIN UNNEST(y) AS t(a)",
        "presto": "SELECT a FROM x CROSS JOIN UNNEST(y) AS t(a)",
        "spark": "SELECT a FROM x LATERAL VIEW EXPLODE(y) t AS a"
      }
    },
    {
      "sql": "SELECT a, b FROM x CROSS JOIN UNNEST(y, z) AS t (a, b)",
      "read": {},
      "write": {
        "drill": "SELECT a, b FROM x CROSS JOIN UNNEST(y, z) AS t(a, b)",
        "presto": "SELECT a, b FROM x CROSS JOIN UNNEST(y, z) AS t(a, b)",
        "spark": "SELECT a, b FROM x LATERAL VIEW INLINE(ARRAYS_ZIP(y, z)) t AS a, b"
      }
    },
    {
      "sql": "SELECT a FROM x CROSS JOIN UNNEST(y) WITH ORDINALITY AS t (a)",
      "read": {},
      "write": {
        "presto": "SELECT a FROM x CROSS JOIN UNNEST(y) WITH ORDINALITY AS t(a)",
        "spark2": "SELECT a FROM x LATERAL VIEW POSEXPLODE(y) t AS pos, a",
        "spark": "SELECT a FROM x LATERAL VIEW POSEXPLODE(y) t AS pos, a",
        "databricks": "SELECT a FROM x LATERAL VIEW POSEXPLODE(y) t AS pos, a"
      }
    },
    {
      "sql": "SELECT * FROM x CROSS JOIN UNNEST(y) AS t",
      "read": {},
      "write": {
        "presto": "SELECT * FROM x CROSS JOIN UNNEST(y) AS t"
      }
    },
    {
      "sql": "SELECT a, b FROM x CROSS JOIN UNNEST(y) AS t (a, b)",
      "read": {},
      "write": {
        "presto": "SELECT a, b FROM x CROSS JOIN UNNEST(y) AS t(a, b)",
        "spark": "SELECT a, b FROM x LATERAL VIEW EXPLODE(y) t AS a, b",
        "hive": "SELECT a, b FROM x LATERAL VIEW EXPLODE(y) t AS a, b"
      }
    },
    {
      "sql": "SELECT numbers, animals, n, a FROM (SELECT ARRAY(2, 5) AS numbers, ARRAY('dog', 'cat', 'bird') AS animals UNION ALL SELECT ARRAY(7, 8, 9), ARRAY('cow', 'pig')) AS x CROSS JOIN UNNEST(numbers, animals) AS t(n, a)",
      "read": {},
      "write": {
        "presto": "SELECT numbers, animals, n, a FROM (SELECT ARRAY[2, 5] AS numbers, ARRAY['dog', 'cat', 'bird'] AS animals UNION ALL SELECT ARRAY[7, 8, 9], ARRAY['cow', 'pig']) AS x CROSS JOIN UNNEST(numbers, animals) AS t(n, a)",
        "spark": "SELECT numbers, animals, n, a FROM (SELECT ARRAY(2, 5) AS numbers, ARRAY('dog', 'cat', 'bird') AS animals UNION ALL SELECT ARRAY(7, 8, 9), ARRAY('cow', 'pig')) AS x LATERAL VIEW INLINE(ARRAYS_ZIP(numbers, animals)) t AS n, a"
      }
    },
    {
      "sql": "SELECT a, b, c, d, e FROM x CROSS JOIN UNNEST(y) AS t(a, b, c, d)",
      "read": {},
      "write": {
        "presto": "SELECT a, b, c, d, e FROM x CROSS JOIN UNNEST(y) AS t(a, b, c, d)"
      }
    },
    {
      "sql": "SELECT * FROM x CROSS JOIN UNNEST(a) AS j(lista) CROSS JOIN UNNEST(b) AS k(listb) CROSS JOIN UNNEST(c) AS l(listc)",
      "read": {},
      "write": {
        "presto": "SELECT * FROM x CROSS JOIN UNNEST(a) AS j(lista) CROSS JOIN UNNEST(b) AS k(listb) CROSS JOIN UNNEST(c) AS l(listc)",
        "spark": "SELECT * FROM x LATERAL VIEW EXPLODE(a) j AS lista LATERAL VIEW EXPLODE(b) k AS listb LATERAL VIEW EXPLODE(c) l AS listc",
        "hive": "SELECT * FROM x LATERAL VIEW EXPLODE(a) j AS lista LATERAL VIEW EXPLODE(b) k AS listb LATERAL VIEW EXPLODE(c) l AS listc"
      }
    },
    {
      "sql": "SELECT * FROM a UNION SELECT * FROM b ORDER BY x LIMIT 1",
      "read": {},
      "write": {
        "clickhouse": "SELECT * FROM (SELECT * FROM a UNION DISTINCT SELECT * FROM b) AS _l_0 ORDER BY x NULLS FIRST LIMIT 1",
        "tsql": "SELECT TOP 1 * FROM (SELECT * FROM a UNION SELECT * FROM b) AS _l_0 ORDER BY x"
      }
    },
    {
      "sql": "SELECT * FROM a UNION SELECT * FROM b",
      "read": {
        "bigquery": "SELECT * FROM a UNION DISTINCT SELECT * FROM b",
        "clickhouse": "SELECT * FROM a UNION DISTINCT SELECT * FROM b",
        "duckdb": "SELECT * FROM a UNION SELECT * FROM b",
        "presto": "SELECT * FROM a UNION SELECT * FROM b",
        "spark": "SELECT * FROM a UNION SELECT * FROM b"
      },
      "write": {
        "bigquery": "SELECT * FROM a UNION DISTINCT SELECT * FROM b",
        "drill": "SELECT * FROM a UNION SELECT * FROM b",
        "duckdb": "SELECT * FROM a UNION SELECT * FROM b",
        "presto": "SELECT * FROM a UNION SELECT * FROM b",
        "spark": "SELECT * FROM a UNION SELECT * FROM b"
      }
    },
    {
      "sql": "SELECT * FROM a UNION ALL SELECT * FROM b",
      "read": {
        "bigquery": "SELECT * FROM a UNION ALL SELECT * FROM b",
        "clickhouse": "SELECT * FROM a UNION ALL SELECT * FROM b",
        "duckdb": "SELECT * FROM a UNION ALL SELECT * FROM b",
        "presto": "SELECT * FROM a UNION ALL SELECT * FROM b",
        "spark": "SELECT * FROM a UNION ALL SELECT * FROM b"
      },
      "write": {
        "bigquery": "SELECT * FROM a UNION ALL SELECT * FROM b",
        "duckdb": "SELECT * FROM a UNION ALL SELECT * FROM b",
        "presto": "SELECT * FROM a UNION ALL SELECT * FROM b",
        "spark": "SELECT * FROM a UNION ALL SELECT * FROM b"
      }
    },
    {
      "sql": "SELECT * FROM a INTERSECT SELECT * FROM b",
      "read": {
        "bigquery": "SELECT * FROM a INTERSECT DISTINCT SELECT * FROM b",
        "clickhouse": "SELECT * FROM a INTERSECT DISTINCT SELECT * FROM b",
        "duckdb": "SELECT * FROM a INTERSECT SELECT * FROM b",
        "presto": "SELECT * FROM a INTERSECT SELECT * FROM b",
        "spark": "SELECT * FROM a INTERSECT SELECT * FROM b"
      },
      "write": {
        "bigquery": "SELECT * FROM a INTERSECT DISTINCT SELECT * FROM b",
        "clickhouse": "SELECT * FROM a INTERSECT DISTINCT SELECT * FROM b",
        "duckdb": "SELECT * FROM a INTERSECT SELECT * FROM b",
        "presto": "SELECT * FROM a INTERSECT SELECT * FROM b",
        "spark": "SELECT * FROM a INTERSECT SELECT * FROM b"
      }
    },
    {
      "sql": "SELECT * FROM a EXCEPT SELECT * FROM b",
      "read": {
        "bigquery": "SELECT * FROM a EXCEPT DISTINCT SELECT * FROM b",
        "clickhouse": "SELECT * FROM a EXCEPT DISTINCT SELECT * FROM b",
        "duckdb": "SELECT * FROM a EXCEPT SELECT * FROM b",
        "presto": "SELECT * FROM a EXCEPT SELECT * FROM b",
        "spark": "SELECT * FROM a EXCEPT SELECT * FROM b"
      },
      "write": {
        "bigquery": "SELECT * FROM a EXCEPT DISTINCT SELECT * FROM b",
        "clickhouse": "SELECT * FROM a EXCEPT DISTINCT SELECT * FROM b",
        "duckdb": "SELECT * FROM a EXCEPT SELECT * FROM b",
        "presto": "SELECT * FROM a EXCEPT SELECT * FROM b",
        "spark": "SELECT * FROM a EXCEPT SELECT * FROM b"
      }
    },
    {
      "sql": "SELECT * FROM a UNION DISTINCT SELECT * FROM b",
      "read": {},
      "write": {
        "bigquery": "SELECT * FROM a UNION DISTINCT SELECT * FROM b",
        "duckdb": "SELECT * FROM a UNION SELECT * FROM b",
        "presto": "SELECT * FROM a UNION SELECT * FROM b",
        "spark": "SELECT * FROM a UNION SELECT * FROM b"
      }
    },
    {
      "sql": "SELECT * FROM a INTERSECT DISTINCT SELECT * FROM b",
      "read": {},
      "write": {
        "bigquery": "SELECT * FROM a INTERSECT DISTINCT SELECT * FROM b",
        "clickhouse": "SELECT * FROM a INTERSECT DISTINCT SELECT * FROM b",
        "duckdb": "SELECT * FROM a INTERSECT SELECT * FROM b",
        "presto": "SELECT * FROM a INTERSECT SELECT * FROM b",
        "spark": "SELECT * FROM a INTERSECT SELECT * FROM b"
      }
    },
    {
      "sql": "SELECT * FROM a INTERSECT ALL SELECT * FROM b",
      "read": {},
      "write": {
        "bigquery": "SELECT * FROM a INTERSECT ALL SELECT * FROM b",
        "clickhouse": "SELECT * FROM a INTERSECT SELECT * FROM b",
        "duckdb": "SELECT * FROM a INTERSECT ALL SELECT * FROM b",
        "presto": "SELECT * FROM a INTERSECT ALL SELECT * FROM b",
        "spark": "SELECT * FROM a INTERSECT ALL SELECT * FROM b"
      }
    },
    {
      "sql": "SELECT * FROM a EXCEPT DISTINCT SELECT * FROM b",
      "read": {},
      "write": {
        "bigquery": "SELECT * FROM a EXCEPT DISTINCT SELECT * FROM b",
        "clickhouse": "SELECT * FROM a EXCEPT DISTINCT SELECT * FROM b",
        "duckdb": "SELECT * FROM a EXCEPT SELECT * FROM b",
        "presto": "SELECT * FROM a EXCEPT SELECT * FROM b",
        "spark": "SELECT * FROM a EXCEPT SELECT * FROM b"
      }
    },
    {
      "sql": "SELECT * FROM a EXCEPT ALL SELECT * FROM b",
      "read": {
        "bigquery": "SELECT * FROM a EXCEPT ALL SELECT * FROM b",
        "clickhouse": "SELECT * FROM a EXCEPT ALL SELECT * FROM b",
        "duckdb": "SELECT * FROM a EXCEPT ALL SELECT * FROM b",
        "presto": "SELECT * FROM a EXCEPT ALL SELECT * FROM b",
        "spark": "SELECT * FROM a EXCEPT ALL SELECT * FROM b"
      },
      "write": {}
    },
    {
      "sql": "CONCAT(a, b, c)",
      "read": {},
      "write": {
        "redshift": "a || b || c",
        "sqlite": "a || b || c"
      }
    },
    {
      "sql": "x ILIKE '%y'",
      "read": {
        "clickhouse": "x ILIKE '%y'",
        "duckdb": "x ILIKE '%y'",
        "postgres": "x ILIKE '%y'",
        "snowflake": "x ILIKE '%y'"
      },
      "write": {
        "bigquery": "LOWER(x) LIKE LOWER('%y')",
        "clickhouse": "x ILIKE '%y'",
        "drill": "x `ILIKE` '%y'",
        "duckdb": "x ILIKE '%y'",
        "hive": "LOWER(x) LIKE LOWER('%y')",
        "mysql": "LOWER(x) LIKE LOWER('%y')",
        "oracle": "LOWER(x) LIKE LOWER('%y')",
        "postgres": "x ILIKE '%y'",
        "presto": "LOWER(x) LIKE LOWER('%y')",
        "snowflake": "x ILIKE '%y'",
        "spark": "x ILIKE '%y'",
        "sqlite": "LOWER(x) LIKE LOWER('%y')",
        "starrocks": "LOWER(x) LIKE LOWER('%y')",
        "trino": "LOWER(x) LIKE LOWER('%y')",
        "doris": "LOWER(x) LIKE LOWER('%y')"
      }
    },
    {
      "sql": "STR_POSITION(haystack, needle)",
      "read": {
        "athena": "POSITION(needle in haystack)",
        "clickhouse": "POSITION(needle in haystack)",
        "databricks": "POSITION(needle in haystack)",
        "drill": "POSITION(needle in haystack)",
        "duckdb": "POSITION(needle in haystack)",
        "materialize": "POSITION(needle in haystack)",
        "mysql": "POSITION(needle in haystack)",
        "postgres": "POSITION(needle in haystack)",
        "presto": "POSITION(needle in haystack)",
        "redshift": "POSITION(needle in haystack)",
        "risingwave": "POSITION(needle in haystack)",
        "snowflake": "POSITION(needle in haystack)",
        "spark": "POSITION(needle in haystack)",
        "spark2": "POSITION(needle in haystack)",
        "teradata": "POSITION(needle in haystack)",
        "trino": "POSITION(needle in haystack)"
      },
      "write": {}
    },
    {
      "sql": "STR_POSITION(haystack, needle)",
      "read": {
        "clickhouse": "POSITION(haystack, needle)",
        "databricks": "POSITION(needle, haystack)",
        "snowflake": "POSITION(needle, haystack)",
        "spark2": "POSITION(needle, haystack)"
      },
      "write": {}
    },
    {
      "sql": "STR_POSITION(haystack, needle)",
      "read": {
        "athena": "STRPOS(haystack, needle)",
        "bigquery": "STRPOS(haystack, needle)",
        "drill": "STRPOS(haystack, needle)",
        "duckdb": "STRPOS(haystack, needle)",
        "postgres": "STRPOS(haystack, needle)",
        "presto": "STRPOS(haystack, needle)",
        "redshift": "STRPOS(haystack, needle)",
        "trino": "STRPOS(haystack, needle)"
      },
      "write": {}
    },
    {
      "sql": "STR_POSITION(haystack, needle)",
      "read": {
        "bigquery": "INSTR(haystack, needle)",
        "databricks": "INSTR(haystack, needle)",
        "doris": "INSTR(haystack, needle)",
        "duckdb": "INSTR(haystack, needle)",
        "hive": "INSTR(haystack, needle)",
        "mysql": "INSTR(haystack, needle)",
        "oracle": "INSTR(haystack, needle)",
        "spark": "INSTR(haystack, needle)",
        "spark2": "INSTR(haystack, needle)",
        "sqlite": "INSTR(haystack, needle)",
        "starrocks": "INSTR(haystack, needle)",
        "teradata": "INSTR(haystack, needle)"
      },
      "write": {}
    },
    {
      "sql": "STR_POSITION(haystack, needle)",
      "read": {
        "clickhouse": "LOCATE(needle, haystack)",
        "databricks": "LOCATE(needle, haystack)",
        "doris": "LOCATE(needle, haystack)",
        "hive": "LOCATE(needle, haystack)",
        "mysql": "LOCATE(needle, haystack)",
        "spark": "LOCATE(needle, haystack)",
        "spark2": "LOCATE(needle, haystack)",
        "starrocks": "LOCATE(needle, haystack)",
        "teradata": "LOCATE(needle, haystack)"
      },
      "write": {}
    },
    {
      "sql": "STR_POSITION(haystack, needle)",
      "read": {
        "athena": "CHARINDEX(needle, haystack)",
        "databricks": "CHARINDEX(needle, haystack)",
        "snowflake": "CHARINDEX(needle, haystack)",
        "tsql": "CHARINDEX(needle, haystack)"
      },
      "write": {}
    },
    {
      "sql": "STR_POSITION(haystack, needle)",
      "read": {
        "tableau": "FIND(haystack, needle)"
      },
      "write": {
        "athena": "STRPOS(haystack, needle)",
        "bigquery": "INSTR(haystack, needle)",
        "clickhouse": "POSITION(haystack, needle)",
        "databricks": "LOCATE(needle, haystack)",
        "doris": "LOCATE(needle, haystack)",
        "drill": "STRPOS(haystack, needle)",
        "duckdb": "STRPOS(haystack, needle)",
        "hive": "LOCATE(needle, haystack)",
        "materialize": "POSITION(needle IN haystack)",
        "mysql": "LOCATE(needle, haystack)",
        "oracle": "INSTR(haystack, needle)",
        "postgres": "POSITION(needle IN haystack)",
        "presto": "STRPOS(haystack, needle)",
        "redshift": "POSITION(needle IN haystack)",
        "risingwave": "POSITION(needle IN haystack)",
        "snowflake": "CHARINDEX(needle, haystack)",
        "spark": "LOCATE(needle, haystack)",
        "spark2": "LOCATE(needle, haystack)",
        "sqlite": "INSTR(haystack, needle)",
        "tableau": "FIND(haystack, needle)",
        "teradata": "INSTR(haystack, needle)",
        "trino": "STRPOS(haystack, needle)",
        "tsql": "CHARINDEX(needle, haystack)"
      }
    },
    {
      "sql": "STR_POSITION(haystack, needle, position)",
      "read": {
        "clickhouse": "POSITION(haystack, needle, position)",
        "databricks": "POSITION(needle, haystack, position)",
        "snowflake": "POSITION(needle, haystack, position)",
        "spark": "POSITION(needle, haystack, position)",
        "spark2": "POSITION(needle, haystack, position)"
      },
      "write": {}
    },
    {
      "sql": "STR_POSITION(haystack, needle, position)",
      "read": {
        "doris": "LOCATE(needle, haystack, position)",
        "hive": "LOCATE(needle, haystack, position)",
        "mysql": "LOCATE(needle, haystack, position)",
        "spark": "LOCATE(needle, haystack, position)",
        "spark2": "LOCATE(needle, haystack, position)",
        "starrocks": "LOCATE(needle, haystack, position)",
        "teradata": "LOCATE(needle, haystack, position)",
        "clickhouse": "LOCATE(needle, haystack, position)",
        "databricks": "LOCATE(needle, haystack, position)"
      },
      "write": {}
    },
    {
      "sql": "STR_POSITION(haystack, needle, position)",
      "read": {
        "bigquery": "INSTR(haystack, needle, position)",
        "doris": "INSTR(haystack, needle, position)",
        "oracle": "INSTR(haystack, needle, position)",
        "teradata": "INSTR(haystack, needle, position)"
      },
      "write": {}
    },
    {
      "sql": "STR_POSITION(haystack, needle, position)",
      "read": {
        "databricks": "CHARINDEX(needle, haystack, position)",
        "snowflake": "CHARINDEX(needle, haystack, position)",
        "tsql": "CHARINDEX(needle, haystack, position)"
      },
      "write": {}
    },
    {
      "sql": "STR_POSITION(haystack, needle, position)",
      "read": {},
      "write": {
        "athena": "IF(STRPOS(SUBSTRING(haystack, position), needle) = 0, 0, STRPOS(SUBSTRING(haystack, position), needle) + position - 1)",
        "bigquery": "INSTR(haystack, needle, position)",
        "clickhouse": "POSITION(haystack, needle, position)",
        "databricks": "LOCATE(needle, haystack, position)",
        "doris": "LOCATE(needle, haystack, position)",
        "drill": "`IF`(STRPOS(SUBSTRING(haystack, position), needle) = 0, 0, STRPOS(SUBSTRING(haystack, position), needle) + position - 1)",
        "duckdb": "CASE WHEN STRPOS(SUBSTRING(haystack, position), needle) = 0 THEN 0 ELSE STRPOS(SUBSTRING(haystack, position), needle) + position - 1 END",
        "hive": "LOCATE(needle, haystack, position)",
        "materialize": "CASE WHEN POSITION(needle IN SUBSTRING(haystack FROM position)) = 0 THEN 0 ELSE POSITION(needle IN SUBSTRING(haystack FROM position)) + position - 1 END",
        "mysql": "LOCATE(needle, haystack, position)",
        "oracle": "INSTR(haystack, needle, position)",
        "postgres": "CASE WHEN POSITION(needle IN SUBSTRING(haystack FROM position)) = 0 THEN 0 ELSE POSITION(needle IN SUBSTRING(haystack FROM position)) + position - 1 END",
        "presto": "IF(STRPOS(SUBSTRING(haystack, position), needle) = 0, 0, STRPOS(SUBSTRING(haystack, position), needle) + position - 1)",
        "redshift": "CASE WHEN POSITION(needle IN SUBSTRING(haystack FROM position)) = 0 THEN 0 ELSE POSITION(needle IN SUBSTRING(haystack FROM position)) + position - 1 END",
        "risingwave": "CASE WHEN POSITION(needle IN SUBSTRING(haystack FROM position)) = 0 THEN 0 ELSE POSITION(needle IN SUBSTRING(haystack FROM position)) + position - 1 END",
        "snowflake": "CHARINDEX(needle, haystack, position)",
        "spark": "LOCATE(needle, haystack, position)",
        "spark2": "LOCATE(needle, haystack, position)",
        "sqlite": "IIF(INSTR(SUBSTRING(haystack, position), needle) = 0, 0, INSTR(SUBSTRING(haystack, position), needle) + position - 1)",
        "tableau": "IF FIND(SUBSTRING(haystack, position), needle) = 0 THEN 0 ELSE FIND(SUBSTRING(haystack, position), needle) + position - 1 END",
        "teradata": "INSTR(haystack, needle, position)",
        "trino": "IF(STRPOS(SUBSTRING(haystack, position), needle) = 0, 0, STRPOS(SUBSTRING(haystack, position), needle) + position - 1)",
        "tsql": "CHARINDEX(needle, haystack, position)"
      }
    },
    {
      "sql": "STR_POSITION(haystack, needle, position, occurrence)",
      "read": {
        "bigquery": "INSTR(haystack, needle, position, occurrence)",
        "oracle": "INSTR(haystack, needle, position, occurrence)",
        "teradata": "INSTR(haystack, needle, position, occurrence)"
      },
      "write": {
        "bigquery": "INSTR(haystack, needle, position, occurrence)",
        "oracle": "INSTR(haystack, needle, position, occurrence)",
        "presto": "IF(STRPOS(SUBSTRING(haystack, position), needle, occurrence) = 0, 0, STRPOS(SUBSTRING(haystack, position), needle, occurrence) + position - 1)",
        "tableau": "IF FINDNTH(SUBSTRING(haystack, position), needle, occurrence) = 0 THEN 0 ELSE FINDNTH(SUBSTRING(haystack, position), needle, occurrence) + position - 1 END",
        "teradata": "INSTR(haystack, needle, position, occurrence)",
        "trino": "IF(STRPOS(SUBSTRING(haystack, position), needle, occurrence) = 0, 0, STRPOS(SUBSTRING(haystack, position), needle, occurrence) + position - 1)"
      }
    },
    {
      "sql": "CONCAT_WS('-', 'a', 'b')",
      "read": {},
      "write": {
        "clickhouse": "CONCAT_WS('-', 'a', 'b')",
        "duckdb": "CONCAT_WS('-', 'a', 'b')",
        "presto": "CONCAT_WS('-', CAST('a' AS VARCHAR), CAST('b' AS VARCHAR))",
        "hive": "CONCAT_WS('-', 'a', 'b')",
        "spark": "CONCAT_WS('-', 'a', 'b')",
        "trino": "CONCAT_WS('-', CAST('a' AS VARCHAR), CAST('b' AS VARCHAR))"
      }
    },
    {
      "sql": "CONCAT_WS('-', x)",
      "read": {},
      "write": {
        "clickhouse": "CONCAT_WS('-', x)",
        "duckdb": "CONCAT_WS('-', x)",
        "hive": "CONCAT_WS('-', x)",
        "presto": "CONCAT_WS('-', CAST(x AS VARCHAR))",
        "spark": "CONCAT_WS('-', x)",
        "trino": "CONCAT_WS('-', CAST(x AS VARCHAR))"
      }
    },
    {
      "sql": "CONCAT(a)",
      "read": {},
      "write": {
        "clickhouse": "CONCAT(a)",
        "presto": "CAST(a AS VARCHAR)",
        "trino": "CAST(a AS VARCHAR)",
        "tsql": "a"
      }
    },
    {
      "sql": "CONCAT(COALESCE(a, ''))",
      "read": {
        "drill": "CONCAT(a)",
        "duckdb": "CONCAT(a)",
        "postgres": "CONCAT(a)",
        "tsql": "CONCAT(a)"
      },
      "write": {}
    },
    {
      "sql": "IF(x > 1, 1, 0)",
      "read": {},
      "write": {
        "drill": "`IF`(x > 1, 1, 0)",
        "duckdb": "CASE WHEN x > 1 THEN 1 ELSE 0 END",
        "presto": "IF(x > 1, 1, 0)",
        "hive": "IF(x > 1, 1, 0)",
        "spark": "IF(x > 1, 1, 0)",
        "tableau": "IF x > 1 THEN 1 ELSE 0 END"
      }
    },
    {
      "sql": "CASE WHEN 1 THEN x ELSE 0 END",
      "read": {},
      "write": {
        "drill": "CASE WHEN 1 THEN x ELSE 0 END",
        "duckdb": "CASE WHEN 1 THEN x ELSE 0 END",
        "presto": "CASE WHEN 1 THEN x ELSE 0 END",
        "hive": "CASE WHEN 1 THEN x ELSE 0 END",
        "spark": "CASE WHEN 1 THEN x ELSE 0 END",
        "tableau": "CASE WHEN 1 THEN x ELSE 0 END"
      }
    },
    {
      "sql": "x[y]",
      "read": {},
      "write": {
        "drill": "x[y]",
        "duckdb": "x[y]",
        "presto": "x[y]",
        "hive": "x[y]",
        "spark": "x[y]"
      }
    },
    {
      "sql": "'[\"x\"]'",
      "read": {},
      "write": {
        "duckdb": "'[\"x\"]'",
        "presto": "'[\"x\"]'",
        "hive": "'[\"x\"]'",
        "spark": "'[\"x\"]'"
      }
    },
    {
      "sql": "true or null as \"foo\"",
      "read": {},
      "write": {
        "bigquery": "TRUE OR NULL AS `foo`",
        "drill": "TRUE OR NULL AS `foo`",
        "duckdb": "TRUE OR NULL AS \"foo\"",
        "presto": "TRUE OR NULL AS \"foo\"",
        "hive": "TRUE OR NULL AS `foo`",
        "spark": "TRUE OR NULL AS `foo`"
      }
    },
    {
      "sql": "SELECT IF(COALESCE(bar, 0) = 1, TRUE, FALSE) as foo FROM baz",
      "read": {},
      "write": {
        "bigquery": "SELECT IF(COALESCE(bar, 0) = 1, TRUE, FALSE) AS foo FROM baz",
        "duckdb": "SELECT CASE WHEN COALESCE(bar, 0) = 1 THEN TRUE ELSE FALSE END AS foo FROM baz",
        "presto": "SELECT IF(COALESCE(bar, 0) = 1, TRUE, FALSE) AS foo FROM baz",
        "hive": "SELECT IF(COALESCE(bar, 0) = 1, TRUE, FALSE) AS foo FROM baz",
        "spark": "SELECT IF(COALESCE(bar, 0) = 1, TRUE, FALSE) AS foo FROM baz"
      }
    },
    {
      "sql": "LEVENSHTEIN(col1, col2)",
      "read": {
        "bigquery": "EDIT_DISTANCE(col1, col2)",
        "clickhouse": "editDistance(col1, col2)",
        "drill": "LEVENSHTEIN_DISTANCE(col1, col2)",
        "duckdb": "LEVENSHTEIN(col1, col2)",
        "hive": "LEVENSHTEIN(col1, col2)",
        "spark": "LEVENSHTEIN(col1, col2)",
        "postgres": "LEVENSHTEIN(col1, col2)",
        "presto": "LEVENSHTEIN_DISTANCE(col1, col2)",
        "snowflake": "EDITDISTANCE(col1, col2)",
        "sqlite": "EDITDIST3(col1, col2)",
        "trino": "LEVENSHTEIN_DISTANCE(col1, col2)"
      },
      "write": {
        "bigquery": "EDIT_DISTANCE(col1, col2)",
        "clickhouse": "editDistance(col1, col2)",
        "drill": "LEVENSHTEIN_DISTANCE(col1, col2)",
        "duckdb": "LEVENSHTEIN(col1, col2)",
        "hive": "LEVENSHTEIN(col1, col2)",
        "spark": "LEVENSHTEIN(col1, col2)",
        "postgres": "LEVENSHTEIN(col1, col2)",
        "presto": "LEVENSHTEIN_DISTANCE(col1, col2)",
        "snowflake": "EDITDISTANCE(col1, col2)",
        "sqlite": "EDITDIST3(col1, col2)",
        "trino": "LEVENSHTEIN_DISTANCE(col1, col2)"
      }
    },
    {
      "sql": "LEVENSHTEIN(col1, col2, 1, 2, 3)",
      "read": {},
      "write": {
        "postgres": "LEVENSHTEIN(col1, col2, 1, 2, 3)"
      }
    },
    {
      "sql": "LEVENSHTEIN(col1, col2, 1, 2, 3, 4)",
      "read": {},
      "write": {
        "postgres": "LEVENSHTEIN_LESS_EQUAL(col1, col2, 1, 2, 3, 4)"
      }
    },
    {
      "sql": "LEVENSHTEIN(coalesce(col1, col2), coalesce(col2, col1))",
      "read": {},
      "write": {
        "bigquery": "EDIT_DISTANCE(COALESCE(col1, col2), COALESCE(col2, col1))",
        "duckdb": "LEVENSHTEIN(COALESCE(col1, col2), COALESCE(col2, col1))",
        "drill": "LEVENSHTEIN_DISTANCE(COALESCE(col1, col2), COALESCE(col2, col1))",
        "presto": "LEVENSHTEIN_DISTANCE(COALESCE(col1, col2), COALESCE(col2, col1))",
        "hive": "LEVENSHTEIN(COALESCE(col1, col2), COALESCE(col2, col1))",
        "spark": "LEVENSHTEIN(COALESCE(col1, col2), COALESCE(col2, col1))"
      }
    },
    {
      "sql": "ARRAY_FILTER(the_array, x -> x > 0)",
      "read": {},
      "write": {
        "presto": "FILTER(the_array, x -> x > 0)",
        "hive": "FILTER(the_array, x -> x > 0)",
        "spark": "FILTER(the_array, x -> x > 0)"
      }
    },
    {
      "sql": "FILTER(the_array, x -> x > 0)",
      "read": {},
      "write": {
        "presto": "FILTER(the_array, x -> x > 0)",
        "starrocks": "ARRAY_FILTER(the_array, x -> x > 0)"
      }
    },
    {
      "sql": "a / b",
      "read": {},
      "write": {
        "bigquery": "a / b",
        "clickhouse": "a / b",
        "databricks": "a / b",
        "duckdb": "a / b",
        "hive": "a / b",
        "mysql": "a / b",
        "oracle": "a / b",
        "snowflake": "a / b",
        "spark": "a / b",
        "starrocks": "a / b",
        "drill": "CAST(a AS DOUBLE) / b",
        "postgres": "CAST(a AS DOUBLE PRECISION) / b",
        "presto": "CAST(a AS DOUBLE) / b",
        "redshift": "CAST(a AS DOUBLE PRECISION) / b",
        "sqlite": "CAST(a AS REAL) / b",
        "teradata": "CAST(a AS DOUBLE PRECISION) / b",
        "trino": "CAST(a AS DOUBLE) / b",
        "tsql": "CAST(a AS FLOAT) / b"
      }
    },
    {
      "sql": "MOD(8 - 1 + 7, 7)",
      "read": {},
      "write": {
        "hive": "(8 - 1 + 7) % 7",
        "presto": "(8 - 1 + 7) % 7",
        "snowflake": "(8 - 1 + 7) % 7",
        "bigquery": "MOD(8 - 1 + 7, 7)"
      }
    },
    {
      "sql": "MOD(a, b + 1)",
      "read": {},
      "write": {
        "hive": "a % (b + 1)",
        "presto": "a % (b + 1)",
        "snowflake": "a % (b + 1)",
        "bigquery": "MOD(a, b + 1)"
      }
    },
    {
      "sql": "ARRAY_REMOVE(the_array, target)",
      "read": {},
      "write": {
        "clickhouse": "arrayFilter(_u -> _u <> target, the_array)",
        "duckdb": "LIST_FILTER(the_array, _u -> _u <> target)",
        "bigquery": "ARRAY(SELECT _u FROM UNNEST(the_array) AS _u WHERE _u <> target)",
        "hive": "ARRAY_REMOVE(the_array, target)",
        "postgres": "ARRAY_REMOVE(the_array, target)",
        "presto": "ARRAY_REMOVE(the_array, target)",
        "starrocks": "ARRAY_REMOVE(the_array, target)",
        "databricks": "ARRAY_REMOVE(the_array, target)",
        "snowflake": "ARRAY_REMOVE(the_array, target)"
      }
    },
    {
      "sql": "SELECT * FROM data LIMIT 10, 20",
      "read": {},
      "write": {
        "sqlite": "SELECT * FROM data LIMIT 20 OFFSET 10"
      }
    },
    {
      "sql": "SELECT x FROM y LIMIT 10",
      "read": {
        "teradata": "SELECT TOP 10 x FROM y",
        "tsql": "SELECT TOP 10 x FROM y",
        "snowflake": "SELECT TOP 10 x FROM y"
      },
      "write": {
        "sqlite": "SELECT x FROM y LIMIT 10",
        "oracle": "SELECT x FROM y FETCH FIRST 10 ROWS ONLY",
        "tsql": "SELECT TOP 10 x FROM y"
      }
    },
    {
      "sql": "SELECT x FROM y LIMIT 10 OFFSET 5",
      "read": {},
      "write": {
        "sqlite": "SELECT x FROM y LIMIT 10 OFFSET 5",
        "oracle": "SELECT x FROM y OFFSET 5 ROWS FETCH FIRST 10 ROWS ONLY"
      }
    },
    {
      "sql": "SELECT x FROM y OFFSET 10 FETCH FIRST 3 ROWS ONLY",
      "read": {},
      "write": {
        "sqlite": "SELECT x FROM y LIMIT 3 OFFSET 10",
        "oracle": "SELECT x FROM y OFFSET 10 ROWS FETCH FIRST 3 ROWS ONLY"
      }
    },
    {
      "sql": "SELECT x FROM y OFFSET 10 ROWS FETCH FIRST 3 ROWS ONLY",
      "read": {},
      "write": {
        "oracle": "SELECT x FROM y OFFSET 10 ROWS FETCH FIRST 3 ROWS ONLY"
      }
    },
    {
      "sql": "\"x\" + \"y\"",
      "read": {
        "clickhouse": "`x` + \"y\"",
        "sqlite": "`x` + \"y\"",
        "redshift": "\"x\" + \"y\""
      },
      "write": {}
    },
    {
      "sql": "[1, 2]",
      "read": {},
      "write": {
        "bigquery": "[1, 2]",
        "clickhouse": "[1, 2]"
      }
    },
    {
      "sql": "SELECT * FROM VALUES ('x'), ('y') AS t(z)",
      "read": {},
      "write": {
        "spark": "SELECT * FROM VALUES ('x'), ('y') AS t(z)"
      }
    },
    {
      "sql": "CREATE TABLE t (c CHAR, nc NCHAR, v1 VARCHAR, v2 VARCHAR2, nv NVARCHAR, nv2 NVARCHAR2)",
      "read": {},
      "write": {
        "duckdb": "CREATE TABLE t (c TEXT, nc TEXT, v1 TEXT, v2 TEXT, nv TEXT, nv2 TEXT)",
        "hive": "CREATE TABLE t (c STRING, nc STRING, v1 STRING, v2 STRING, nv STRING, nv2 STRING)",
        "oracle": "CREATE TABLE t (c CHAR, nc NCHAR, v1 VARCHAR2, v2 VARCHAR2, nv NVARCHAR2, nv2 NVARCHAR2)",
        "postgres": "CREATE TABLE t (c CHAR, nc CHAR, v1 VARCHAR, v2 VARCHAR, nv VARCHAR, nv2 VARCHAR)",
        "sqlite": "CREATE TABLE t (c TEXT, nc TEXT, v1 TEXT, v2 TEXT, nv TEXT, nv2 TEXT)"
      }
    },
    {
      "sql": "POWER(1.2, 3.4)",
      "read": {
        "hive": "pow(1.2, 3.4)",
        "postgres": "power(1.2, 3.4)"
      },
      "write": {}
    },
    {
      "sql": "CREATE INDEX my_idx ON tbl(a, b)",
      "read": {
        "hive": "CREATE INDEX my_idx ON TABLE tbl(a, b)",
        "sqlite": "CREATE INDEX my_idx ON tbl(a, b)"
      },
      "write": {
        "hive": "CREATE INDEX my_idx ON TABLE tbl(a, b)",
        "postgres": "CREATE INDEX my_idx ON tbl(a NULLS FIRST, b NULLS FIRST)",
        "sqlite": "CREATE INDEX my_idx ON tbl(a, b)"
      }
    },
    {
      "sql": "CREATE UNIQUE INDEX my_idx ON tbl(a, b)",
      "read": {
        "hive": "CREATE UNIQUE INDEX my_idx ON TABLE tbl(a, b)",
        "sqlite": "CREATE UNIQUE INDEX my_idx ON tbl(a, b)"
      },
      "write": {
        "hive": "CREATE UNIQUE INDEX my_idx ON TABLE tbl(a, b)",
        "postgres": "CREATE UNIQUE INDEX my_idx ON tbl(a NULLS FIRST, b NULLS FIRST)",
        "sqlite": "CREATE UNIQUE INDEX my_idx ON tbl(a, b)"
      }
    },
    {
      "sql": "CREATE TABLE t (b1 BINARY, b2 BINARY(1024), c1 TEXT, c2 TEXT(1024))",
      "read": {},
      "write": {
        "duckdb": "CREATE TABLE t (b1 BLOB, b2 BLOB(1024), c1 TEXT, c2 TEXT(1024))",
        "hive": "CREATE TABLE t (b1 BINARY, b2 BINARY(1024), c1 STRING, c2 VARCHAR(1024))",
        "oracle": "CREATE TABLE t (b1 BLOB, b2 BLOB(1024), c1 CLOB, c2 CLOB(1024))",
        "postgres": "CREATE TABLE t (b1 BYTEA, b2 BYTEA(1024), c1 TEXT, c2 TEXT(1024))",
        "sqlite": "CREATE TABLE t (b1 BLOB, b2 BLOB(1024), c1 TEXT, c2 TEXT(1024))",
        "redshift": "CREATE TABLE t (b1 VARBYTE, b2 VARBYTE(1024), c1 VARCHAR(MAX), c2 VARCHAR(1024))"
      }
    },
    {
      "sql": "WITH t AS (SELECT 1 AS x, 2 AS y) SELECT x AS x FROM t GROUP BY x",
      "read": {},
      "write": {
        "hive": "WITH t AS (SELECT 1 AS x, 2 AS y) SELECT x AS x FROM t GROUP BY x",
        "oracle": "WITH t AS (SELECT 1 AS x, 2 AS y) SELECT x AS x FROM t GROUP BY x",
        "presto": "WITH t AS (SELECT 1 AS x, 2 AS y) SELECT x AS x FROM t GROUP BY x"
      }
    },
    {
      "sql": "WITH t AS (SELECT 1 AS x, 2 AS y) SELECT SUM(x) AS y, y AS x FROM t GROUP BY y",
      "read": {},
      "write": {
        "hive": "WITH t AS (SELECT 1 AS x, 2 AS y) SELECT SUM(x) AS y, y AS x FROM t GROUP BY y",
        "oracle": "WITH t AS (SELECT 1 AS x, 2 AS y) SELECT SUM(x) AS y, y AS x FROM t GROUP BY y",
        "presto": "WITH t AS (SELECT 1 AS x, 2 AS y) SELECT SUM(x) AS y, y AS x FROM t GROUP BY y"
      }
    },
    {
      "sql": "SELECT 1 AS \"foo\"",
      "read": {
        "mysql": "SELECT 1 'foo'",
        "sqlite": "SELECT 1 'foo'",
        "tsql": "SELECT 1 'foo'"
      },
      "write": {}
    },
    {
      "sql": "SELECT y x FROM my_table t",
      "read": {},
      "write": {
        "drill": "SELECT y AS x FROM my_table AS t",
        "hive": "SELECT y AS x FROM my_table AS t",
        "oracle": "SELECT y AS x FROM my_table t",
        "postgres": "SELECT y AS x FROM my_table AS t",
        "sqlite": "SELECT y AS x FROM my_table AS t"
      }
    },
    {
      "sql": "SELECT * FROM (SELECT * FROM my_table AS t) AS tbl",
      "read": {},
      "write": {
        "drill": "SELECT * FROM (SELECT * FROM my_table AS t) AS tbl",
        "hive": "SELECT * FROM (SELECT * FROM my_table AS t) AS tbl",
        "oracle": "SELECT * FROM (SELECT * FROM my_table t) tbl",
        "postgres": "SELECT * FROM (SELECT * FROM my_table AS t) AS tbl",
        "sqlite": "SELECT * FROM (SELECT * FROM my_table AS t) AS tbl"
      }
    },
    {
      "sql": "WITH cte1 AS (SELECT a, b FROM table1), cte2 AS (SELECT c, e AS d FROM table2) SELECT b, d AS dd FROM cte1 AS t CROSS JOIN cte2 WHERE cte1.a = cte2.c",
      "read": {},
      "write": {
        "hive": "WITH cte1 AS (SELECT a, b FROM table1), cte2 AS (SELECT c, e AS d FROM table2) SELECT b, d AS dd FROM cte1 AS t CROSS JOIN cte2 WHERE cte1.a = cte2.c",
        "oracle": "WITH cte1 AS (SELECT a, b FROM table1), cte2 AS (SELECT c, e AS d FROM table2) SELECT b, d AS dd FROM cte1 t CROSS JOIN cte2 WHERE cte1.a = cte2.c",
        "postgres": "WITH cte1 AS (SELECT a, b FROM table1), cte2 AS (SELECT c, e AS d FROM table2) SELECT b, d AS dd FROM cte1 AS t CROSS JOIN cte2 WHERE cte1.a = cte2.c",
        "sqlite": "WITH cte1 AS (SELECT a, b FROM table1), cte2 AS (SELECT c, e AS d FROM table2) SELECT b, d AS dd FROM cte1 AS t CROSS JOIN cte2 WHERE cte1.a = cte2.c"
      }
    },
    {
      "sql": "SELECT * FROM (SELECT 1 AS col) AS apply",
      "read": {
        "hive": "SELECT * FROM (SELECT 1 AS col) apply",
        "postgres": "SELECT * FROM (SELECT 1 AS col) apply",
        "duckdb": "SELECT * FROM (SELECT 1 AS col) apply",
        "presto": "SELECT * FROM (SELECT 1 AS col) apply",
        "spark": "SELECT * FROM (SELECT 1 AS col) apply",
        "spark2": "SELECT * FROM (SELECT 1 AS col) apply",
        "trino": "SELECT * FROM (SELECT 1 AS col) apply",
        "snowflake": "SELECT * FROM (SELECT 1 AS col) apply",
        "bigquery": "SELECT * FROM (SELECT 1 AS col) apply",
        "athena": "SELECT * FROM (SELECT 1 AS col) apply"
      },
      "write": {}
    },
    {
      "sql": "SELECT a IS NOT DISTINCT FROM b",
      "read": {
        "mysql": "SELECT a <=> b",
        "postgres": "SELECT a IS NOT DISTINCT FROM b"
      },
      "write": {
        "mysql": "SELECT a <=> b",
        "postgres": "SELECT a IS NOT DISTINCT FROM b"
      }
    },
    {
      "sql": "SELECT a IS DISTINCT FROM b",
      "read": {
        "postgres": "SELECT a IS DISTINCT FROM b"
      },
      "write": {
        "mysql": "SELECT NOT a <=> b",
        "postgres": "SELECT a IS DISTINCT FROM b"
      }
    },
    {
      "sql": "SELECT 1 /* arbitrary content,,, until end-of-line */",
      "read": {
        "mysql": "SELECT 1 # arbitrary content,,, until end-of-line",
        "bigquery": "SELECT 1 # arbitrary content,,, until end-of-line",
        "clickhouse": "SELECT 1 #! arbitrary content,,, until end-of-line"
      },
      "write": {}
    },
    {
      "sql": "/* comment1 */\nSELECT\n  x, /* comment2 */\n  y /* comment3 */",
      "read": {
        "mysql": "SELECT # comment1\n  x, # comment2\n  y # comment3",
        "bigquery": "SELECT # comment1\n  x, # comment2\n  y # comment3",
        "clickhouse": "SELECT # comment1\n  x, # comment2\n  y # comment3"
      },
      "write": {}
    },
    {
      "sql": "BEGIN TRANSACTION",
      "read": {},
      "write": {
        "bigquery": "BEGIN TRANSACTION",
        "mysql": "BEGIN",
        "postgres": "BEGIN",
        "presto": "START TRANSACTION",
        "trino": "START TRANSACTION",
        "redshift": "BEGIN",
        "snowflake": "BEGIN",
        "sqlite": "BEGIN TRANSACTION",
        "tsql": "BEGIN TRANSACTION"
      }
    },
    {
      "sql": "BEGIN READ WRITE, ISOLATION LEVEL SERIALIZABLE",
      "read": {
        "presto": "START TRANSACTION READ WRITE, ISOLATION LEVEL SERIALIZABLE",
        "trino": "START TRANSACTION READ WRITE, ISOLATION LEVEL SERIALIZABLE"
      },
      "write": {}
    },
    {
      "sql": "BEGIN ISOLATION LEVEL REPEATABLE READ",
      "read": {
        "presto": "START TRANSACTION ISOLATION LEVEL REPEATABLE READ",
        "trino": "START TRANSACTION ISOLATION LEVEL REPEATABLE READ"
      },
      "write": {}
    },
    {
      "sql": "BEGIN IMMEDIATE TRANSACTION",
      "read": {},
      "write": {
        "sqlite": "BEGIN IMMEDIATE TRANSACTION"
      }
    },
    {
      "sql": "\n            MERGE INTO target USING source ON target.id = source.id\n                WHEN NOT MATCHED THEN INSERT (id) values (source.id)\n            ",
      "read": {},
      "write": {
        "bigquery": "MERGE INTO target USING source ON target.id = source.id WHEN NOT MATCHED THEN INSERT (id) VALUES (source.id)",
        "snowflake": "MERGE INTO target USING source ON target.id = source.id WHEN NOT MATCHED THEN INSERT (id) VALUES (source.id)",
        "spark": "MERGE INTO target USING source ON target.id = source.id WHEN NOT MATCHED THEN INSERT (id) VALUES (source.id)"
      }
    },
    {
      "sql": "\n            MERGE INTO target USING source ON target.id = source.id\n                WHEN MATCHED AND source.is_deleted = 1 THEN DELETE\n                WHEN MATCHED THEN UPDATE SET val = source.val\n                WHEN NOT MATCHED THEN INSERT (id, val) VALUES (source.id, source.val)\n            ",
      "read": {},
      "write": {
        "bigquery": "MERGE INTO target USING source ON target.id = source.id WHEN MATCHED AND source.is_deleted = 1 THEN DELETE WHEN MATCHED THEN UPDATE SET val = source.val WHEN NOT MATCHED THEN INSERT (id, val) VALUES (source.id, source.val)",
        "snowflake": "MERGE INTO target USING source ON target.id = source.id WHEN MATCHED AND source.is_deleted = 1 THEN DELETE WHEN MATCHED THEN UPDATE SET val = source.val WHEN NOT MATCHED THEN INSERT (id, val) VALUES (source.id, source.val)",
        "spark": "MERGE INTO target USING source ON target.id = source.id WHEN MATCHED AND source.is_deleted = 1 THEN DELETE WHEN MATCHED THEN UPDATE SET val = source.val WHEN NOT MATCHED THEN INSERT (id, val) VALUES (source.id, source.val)"
      }
    },
    {
      "sql": "\n            MERGE INTO target USING source ON target.id = source.id\n                WHEN MATCHED THEN UPDATE *\n                WHEN NOT MATCHED THEN INSERT *\n            ",
      "read": {},
      "write": {
        "spark": "MERGE INTO target USING source ON target.id = source.id WHEN MATCHED THEN UPDATE * WHEN NOT MATCHED THEN INSERT *"
      }
    },
    {
      "sql": "\n            MERGE a b USING c d ON b.id = d.id\n            WHEN MATCHED AND EXISTS (\n                SELECT b.name\n                EXCEPT\n                SELECT d.name\n            )\n            THEN UPDATE SET b.name = d.name\n            ",
      "read": {},
      "write": {
        "bigquery": "MERGE INTO a AS b USING c AS d ON b.id = d.id WHEN MATCHED AND EXISTS(SELECT b.name EXCEPT DISTINCT SELECT d.name) THEN UPDATE SET b.name = d.name",
        "snowflake": "MERGE INTO a AS b USING c AS d ON b.id = d.id WHEN MATCHED AND EXISTS(SELECT b.name EXCEPT SELECT d.name) THEN UPDATE SET b.name = d.name",
        "spark": "MERGE INTO a AS b USING c AS d ON b.id = d.id WHEN MATCHED AND EXISTS(SELECT b.name EXCEPT SELECT d.name) THEN UPDATE SET b.name = d.name"
      }
    },
    {
      "sql": "MERGE INTO foo AS target USING (SELECT a, b FROM tbl) AS src ON src.a = target.a\n            WHEN MATCHED AND target.a <> src.a THEN UPDATE SET target.b = 'FOO'\n            WHEN NOT MATCHED THEN INSERT (target.a, target.b) VALUES (src.a, src.b)",
      "read": {},
      "write": {
        "trino": "MERGE INTO foo AS target USING (SELECT a, b FROM tbl) AS src ON src.a = target.a WHEN MATCHED AND target.a <> src.a THEN UPDATE SET b = 'FOO' WHEN NOT MATCHED THEN INSERT (a, b) VALUES (src.a, src.b)",
        "postgres": "MERGE INTO foo AS target USING (SELECT a, b FROM tbl) AS src ON src.a = target.a WHEN MATCHED AND target.a <> src.a THEN UPDATE SET b = 'FOO' WHEN NOT MATCHED THEN INSERT (a, b) VALUES (src.a, src.b)"
      }
    },
    {
      "sql": "MERGE INTO foo AS target USING (SELECT a, b FROM tbl) AS src ON src.a = target.a\n            WHEN MATCHED THEN UPDATE SET target.b = COALESCE(src.b, target.b)\n            WHEN NOT MATCHED THEN INSERT (target.a, target.b) VALUES (src.a, src.b)",
      "read": {},
      "write": {
        "trino": "MERGE INTO foo AS target USING (SELECT a, b FROM tbl) AS src ON src.a = target.a WHEN MATCHED THEN UPDATE SET b = COALESCE(src.b, target.b) WHEN NOT MATCHED THEN INSERT (a, b) VALUES (src.a, src.b)",
        "postgres": "MERGE INTO foo AS target USING (SELECT a, b FROM tbl) AS src ON src.a = target.a WHEN MATCHED THEN UPDATE SET b = COALESCE(src.b, target.b) WHEN NOT MATCHED THEN INSERT (a, b) VALUES (src.a, src.b)"
      }
    },
    {
      "sql": "SUBSTR('123456', 2, 3)",
      "read": {},
      "write": {
        "bigquery": "SUBSTRING('123456', 2, 3)",
        "oracle": "SUBSTR('123456', 2, 3)",
        "postgres": "SUBSTRING('123456' FROM 2 FOR 3)"
      }
    },
    {
      "sql": "SUBSTRING('123456', 2, 3)",
      "read": {},
      "write": {
        "bigquery": "SUBSTRING('123456', 2, 3)",
        "oracle": "SUBSTR('123456', 2, 3)",
        "postgres": "SUBSTRING('123456' FROM 2 FOR 3)"
      }
    },
    {
      "sql": "LOG(x)",
      "read": {
        "duckdb": "LOG(x)",
        "postgres": "LOG(x)",
        "redshift": "LOG(x)",
        "sqlite": "LOG(x)",
        "teradata": "LOG(x)"
      },
      "write": {}
    },
    {
      "sql": "LN(x)",
      "read": {
        "dremio": "LOG(x)",
        "bigquery": "LOG(x)",
        "clickhouse": "LOG(x)",
        "databricks": "LOG(x)",
        "drill": "LOG(x)",
        "hive": "LOG(x)",
        "mysql": "LOG(x)",
        "tsql": "LOG(x)"
      },
      "write": {}
    },
    {
      "sql": "LOG(b, n)",
      "read": {
        "bigquery": "LOG(n, b)",
        "databricks": "LOG(b, n)",
        "drill": "LOG(b, n)",
        "duckdb": "LOG(b, n)",
        "hive": "LOG(b, n)",
        "mysql": "LOG(b, n)",
        "oracle": "LOG(b, n)",
        "postgres": "LOG(b, n)",
        "snowflake": "LOG(b, n)",
        "spark": "LOG(b, n)",
        "sqlite": "LOG(b, n)",
        "trino": "LOG(b, n)",
        "tsql": "LOG(n, b)"
      },
      "write": {}
    },
    {
      "sql": "SELECT COUNT_IF(col % 2 = 0) FROM foo",
      "read": {},
      "write": {
        "databricks": "SELECT COUNT_IF(col % 2 = 0) FROM foo",
        "presto": "SELECT COUNT_IF(col % 2 = 0) FROM foo",
        "snowflake": "SELECT COUNT_IF(col % 2 = 0) FROM foo",
        "sqlite": "SELECT SUM(IIF(col % 2 = 0, 1, 0)) FROM foo",
        "tsql": "SELECT COUNT_IF(col % 2 = 0) FROM foo",
        "postgres": "SELECT SUM(CASE WHEN col % 2 = 0 THEN 1 ELSE 0 END) FROM foo",
        "redshift": "SELECT SUM(CASE WHEN col % 2 = 0 THEN 1 ELSE 0 END) FROM foo"
      }
    },
    {
      "sql": "SELECT COUNT_IF(col % 2 = 0) FILTER(WHERE col < 1000) FROM foo",
      "read": {
        "databricks": "SELECT COUNT_IF(col % 2 = 0) FILTER(WHERE col < 1000) FROM foo",
        "tsql": "SELECT COUNT_IF(col % 2 = 0) FILTER(WHERE col < 1000) FROM foo"
      },
      "write": {
        "databricks": "SELECT COUNT_IF(col % 2 = 0) FILTER(WHERE col < 1000) FROM foo",
        "presto": "SELECT COUNT_IF(col % 2 = 0) FILTER(WHERE col < 1000) FROM foo",
        "sqlite": "SELECT SUM(IIF(col % 2 = 0, 1, 0)) FILTER(WHERE col < 1000) FROM foo",
        "tsql": "SELECT COUNT_IF(col % 2 = 0) FILTER(WHERE col < 1000) FROM foo"
      }
    },
    {
      "sql": "CAST(x AS some_udt)",
      "read": {},
      "write": {
        "oracle": "CAST(x AS some_udt)",
        "postgres": "CAST(x AS some_udt)",
        "presto": "CAST(x AS some_udt)",
        "teradata": "CAST(x AS some_udt)",
        "tsql": "CAST(x AS some_udt)"
      }
    },
    {
      "sql": "SELECT * FROM t QUALIFY COUNT(*) OVER () > 1",
      "read": {},
      "write": {
        "duckdb": "SELECT * FROM t QUALIFY COUNT(*) OVER () > 1",
        "snowflake": "SELECT * FROM t QUALIFY COUNT(*) OVER () > 1",
        "clickhouse": "SELECT * FROM t QUALIFY COUNT(*) OVER () > 1",
        "mysql": "SELECT * FROM (SELECT *, COUNT(*) OVER () AS _w FROM t) AS _t WHERE _w > 1",
        "oracle": "SELECT * FROM (SELECT *, COUNT(*) OVER () AS _w FROM t) _t WHERE _w > 1",
        "postgres": "SELECT * FROM (SELECT *, COUNT(*) OVER () AS _w FROM t) AS _t WHERE _w > 1",
        "tsql": "SELECT * FROM (SELECT *, COUNT_BIG(*) OVER () AS _w FROM t) AS _t WHERE _w > 1"
      }
    },
    {
      "sql": "SELECT \"user id\", some_id, 1 as other_id, 2 as \"2 nd id\" FROM t QUALIFY COUNT(*) OVER () > 1",
      "read": {},
      "write": {
        "duckdb": "SELECT \"user id\", some_id, 1 AS other_id, 2 AS \"2 nd id\" FROM t QUALIFY COUNT(*) OVER () > 1",
        "snowflake": "SELECT \"user id\", some_id, 1 AS other_id, 2 AS \"2 nd id\" FROM t QUALIFY COUNT(*) OVER () > 1",
        "clickhouse": "SELECT \"user id\", some_id, 1 AS other_id, 2 AS \"2 nd id\" FROM t QUALIFY COUNT(*) OVER () > 1",
        "mysql": "SELECT `user id`, some_id, other_id, `2 nd id` FROM (SELECT `user id`, some_id, 1 AS other_id, 2 AS `2 nd id`, COUNT(*) OVER () AS _w FROM t) AS _t WHERE _w > 1",
        "oracle": "SELECT \"user id\", some_id, other_id, \"2 nd id\" FROM (SELECT \"user id\", some_id, 1 AS other_id, 2 AS \"2 nd id\", COUNT(*) OVER () AS _w FROM t) _t WHERE _w > 1",
        "postgres": "SELECT \"user id\", some_id, other_id, \"2 nd id\" FROM (SELECT \"user id\", some_id, 1 AS other_id, 2 AS \"2 nd id\", COUNT(*) OVER () AS _w FROM t) AS _t WHERE _w > 1",
        "tsql": "SELECT [user id], some_id, other_id, [2 nd id] FROM (SELECT [user id] AS [user id], some_id AS some_id, 1 AS other_id, 2 AS [2 nd id], COUNT_BIG(*) OVER () AS _w FROM t) AS _t WHERE _w > 1"
      }
    },
    {
      "sql": "SELECT SUM(X) OVER (PARTITION BY x RANGE BETWEEN 1 PRECEDING AND CURRENT ROW)",
      "read": {
        "duckdb": "SELECT SUM(X) OVER (PARTITION BY x RANGE BETWEEN 1 PRECEDING AND CURRENT ROW EXCLUDE NO OTHERS)",
        "postgres": "SELECT SUM(X) OVER (PARTITION BY x RANGE BETWEEN 1 PRECEDING AND CURRENT ROW EXCLUDE NO OTHERS)",
        "sqlite": "SELECT SUM(X) OVER (PARTITION BY x RANGE BETWEEN 1 PRECEDING AND CURRENT ROW EXCLUDE NO OTHERS)",
        "oracle": "SELECT SUM(X) OVER (PARTITION BY x RANGE BETWEEN 1 PRECEDING AND CURRENT ROW EXCLUDE NO OTHERS)"
      },
      "write": {
        "duckdb": "SELECT SUM(X) OVER (PARTITION BY x RANGE BETWEEN 1 PRECEDING AND CURRENT ROW)",
        "postgres": "SELECT SUM(X) OVER (PARTITION BY x RANGE BETWEEN 1 PRECEDING AND CURRENT ROW)",
        "sqlite": "SELECT SUM(X) OVER (PARTITION BY x RANGE BETWEEN 1 PRECEDING AND CURRENT ROW)",
        "oracle": "SELECT SUM(X) OVER (PARTITION BY x RANGE BETWEEN 1 PRECEDING AND CURRENT ROW)"
      }
    },
    {
      "sql": "SELECT * FROM (WITH t AS (SELECT 1 AS c) SELECT c FROM t) AS subq",
      "read": {},
      "write": {
        "bigquery": "SELECT * FROM (WITH t AS (SELECT 1 AS c) SELECT c FROM t) AS subq",
        "clickhouse": "SELECT * FROM (WITH t AS (SELECT 1 AS c) SELECT c FROM t) AS subq",
        "databricks": "WITH t AS (SELECT 1 AS c) SELECT * FROM (SELECT c FROM t) AS subq",
        "duckdb": "SELECT * FROM (WITH t AS (SELECT 1 AS c) SELECT c FROM t) AS subq",
        "hive": "WITH t AS (SELECT 1 AS c) SELECT * FROM (SELECT c FROM t) AS subq",
        "mysql": "SELECT * FROM (WITH t AS (SELECT 1 AS c) SELECT c FROM t) AS subq",
        "postgres": "SELECT * FROM (WITH t AS (SELECT 1 AS c) SELECT c FROM t) AS subq",
        "presto": "SELECT * FROM (WITH t AS (SELECT 1 AS c) SELECT c FROM t) AS subq",
        "redshift": "SELECT * FROM (WITH t AS (SELECT 1 AS c) SELECT c FROM t) AS subq",
        "snowflake": "SELECT * FROM (WITH t AS (SELECT 1 AS c) SELECT c FROM t) AS subq",
        "spark": "WITH t AS (SELECT 1 AS c) SELECT * FROM (SELECT c FROM t) AS subq",
        "spark2": "WITH t AS (SELECT 1 AS c) SELECT * FROM (SELECT c FROM t) AS subq",
        "sqlite": "SELECT * FROM (WITH t AS (SELECT 1 AS c) SELECT c FROM t) AS subq",
        "trino": "SELECT * FROM (WITH t AS (SELECT 1 AS c) SELECT c FROM t) AS subq",
        "tsql": "WITH t AS (SELECT 1 AS c) SELECT * FROM (SELECT c AS c FROM t) AS subq"
      }
    },
    {
      "sql": "SELECT * FROM (SELECT * FROM (WITH t AS (SELECT 1 AS c) SELECT c FROM t) AS subq1) AS subq2",
      "read": {},
      "write": {
        "bigquery": "SELECT * FROM (SELECT * FROM (WITH t AS (SELECT 1 AS c) SELECT c FROM t) AS subq1) AS subq2",
        "duckdb": "SELECT * FROM (SELECT * FROM (WITH t AS (SELECT 1 AS c) SELECT c FROM t) AS subq1) AS subq2",
        "hive": "WITH t AS (SELECT 1 AS c) SELECT * FROM (SELECT * FROM (SELECT c FROM t) AS subq1) AS subq2",
        "tsql": "WITH t AS (SELECT 1 AS c) SELECT * FROM (SELECT * FROM (SELECT c AS c FROM t) AS subq1) AS subq2"
      }
    },
    {
      "sql": "WITH t1(x) AS (SELECT 1) SELECT * FROM (WITH t2(y) AS (SELECT 2) SELECT y FROM t2) AS subq",
      "read": {},
      "write": {
        "duckdb": "WITH t1(x) AS (SELECT 1) SELECT * FROM (WITH t2(y) AS (SELECT 2) SELECT y FROM t2) AS subq",
        "tsql": "WITH t1(x) AS (SELECT 1), t2(y) AS (SELECT 2) SELECT * FROM (SELECT y AS y FROM t2) AS subq"
      }
    },
    {
      "sql": "\nWITH c AS (\n  WITH b AS (\n    WITH a1 AS (\n      SELECT 1\n    ), a2 AS (\n      SELECT 2\n    )\n    SELECT * FROM a1, a2\n  )\n  SELECT *\n  FROM b\n)\nSELECT *\nFROM c",
      "read": {},
      "write": {
        "duckdb": "WITH c AS (WITH b AS (WITH a1 AS (SELECT 1), a2 AS (SELECT 2) SELECT * FROM a1, a2) SELECT * FROM b) SELECT * FROM c",
        "hive": "WITH a1 AS (SELECT 1), a2 AS (SELECT 2), b AS (SELECT * FROM a1, a2), c AS (SELECT * FROM b) SELECT * FROM c"
      }
    },
    {
      "sql": "\nWITH subquery1 AS (\n  WITH tmp AS (\n    SELECT\n      *\n    FROM table0\n  )\n  SELECT\n    *\n  FROM tmp\n), subquery2 AS (\n  WITH tmp2 AS (\n    SELECT\n      *\n    FROM table1\n    WHERE\n      a IN subquery1\n  )\n  SELECT\n    *\n  FROM tmp2\n)\nSELECT\n  *\nFROM subquery2\n",
      "read": {},
      "write": {
        "hive": "WITH tmp AS (\n  SELECT\n    *\n  FROM table0\n), subquery1 AS (\n  SELECT\n    *\n  FROM tmp\n), tmp2 AS (\n  SELECT\n    *\n  FROM table1\n  WHERE\n    a IN subquery1\n), subquery2 AS (\n  SELECT\n    *\n  FROM tmp2\n)\nSELECT\n  *\nFROM subquery2"
      }
    },
    {
      "sql": "RAND()",
      "read": {
        "bigquery": "RAND()",
        "clickhouse": "randCanonical()",
        "databricks": "RAND()",
        "doris": "RAND()",
        "drill": "RAND()",
        "duckdb": "RANDOM()",
        "hive": "RAND()",
        "mysql": "RAND()",
        "oracle": "DBMS_RANDOM.VALUE()",
        "postgres": "RANDOM()",
        "presto": "RAND()",
        "spark": "RAND()",
        "sqlite": "RANDOM()",
        "tsql": "RAND()"
      },
      "write": {
        "bigquery": "RAND()",
        "clickhouse": "randCanonical()",
        "databricks": "RAND()",
        "doris": "RAND()",
        "drill": "RAND()",
        "duckdb": "RANDOM()",
        "hive": "RAND()",
        "mysql": "RAND()",
        "oracle": "DBMS_RANDOM.VALUE()",
        "postgres": "RANDOM()",
        "presto": "RAND()",
        "spark": "RAND()",
        "sqlite": "RANDOM()",
        "tsql": "RAND()"
      }
    },
    {
      "sql": "ARRAY_ANY(arr, x -> pred)",
      "read": {},
      "write": {
        "bigquery": "(ARRAY_LENGTH(arr) = 0 OR ARRAY_LENGTH(ARRAY(SELECT x FROM UNNEST(arr) AS x WHERE pred)) <> 0)",
        "clickhouse": "(LENGTH(arr) = 0 OR LENGTH(arrayFilter(x -> pred, arr)) <> 0)",
        "databricks": "(SIZE(arr) = 0 OR SIZE(FILTER(arr, x -> pred)) <> 0)",
        "duckdb": "(ARRAY_LENGTH(arr) = 0 OR ARRAY_LENGTH(LIST_FILTER(arr, x -> pred)) <> 0)",
        "postgres": "(ARRAY_LENGTH(arr, 1) = 0 OR ARRAY_LENGTH(ARRAY(SELECT x FROM UNNEST(arr) AS _t0(x) WHERE pred), 1) <> 0)",
        "presto": "ANY_MATCH(arr, x -> pred)",
        "spark": "(SIZE(arr) = 0 OR SIZE(FILTER(arr, x -> pred)) <> 0)",
        "spark2": "(SIZE(arr) = 0 OR SIZE(FILTER(arr, x -> pred)) <> 0)",
        "teradata": "(CARDINALITY(arr) = 0 OR CARDINALITY(FILTER(arr, x -> pred)) <> 0)",
        "trino": "ANY_MATCH(arr, x -> pred)"
      }
    },
    {
      "sql": "SELECT * FROM UNNEST(GENERATE_DATE_ARRAY(DATE '2020-01-01', DATE '2020-02-01', INTERVAL 1 WEEK))",
      "read": {},
      "write": {
        "bigquery": "SELECT * FROM UNNEST(GENERATE_DATE_ARRAY(CAST('2020-01-01' AS DATE), CAST('2020-02-01' AS DATE), INTERVAL '1' WEEK))",
        "databricks": "SELECT * FROM EXPLODE(SEQUENCE(CAST('2020-01-01' AS DATE), CAST('2020-02-01' AS DATE), INTERVAL '1' WEEK))",
        "duckdb": "SELECT * FROM UNNEST(CAST(GENERATE_SERIES(CAST('2020-01-01' AS DATE), CAST('2020-02-01' AS DATE), INTERVAL '1' WEEK) AS DATE[]))",
        "mysql": "WITH RECURSIVE _generated_dates(date_value) AS (SELECT CAST('2020-01-01' AS DATE) AS date_value UNION ALL SELECT CAST(DATE_ADD(date_value, INTERVAL 1 WEEK) AS DATE) FROM _generated_dates WHERE CAST(DATE_ADD(date_value, INTERVAL 1 WEEK) AS DATE) <= CAST('2020-02-01' AS DATE)) SELECT * FROM (SELECT date_value FROM _generated_dates) AS _generated_dates",
        "postgres": "SELECT * FROM (SELECT CAST(value AS DATE) FROM GENERATE_SERIES(CAST('2020-01-01' AS DATE), CAST('2020-02-01' AS DATE), INTERVAL '1 WEEK') AS _t(value)) AS _unnested_generate_series",
        "presto": "SELECT * FROM UNNEST(SEQUENCE(CAST('2020-01-01' AS DATE), CAST('2020-02-01' AS DATE), (1 * INTERVAL '7' DAY)))",
        "redshift": "WITH RECURSIVE _generated_dates(date_value) AS (SELECT CAST('2020-01-01' AS DATE) AS date_value UNION ALL SELECT CAST(DATEADD(WEEK, 1, date_value) AS DATE) FROM _generated_dates WHERE CAST(DATEADD(WEEK, 1, date_value) AS DATE) <= CAST('2020-02-01' AS DATE)) SELECT * FROM (SELECT date_value FROM _generated_dates) AS _generated_dates",
        "snowflake": "SELECT * FROM (SELECT DATEADD(WEEK, CAST(value AS INT), CAST('2020-01-01' AS DATE)) AS value FROM TABLE(FLATTEN(INPUT => ARRAY_GENERATE_RANGE(0, (DATEDIFF(WEEK, CAST('2020-01-01' AS DATE), CAST('2020-02-01' AS DATE)) + 1 - 1) + 1))) AS _t0(seq, key, path, index, value, this))",
        "spark": "SELECT * FROM EXPLODE(SEQUENCE(CAST('2020-01-01' AS DATE), CAST('2020-02-01' AS DATE), INTERVAL '1' WEEK))",
        "trino": "SELECT * FROM UNNEST(SEQUENCE(CAST('2020-01-01' AS DATE), CAST('2020-02-01' AS DATE), (1 * INTERVAL '7' DAY)))",
        "tsql": "WITH _generated_dates(date_value) AS (SELECT CAST('2020-01-01' AS DATE) AS date_value UNION ALL SELECT CAST(DATEADD(WEEK, 1, date_value) AS DATE) FROM _generated_dates WHERE CAST(DATEADD(WEEK, 1, date_value) AS DATE) <= CAST('2020-02-01' AS DATE)) SELECT * FROM (SELECT date_value AS date_value FROM _generated_dates) AS _generated_dates"
      }
    },
    {
      "sql": "WITH dates AS (SELECT * FROM UNNEST(GENERATE_DATE_ARRAY(DATE '2020-01-01', DATE '2020-02-01', INTERVAL 1 WEEK))) SELECT * FROM dates",
      "read": {},
      "write": {
        "mysql": "WITH RECURSIVE _generated_dates(date_value) AS (SELECT CAST('2020-01-01' AS DATE) AS date_value UNION ALL SELECT CAST(DATE_ADD(date_value, INTERVAL 1 WEEK) AS DATE) FROM _generated_dates WHERE CAST(DATE_ADD(date_value, INTERVAL 1 WEEK) AS DATE) <= CAST('2020-02-01' AS DATE)), dates AS (SELECT * FROM (SELECT date_value FROM _generated_dates) AS _generated_dates) SELECT * FROM dates",
        "redshift": "WITH RECURSIVE _generated_dates(date_value) AS (SELECT CAST('2020-01-01' AS DATE) AS date_value UNION ALL SELECT CAST(DATEADD(WEEK, 1, date_value) AS DATE) FROM _generated_dates WHERE CAST(DATEADD(WEEK, 1, date_value) AS DATE) <= CAST('2020-02-01' AS DATE)), dates AS (SELECT * FROM (SELECT date_value FROM _generated_dates) AS _generated_dates) SELECT * FROM dates",
        "tsql": "WITH _generated_dates(date_value) AS (SELECT CAST('2020-01-01' AS DATE) AS date_value UNION ALL SELECT CAST(DATEADD(WEEK, 1, date_value) AS DATE) FROM _generated_dates WHERE CAST(DATEADD(WEEK, 1, date_value) AS DATE) <= CAST('2020-02-01' AS DATE)), dates AS (SELECT * FROM (SELECT date_value AS date_value FROM _generated_dates) AS _generated_dates) SELECT * FROM dates"
      }
    },
    {
      "sql": "WITH dates1 AS (SELECT * FROM UNNEST(GENERATE_DATE_ARRAY(DATE '2020-01-01', DATE '2020-02-01', INTERVAL 1 WEEK))), dates2 AS (SELECT * FROM UNNEST(GENERATE_DATE_ARRAY(DATE '2020-01-01', DATE '2020-03-01', INTERVAL 1 MONTH))) SELECT * FROM dates1 CROSS JOIN dates2",
      "read": {},
      "write": {
        "mysql": "WITH RECURSIVE _generated_dates(date_value) AS (SELECT CAST('2020-01-01' AS DATE) AS date_value UNION ALL SELECT CAST(DATE_ADD(date_value, INTERVAL 1 WEEK) AS DATE) FROM _generated_dates WHERE CAST(DATE_ADD(date_value, INTERVAL 1 WEEK) AS DATE) <= CAST('2020-02-01' AS DATE)), _generated_dates_1(date_value) AS (SELECT CAST('2020-01-01' AS DATE) AS date_value UNION ALL SELECT CAST(DATE_ADD(date_value, INTERVAL 1 MONTH) AS DATE) FROM _generated_dates_1 WHERE CAST(DATE_ADD(date_value, INTERVAL 1 MONTH) AS DATE) <= CAST('2020-03-01' AS DATE)), dates1 AS (SELECT * FROM (SELECT date_value FROM _generated_dates) AS _generated_dates), dates2 AS (SELECT * FROM (SELECT date_value FROM _generated_dates_1) AS _generated_dates_1) SELECT * FROM dates1 CROSS JOIN dates2",
        "redshift": "WITH RECURSIVE _generated_dates(date_value) AS (SELECT CAST('2020-01-01' AS DATE) AS date_value UNION ALL SELECT CAST(DATEADD(WEEK, 1, date_value) AS DATE) FROM _generated_dates WHERE CAST(DATEADD(WEEK, 1, date_value) AS DATE) <= CAST('2020-02-01' AS DATE)), _generated_dates_1(date_value) AS (SELECT CAST('2020-01-01' AS DATE) AS date_value UNION ALL SELECT CAST(DATEADD(MONTH, 1, date_value) AS DATE) FROM _generated_dates_1 WHERE CAST(DATEADD(MONTH, 1, date_value) AS DATE) <= CAST('2020-03-01' AS DATE)), dates1 AS (SELECT * FROM (SELECT date_value FROM _generated_dates) AS _generated_dates), dates2 AS (SELECT * FROM (SELECT date_value FROM _generated_dates_1) AS _generated_dates_1) SELECT * FROM dates1 CROSS JOIN dates2",
        "tsql": "WITH _generated_dates(date_value) AS (SELECT CAST('2020-01-01' AS DATE) AS date_value UNION ALL SELECT CAST(DATEADD(WEEK, 1, date_value) AS DATE) FROM _generated_dates WHERE CAST(DATEADD(WEEK, 1, date_value) AS DATE) <= CAST('2020-02-01' AS DATE)), _generated_dates_1(date_value) AS (SELECT CAST('2020-01-01' AS DATE) AS date_value UNION ALL SELECT CAST(DATEADD(MONTH, 1, date_value) AS DATE) FROM _generated_dates_1 WHERE CAST(DATEADD(MONTH, 1, date_value) AS DATE) <= CAST('2020-03-01' AS DATE)), dates1 AS (SELECT * FROM (SELECT date_value AS date_value FROM _generated_dates) AS _generated_dates), dates2 AS (SELECT * FROM (SELECT date_value AS date_value FROM _generated_dates_1) AS _generated_dates_1) SELECT * FROM dates1 CROSS JOIN dates2"
      }
    },
    {
      "sql": "SELECT * FROM UNNEST(GENERATE_DATE_ARRAY(DATE '2020-01-01', DATE '2020-02-01', INTERVAL 1 WEEK)) AS _q(date_week)",
      "read": {},
      "write": {
        "mysql": "WITH RECURSIVE _generated_dates(date_week) AS (SELECT CAST('2020-01-01' AS DATE) AS date_week UNION ALL SELECT CAST(DATE_ADD(date_week, INTERVAL 1 WEEK) AS DATE) FROM _generated_dates WHERE CAST(DATE_ADD(date_week, INTERVAL 1 WEEK) AS DATE) <= CAST('2020-02-01' AS DATE)) SELECT * FROM (SELECT date_week FROM _generated_dates) AS _generated_dates",
        "redshift": "WITH RECURSIVE _generated_dates(date_week) AS (SELECT CAST('2020-01-01' AS DATE) AS date_week UNION ALL SELECT CAST(DATEADD(WEEK, 1, date_week) AS DATE) FROM _generated_dates WHERE CAST(DATEADD(WEEK, 1, date_week) AS DATE) <= CAST('2020-02-01' AS DATE)) SELECT * FROM (SELECT date_week FROM _generated_dates) AS _generated_dates",
        "snowflake": "SELECT * FROM (SELECT DATEADD(WEEK, CAST(date_week AS INT), CAST('2020-01-01' AS DATE)) AS date_week FROM TABLE(FLATTEN(INPUT => ARRAY_GENERATE_RANGE(0, (DATEDIFF(WEEK, CAST('2020-01-01' AS DATE), CAST('2020-02-01' AS DATE)) + 1 - 1) + 1))) AS _q(seq, key, path, index, date_week, this)) AS _q(date_week)",
        "tsql": "WITH _generated_dates(date_week) AS (SELECT CAST('2020-01-01' AS DATE) AS date_week UNION ALL SELECT CAST(DATEADD(WEEK, 1, date_week) AS DATE) FROM _generated_dates WHERE CAST(DATEADD(WEEK, 1, date_week) AS DATE) <= CAST('2020-02-01' AS DATE)) SELECT * FROM (SELECT date_week AS date_week FROM _generated_dates) AS _generated_dates"
      }
    },
    {
      "sql": "SELECT ARRAY_LENGTH(GENERATE_DATE_ARRAY(DATE '2020-01-01', DATE '2020-02-01', INTERVAL 1 WEEK))",
      "read": {},
      "write": {
        "snowflake": "SELECT ARRAY_SIZE((SELECT ARRAY_AGG(*) FROM (SELECT DATEADD(WEEK, CAST(value AS INT), CAST('2020-01-01' AS DATE)) AS value FROM TABLE(FLATTEN(INPUT => ARRAY_GENERATE_RANGE(0, (DATEDIFF(WEEK, CAST('2020-01-01' AS DATE), CAST('2020-02-01' AS DATE)) + 1 - 1) + 1))) AS _t0(seq, key, path, index, value, this))))"
      }
    },
    {
      "sql": "SELECT 1 EXCEPT ALL SELECT 1",
      "read": {},
      "write": {
        "clickhouse": "SELECT 1 EXCEPT SELECT 1",
        "databricks": "SELECT 1 EXCEPT ALL SELECT 1",
        "duckdb": "SELECT 1 EXCEPT ALL SELECT 1",
        "mysql": "SELECT 1 EXCEPT ALL SELECT 1",
        "oracle": "SELECT 1 EXCEPT ALL SELECT 1",
        "postgres": "SELECT 1 EXCEPT ALL SELECT 1",
        "spark": "SELECT 1 EXCEPT ALL SELECT 1",
        "trino": "SELECT 1 EXCEPT ALL SELECT 1"
      }
    },
    {
      "sql": "TRIM('abc', 'a')",
      "read": {
        "bigquery": "TRIM('abc', 'a')",
        "snowflake": "TRIM('abc', 'a')",
        "hive": "TRIM('abc', 'a')",
        "spark2": "TRIM('a', 'abc')",
        "spark": "TRIM('a', 'abc')",
        "databricks": "TRIM('a', 'abc')"
      },
      "write": {
        "bigquery": "TRIM('abc', 'a')",
        "snowflake": "TRIM('abc', 'a')",
        "hive": "TRIM('a' FROM 'abc')",
        "spark2": "TRIM('a' FROM 'abc')",
        "spark": "TRIM('a' FROM 'abc')",
        "databricks": "TRIM('a' FROM 'abc')"
      }
    },
    {
      "sql": "LTRIM('Hello World', 'H')",
      "read": {
        "oracle": "LTRIM('Hello World', 'H')",
        "clickhouse": "TRIM(LEADING 'H' FROM 'Hello World')",
        "snowflake": "LTRIM('Hello World', 'H')",
        "bigquery": "LTRIM('Hello World', 'H')",
        "hive": "LTRIM('Hello World', 'H')",
        "spark2": "LTRIM('H', 'Hello World')",
        "spark": "LTRIM('H', 'Hello World')",
        "databricks": "LTRIM('H', 'Hello World')"
      },
      "write": {
        "clickhouse": "TRIM(LEADING 'H' FROM 'Hello World')",
        "oracle": "LTRIM('Hello World', 'H')",
        "snowflake": "LTRIM('Hello World', 'H')",
        "bigquery": "LTRIM('Hello World', 'H')",
        "hive": "TRIM(LEADING 'H' FROM 'Hello World')",
        "spark2": "TRIM(LEADING 'H' FROM 'Hello World')",
        "spark": "TRIM(LEADING 'H' FROM 'Hello World')",
        "databricks": "TRIM(LEADING 'H' FROM 'Hello World')"
      }
    },
    {
      "sql": "RTRIM('Hello World', 'd')",
      "read": {
        "clickhouse": "TRIM(TRAILING 'd' FROM 'Hello World')",
        "oracle": "RTRIM('Hello World', 'd')",
        "snowflake": "RTRIM('Hello World', 'd')",
        "bigquery": "RTRIM('Hello World', 'd')",
        "hive": "RTRIM('Hello World', 'd')",
        "spark2": "RTRIM('d', 'Hello World')",
        "spark": "RTRIM('d', 'Hello World')",
        "databricks": "RTRIM('d', 'Hello World')"
      },
      "write": {
        "clickhouse": "TRIM(TRAILING 'd' FROM 'Hello World')",
        "oracle": "RTRIM('Hello World', 'd')",
        "snowflake": "RTRIM('Hello World', 'd')",
        "bigquery": "RTRIM('Hello World', 'd')",
        "hive": "TRIM(TRAILING 'd' FROM 'Hello World')",
        "spark2": "TRIM(TRAILING 'd' FROM 'Hello World')",
        "spark": "TRIM(TRAILING 'd' FROM 'Hello World')",
        "databricks": "TRIM(TRAILING 'd' FROM 'Hello World')"
      }
    },
    {
      "sql": "LTRIM('Hello World')",
      "read": {
        "clickhouse": "LTRIM('Hello World')",
        "oracle": "LTRIM('Hello World')",
        "snowflake": "LTRIM('Hello World')",
        "bigquery": "LTRIM('Hello World')",
        "hive": "LTRIM('Hello World')",
        "spark2": "LTRIM('Hello World')",
        "spark": "LTRIM('Hello World')",
        "databricks": "LTRIM('Hello World')"
      },
      "write": {
        "clickhouse": "LTRIM('Hello World')",
        "oracle": "LTRIM('Hello World')",
        "snowflake": "LTRIM('Hello World')",
        "bigquery": "LTRIM('Hello World')",
        "hive": "LTRIM('Hello World')",
        "spark2": "LTRIM('Hello World')",
        "spark": "LTRIM('Hello World')",
        "databricks": "LTRIM('Hello World')"
      }
    },
    {
      "sql": "RTRIM('Hello World')",
      "read": {
        "clickhouse": "RTRIM('Hello World')",
        "oracle": "RTRIM('Hello World')",
        "snowflake": "RTRIM('Hello World')",
        "bigquery": "RTRIM('Hello World')",
        "hive": "RTRIM('Hello World')",
        "spark2": "RTRIM('Hello World')",
        "spark": "RTRIM('Hello World')",
        "databricks": "RTRIM('Hello World')"
      },
      "write": {
        "clickhouse": "RTRIM('Hello World')",
        "oracle": "RTRIM('Hello World')",
        "snowflake": "RTRIM('Hello World')",
        "bigquery": "RTRIM('Hello World')",
        "hive": "RTRIM('Hello World')",
        "spark2": "RTRIM('Hello World')",
        "spark": "RTRIM('Hello World')",
        "databricks": "RTRIM('Hello World')"
      }
    },
    {
      "sql": "UUID()",
      "read": {
        "hive": "UUID()",
        "spark2": "UUID()",
        "spark": "UUID()",
        "databricks": "UUID()",
        "duckdb": "UUID()",
        "presto": "UUID()",
        "trino": "UUID()",
        "mysql": "UUID()",
        "postgres": "GEN_RANDOM_UUID()",
        "snowflake": "UUID_STRING()",
        "tsql": "NEWID()"
      },
      "write": {
        "hive": "UUID()",
        "spark2": "UUID()",
        "spark": "UUID()",
        "databricks": "UUID()",
        "duckdb": "UUID()",
        "presto": "UUID()",
        "trino": "UUID()",
        "mysql": "UUID()",
        "postgres": "GEN_RANDOM_UUID()",
        "bigquery": "GENERATE_UUID()",
        "snowflake": "UUID_STRING()",
        "tsql": "NEWID()"
      }
    },
    {
      "sql": "SELECT 1 AS \"x`\"",
      "read": {
        "clickhouse": "SELECT 1 AS `x\\``"
      },
      "write": {
        "clickhouse": "SELECT 1 AS \"x`\""
      }
    },
    {
      "sql": "CURRENT_SCHEMA()",
      "read": {
        "mysql": "SCHEMA()",
        "postgres": "CURRENT_SCHEMA()",
        "tsql": "SCHEMA_NAME()"
      },
      "write": {
        "sqlite": "'main'",
        "mysql": "SCHEMA()",
        "postgres": "CURRENT_SCHEMA",
        "tsql": "SCHEMA_NAME()"
      }
    },
    {
      "sql": "ASCII('A')",
      "read": {
        "bigquery": "ASCII('A')",
        "clickhouse": "ASCII('A')",
        "databricks": "ASCII('A')",
        "hive": "ASCII('A')",
        "mysql": "ASCII('A')",
        "postgres": "ASCII('A')",
        "redshift": "ASCII('A')",
        "snowflake": "ASCII('A')",
        "tsql": "ASCII('A')"
      },
      "write": {
        "bigquery": "ASCII('A')",
        "clickhouse": "ASCII('A')",
        "databricks": "ASCII('A')",
        "hive": "ASCII('A')",
        "mysql": "ASCII('A')",
        "postgres": "ASCII('A')",
        "redshift": "ASCII('A')",
        "snowflake": "ASCII('A')",
        "tsql": "ASCII('A')"
      }
    },
    {
      "sql": "SELECT x BETWEEN 2 AND 10",
      "read": {
        "clickhouse": "SELECT x BETWEEN 2 AND 10",
        "dremio": "SELECT x BETWEEN 2 AND 10",
        "duckdb": "SELECT x BETWEEN 2 AND 10",
        "materialize": "SELECT x BETWEEN 2 AND 10",
        "mysql": "SELECT x BETWEEN 2 AND 10",
        "oracle": "SELECT x BETWEEN 2 AND 10",
        "postgres": "SELECT x BETWEEN 2 AND 10",
        "redshift": "SELECT x BETWEEN 2 AND 10",
        "risingwave": "SELECT x BETWEEN 2 AND 10",
        "tsql": "SELECT x BETWEEN 2 AND 10"
      },
      "write": {
        "clickhouse": "SELECT x BETWEEN 2 AND 10",
        "dremio": "SELECT x BETWEEN 2 AND 10",
        "duckdb": "SELECT x BETWEEN 2 AND 10",
        "materialize": "SELECT x BETWEEN 2 AND 10",
        "mysql": "SELECT x BETWEEN 2 AND 10",
        "oracle": "SELECT x BETWEEN 2 AND 10",
        "postgres": "SELECT x BETWEEN 2 AND 10",
        "redshift": "SELECT x BETWEEN 2 AND 10",
        "risingwave": "SELECT x BETWEEN 2 AND 10",
        "tsql": "SELECT x BETWEEN 2 AND 10"
      }
    },
    {
      "sql": "SELECT x BETWEEN SYMMETRIC 10 AND 2",
      "read": {},
      "write": {
        "clickhouse": "SELECT (x BETWEEN 10 AND 2 OR x BETWEEN 2 AND 10)",
        "dremio": "SELECT x BETWEEN SYMMETRIC 10 AND 2",
        "duckdb": "SELECT (x BETWEEN 10 AND 2 OR x BETWEEN 2 AND 10)",
        "materialize": "SELECT (x BETWEEN 10 AND 2 OR x BETWEEN 2 AND 10)",
        "mysql": "SELECT (x BETWEEN 10 AND 2 OR x BETWEEN 2 AND 10)",
        "oracle": "SELECT (x BETWEEN 10 AND 2 OR x BETWEEN 2 AND 10)",
        "postgres": "SELECT x BETWEEN SYMMETRIC 10 AND 2",
        "redshift": "SELECT (x BETWEEN 10 AND 2 OR x BETWEEN 2 AND 10)",
        "risingwave": "SELECT (x BETWEEN 10 AND 2 OR x BETWEEN 2 AND 10)",
        "tsql": "SELECT (x BETWEEN 10 AND 2 OR x BETWEEN 2 AND 10)"
      }
    },
    {
      "sql": "SELECT x BETWEEN ASYMMETRIC 10 AND 2",
      "read": {},
      "write": {
        "clickhouse": "SELECT x BETWEEN 10 AND 2",
        "dremio": "SELECT x BETWEEN ASYMMETRIC 10 AND 2",
        "duckdb": "SELECT x BETWEEN 10 AND 2",
        "materialize": "SELECT x BETWEEN 10 AND 2",
        "mysql": "SELECT x BETWEEN 10 AND 2",
        "oracle": "SELECT x BETWEEN 10 AND 2",
        "postgres": "SELECT x BETWEEN ASYMMETRIC 10 AND 2",
        "redshift": "SELECT x BETWEEN 10 AND 2",
        "risingwave": "SELECT x BETWEEN 10 AND 2",
        "tsql": "SELECT x BETWEEN 10 AND 2"
      }
    },
    {
      "sql": "SELECT 'foo' LIKE ANY((('bar', 'fo%')))",
      "read": {},
      "write": {
        "duckdb": "SELECT 'foo' LIKE 'bar' OR 'foo' LIKE 'fo%'"
      }
    },
    {
      "sql": "DATE_FROM_UNIX_DATE(1)",
      "read": {},
      "write": {
        "bigquery": "DATE_FROM_UNIX_DATE(1)",
        "spark": "DATE_FROM_UNIX_DATE(1)",
        "databricks": "DATE_FROM_UNIX_DATE(1)",
        "snowflake": "DATEADD(DAY, 1, CAST('1970-01-01' AS DATE))",
        "duckdb": "CAST('1970-01-01' AS DATE) + INTERVAL 1 DAY",
        "redshift": "DATEADD(DAY, 1, CAST('1970-01-01' AS DATE))",
        "presto": "DATE_ADD('DAY', 1, CAST('1970-01-01' AS DATE))",
        "trino": "DATE_ADD('DAY', 1, CAST('1970-01-01' AS DATE))"
      }
    },
    {
      "sql": "WEEKOFYEAR(CAST('2025-01-01' AS DATE))",
      "read": {},
      "write": {
        "duckdb": "WEEKOFYEAR(CAST('2025-01-01' AS DATE))",
        "exasol": "WEEK(CAST('2025-01-01' AS DATE))",
        "hive": "WEEKOFYEAR(CAST('2025-01-01' AS DATE))",
        "mysql": "WEEKOFYEAR(CAST('2025-01-01' AS DATE))",
        "spark": "WEEKOFYEAR(CAST('2025-01-01' AS DATE))",
        "snowflake": "WEEKISO(CAST('2025-01-01' AS DATE))"
      }
    },
    {
      "sql": "JUSTIFY_DAYS(INTERVAL '1' DAY)",
      "read": {
        "bigquery": "JUSTIFY_DAYS(INTERVAL '1' DAY)",
        "postgres": "JUSTIFY_DAYS(INTERVAL '1 DAY')",
        "materialize": "JUSTIFY_DAYS(INTERVAL '1 DAY')"
      },
      "write": {
        "bigquery": "JUSTIFY_DAYS(INTERVAL '1' DAY)",
        "postgres": "JUSTIFY_DAYS(INTERVAL '1 DAY')",
        "materialize": "JUSTIFY_DAYS(INTERVAL '1 DAY')"
      }
    },
    {
      "sql": "JUSTIFY_HOURS(INTERVAL '1' HOUR)",
      "read": {
        "bigquery": "JUSTIFY_HOURS(INTERVAL '1' HOUR)",
        "postgres": "JUSTIFY_HOURS(INTERVAL '1 HOUR')",
        "materialize": "JUSTIFY_HOURS(INTERVAL '1 HOUR')"
      },
      "write": {
        "bigquery": "JUSTIFY_HOURS(INTERVAL '1' HOUR)",
        "postgres": "JUSTIFY_HOURS(INTERVAL '1 HOUR')",
        "materialize": "JUSTIFY_HOURS(INTERVAL '1 HOUR')"
      }
    },
    {
      "sql": "JUSTIFY_INTERVAL(INTERVAL '1' HOUR)",
      "read": {
        "bigquery": "JUSTIFY_INTERVAL(INTERVAL '1' HOUR)",
        "postgres": "JUSTIFY_INTERVAL(INTERVAL '1 HOUR')",
        "materialize": "JUSTIFY_INTERVAL(INTERVAL '1 HOUR')"
      },
      "write": {
        "bigquery": "JUSTIFY_INTERVAL(INTERVAL '1' HOUR)",
        "postgres": "JUSTIFY_INTERVAL(INTERVAL '1 HOUR')",
        "materialize": "JUSTIFY_INTERVAL(INTERVAL '1 HOUR')"
      }
    },
    {
      "sql": "UNIX_MICROS(foo)",
      "read": {
        "bigquery": "UNIX_MICROS(foo)",
        "spark": "UNIX_MICROS(foo)",
        "databricks": "UNIX_MICROS(foo)"
      },
      "write": {
        "bigquery": "UNIX_MICROS(foo)",
        "spark": "UNIX_MICROS(foo)",
        "databricks": "UNIX_MICROS(foo)"
      }
    },
    {
      "sql": "UNIX_MILLIS(foo)",
      "read": {
        "bigquery": "UNIX_MILLIS(foo)",
        "spark": "UNIX_MILLIS(foo)",
        "databricks": "UNIX_MILLIS(foo)"
      },
      "write": {
        "bigquery": "UNIX_MILLIS(foo)",
        "spark": "UNIX_MILLIS(foo)",
        "databricks": "UNIX_MILLIS(foo)"
      }
    },
    {
      "sql": "REVERSE(x)",
      "read": {
        "bigquery": "REVERSE(x)",
        "hive": "REVERSE(x)",
        "spark2": "REVERSE(x)",
        "spark": "REVERSE(x)",
        "databricks": "REVERSE(x)",
        "mysql": "REVERSE(x)",
        "postgres": "REVERSE(x)",
        "tsql": "REVERSE(x)",
        "snowflake": "REVERSE(x)",
        "doris": "REVERSE(x)",
        "presto": "REVERSE(x)",
        "trino": "REVERSE(x)",
        "clickhouse": "REVERSE(x)",
        "redshift": "REVERSE(x)"
      },
      "write": {
        "bigquery": "REVERSE(x)",
        "hive": "REVERSE(x)",
        "spark2": "REVERSE(x)",
        "spark": "REVERSE(x)",
        "databricks": "REVERSE(x)",
        "mysql": "REVERSE(x)",
        "postgres": "REVERSE(x)",
        "tsql": "REVERSE(x)",
        "snowflake": "REVERSE(x)",
        "doris": "REVERSE(x)",
        "presto": "REVERSE(x)",
        "trino": "REVERSE(x)",
        "clickhouse": "REVERSE(x)",
        "redshift": "REVERSE(x)"
      }
    },
    {
      "sql": "REGR_COUNT(x, y)",
      "read": {
        "databricks": "REGR_COUNT(x, y)",
        "duckdb": "REGR_COUNT(x, y)",
        "exasol": "REGR_COUNT(x, y)",
        "hive": "REGR_COUNT(x, y)",
        "oracle": "REGR_COUNT(x, y)",
        "postgres": "REGR_COUNT(x, y)",
        "presto": "REGR_COUNT(x, y)",
        "snowflake": "REGR_COUNT(x, y)",
        "spark": "REGR_COUNT(x, y)",
        "teradata": "REGR_COUNT(x, y)",
        "trino": "REGR_COUNT(x, y)"
      },
      "write": {
        "databricks": "REGR_COUNT(x, y)",
        "duckdb": "REGR_COUNT(x, y)",
        "exasol": "REGR_COUNT(x, y)",
        "hive": "REGR_COUNT(x, y)",
        "oracle": "REGR_COUNT(x, y)",
        "postgres": "REGR_COUNT(x, y)",
        "presto": "REGR_COUNT(x, y)",
        "snowflake": "REGR_COUNT(x, y)",
        "spark": "REGR_COUNT(x, y)",
        "teradata": "REGR_COUNT(x, y)",
        "trino": "REGR_COUNT(x, y)"
      }
    },
    {
      "sql": "REGR_INTERCEPT(x, y)",
      "read": {
        "databricks": "REGR_INTERCEPT(x, y)",
        "duckdb": "REGR_INTERCEPT(x, y)",
        "exasol": "REGR_INTERCEPT(x, y)",
        "hive": "REGR_INTERCEPT(x, y)",
        "oracle": "REGR_INTERCEPT(x, y)",
        "postgres": "REGR_INTERCEPT(x, y)",
        "presto": "REGR_INTERCEPT(x, y)",
        "snowflake": "REGR_INTERCEPT(x, y)",
        "spark": "REGR_INTERCEPT(x, y)",
        "teradata": "REGR_INTERCEPT(x, y)"
      },
      "write": {
        "databricks": "REGR_INTERCEPT(x, y)",
        "duckdb": "REGR_INTERCEPT(x, y)",
        "exasol": "REGR_INTERCEPT(x, y)",
        "hive": "REGR_INTERCEPT(x, y)",
        "oracle": "REGR_INTERCEPT(x, y)",
        "postgres": "REGR_INTERCEPT(x, y)",
        "presto": "REGR_INTERCEPT(x, y)",
        "snowflake": "REGR_INTERCEPT(x, y)",
        "spark": "REGR_INTERCEPT(x, y)",
        "teradata": "REGR_INTERCEPT(x, y)"
      }
    },
    {
      "sql": "REGR_R2(x, y)",
      "read": {
        "databricks": "REGR_R2(x, y)",
        "duckdb": "REGR_R2(x, y)",
        "exasol": "REGR_R2(x, y)",
        "hive": "REGR_R2(x, y)",
        "oracle": "REGR_R2(x, y)",
        "postgres": "REGR_R2(x, y)",
        "presto": "REGR_R2(x, y)",
        "snowflake": "REGR_R2(x, y)",
        "spark": "REGR_R2(x, y)",
        "teradata": "REGR_R2(x, y)"
      },
      "write": {
        "databricks": "REGR_R2(x, y)",
        "duckdb": "REGR_R2(x, y)",
        "exasol": "REGR_R2(x, y)",
        "hive": "REGR_R2(x, y)",
        "oracle": "REGR_R2(x, y)",
        "postgres": "REGR_R2(x, y)",
        "presto": "REGR_R2(x, y)",
        "snowflake": "REGR_R2(x, y)",
        "spark": "REGR_R2(x, y)",
        "teradata": "REGR_R2(x, y)"
      }
    },
    {
      "sql": "REGR_SLOPE(x, y)",
      "read": {
        "databricks": "REGR_SLOPE(x, y)",
        "duckdb": "REGR_SLOPE(x, y)",
        "exasol": "REGR_SLOPE(x, y)",
        "oracle": "REGR_SLOPE(x, y)",
        "postgres": "REGR_SLOPE(x, y)",
        "presto": "REGR_SLOPE(x, y)",
        "snowflake": "REGR_SLOPE(x, y)",
        "spark": "REGR_SLOPE(x, y)",
        "teradata": "REGR_SLOPE(x, y)",
        "trino": "REGR_SLOPE(x, y)"
      },
      "write": {
        "databricks": "REGR_SLOPE(x, y)",
        "duckdb": "REGR_SLOPE(x, y)",
        "exasol": "REGR_SLOPE(x, y)",
        "oracle": "REGR_SLOPE(x, y)",
        "postgres": "REGR_SLOPE(x, y)",
        "presto": "REGR_SLOPE(x, y)",
        "snowflake": "REGR_SLOPE(x, y)",
        "spark": "REGR_SLOPE(x, y)",
        "teradata": "REGR_SLOPE(x, y)",
        "trino": "REGR_SLOPE(x, y)"
      }
    },
    {
      "sql": "REGR_SXX(x, y)",
      "read": {
        "databricks": "REGR_SXX(x, y)",
        "duckdb": "REGR_SXX(x, y)",
        "exasol": "REGR_SXX(x, y)",
        "hive": "REGR_SXX(x, y)",
        "oracle": "REGR_SXX(x, y)",
        "postgres": "REGR_SXX(x, y)",
        "presto": "REGR_SXX(x, y)",
        "snowflake": "REGR_SXX(x, y)",
        "spark": "REGR_SXX(x, y)",
        "teradata": "REGR_SXX(x, y)"
      },
      "write": {
        "databricks": "REGR_SXX(x, y)",
        "duckdb": "REGR_SXX(x, y)",
        "exasol": "REGR_SXX(x, y)",
        "hive": "REGR_SXX(x, y)",
        "oracle": "REGR_SXX(x, y)",
        "postgres": "REGR_SXX(x, y)",
        "presto": "REGR_SXX(x, y)",
        "snowflake": "REGR_SXX(x, y)",
        "spark": "REGR_SXX(x, y)",
        "teradata": "REGR_SXX(x, y)"
      }
    },
    {
      "sql": "REGR_SXY(x, y)",
      "read": {
        "databricks": "REGR_SXY(x, y)",
        "duckdb": "REGR_SXY(x, y)",
        "exasol": "REGR_SXY(x, y)",
        "hive": "REGR_SXY(x, y)",
        "oracle": "REGR_SXY(x, y)",
        "postgres": "REGR_SXY(x, y)",
        "presto": "REGR_SXY(x, y)",
        "snowflake": "REGR_SXY(x, y)",
        "spark": "REGR_SXY(x, y)",
        "teradata": "REGR_SXY(x, y)"
      },
      "write": {
        "databricks": "REGR_SXY(x, y)",
        "duckdb": "REGR_SXY(x, y)",
        "exasol": "REGR_SXY(x, y)",
        "hive": "REGR_SXY(x, y)",
        "oracle": "REGR_SXY(x, y)",
        "postgres": "REGR_SXY(x, y)",
        "presto": "REGR_SXY(x, y)",
        "snowflake": "REGR_SXY(x, y)",
        "spark": "REGR_SXY(x, y)",
        "teradata": "REGR_SXY(x, y)"
      }
    },
    {
      "sql": "REGR_SYY(x, y)",
      "read": {
        "databricks": "REGR_SYY(x, y)",
        "duckdb": "REGR_SYY(x, y)",
        "exasol": "REGR_SYY(x, y)",
        "hive": "REGR_SYY(x, y)",
        "oracle": "REGR_SYY(x, y)",
        "postgres": "REGR_SYY(x, y)",
        "presto": "REGR_SYY(x, y)",
        "snowflake": "REGR_SYY(x, y)",
        "spark": "REGR_SYY(x, y)",
        "teradata": "REGR_SYY(x, y)"
      },
      "write": {
        "databricks": "REGR_SYY(x, y)",
        "duckdb": "REGR_SYY(x, y)",
        "exasol": "REGR_SYY(x, y)",
        "hive": "REGR_SYY(x, y)",
        "oracle": "REGR_SYY(x, y)",
        "postgres": "REGR_SYY(x, y)",
        "presto": "REGR_SYY(x, y)",
        "snowflake": "REGR_SYY(x, y)",
        "spark": "REGR_SYY(x, y)",
        "teradata": "REGR_SYY(x, y)"
      }
    },
    {
      "sql": "TRANSLATE(x, y, z)",
      "read": {
        "bigquery": "TRANSLATE(x, y, z)",
        "hive": "TRANSLATE(x, y, z)",
        "spark2": "TRANSLATE(x, y, z)",
        "spark": "TRANSLATE(x, y, z)",
        "databricks": "TRANSLATE(x, y, z)",
        "postgres": "TRANSLATE(x, y, z)",
        "tsql": "TRANSLATE(x, y, z)",
        "snowflake": "TRANSLATE(x, y, z)",
        "doris": "TRANSLATE(x, y, z)",
        "trino": "TRANSLATE(x, y, z)",
        "clickhouse": "TRANSLATE(x, y, z)",
        "redshift": "TRANSLATE(x, y, z)",
        "oracle": "TRANSLATE(x, y, z)"
      },
      "write": {
        "bigquery": "TRANSLATE(x, y, z)",
        "hive": "TRANSLATE(x, y, z)",
        "spark2": "TRANSLATE(x, y, z)",
        "spark": "TRANSLATE(x, y, z)",
        "databricks": "TRANSLATE(x, y, z)",
        "postgres": "TRANSLATE(x, y, z)",
        "tsql": "TRANSLATE(x, y, z)",
        "snowflake": "TRANSLATE(x, y, z)",
        "doris": "TRANSLATE(x, y, z)",
        "trino": "TRANSLATE(x, y, z)",
        "clickhouse": "TRANSLATE(x, y, z)",
        "redshift": "TRANSLATE(x, y, z)",
        "oracle": "TRANSLATE(x, y, z)"
      }
    },
    {
      "sql": "SOUNDEX(x)",
      "read": {
        "bigquery": "SOUNDEX(x)",
        "hive": "SOUNDEX(x)",
        "spark2": "SOUNDEX(x)",
        "spark": "SOUNDEX(x)",
        "databricks": "SOUNDEX(x)",
        "mysql": "SOUNDEX(x)",
        "postgres": "SOUNDEX(x)",
        "tsql": "SOUNDEX(x)",
        "snowflake": "SOUNDEX(x)",
        "dremio": "SOUNDEX(x)",
        "trino": "SOUNDEX(x)",
        "clickhouse": "SOUNDEX(x)",
        "redshift": "SOUNDEX(x)",
        "oracle": "SOUNDEX(x)"
      },
      "write": {
        "bigquery": "SOUNDEX(x)",
        "hive": "SOUNDEX(x)",
        "spark2": "SOUNDEX(x)",
        "spark": "SOUNDEX(x)",
        "databricks": "SOUNDEX(x)",
        "mysql": "SOUNDEX(x)",
        "postgres": "SOUNDEX(x)",
        "tsql": "SOUNDEX(x)",
        "snowflake": "SOUNDEX(x)",
        "dremio": "SOUNDEX(x)",
        "trino": "SOUNDEX(x)",
        "clickhouse": "SOUNDEX(x)",
        "redshift": "SOUNDEX(x)",
        "oracle": "SOUNDEX(x)"
      }
    },
    {
      "sql": "GROUPING(x)",
      "read": {
        "bigquery": "GROUPING(x)",
        "hive": "GROUPING(x)",
        "spark2": "GROUPING(x)",
        "spark": "GROUPING(x)",
        "databricks": "GROUPING(x)",
        "mysql": "GROUPING(x)",
        "postgres": "GROUPING(x)",
        "tsql": "GROUPING(x)",
        "snowflake": "GROUPING(x)",
        "clickhouse": "GROUPING(x)",
        "redshift": "GROUPING(x)",
        "oracle": "GROUPING(x)"
      },
      "write": {
        "bigquery": "GROUPING(x)",
        "hive": "GROUPING(x)",
        "spark2": "GROUPING(x)",
        "spark": "GROUPING(x)",
        "databricks": "GROUPING(x)",
        "mysql": "GROUPING(x)",
        "postgres": "GROUPING(x)",
        "tsql": "GROUPING(x)",
        "snowflake": "GROUPING(x)",
        "clickhouse": "GROUPING(x)",
        "redshift": "GROUPING(x)",
        "oracle": "GROUPING(x)"
      }
    },
    {
      "sql": "GROUPING(col1, col2, col3)",
      "read": {
        "snowflake": "GROUPING(col1, col2, col3)",
        "mysql": "GROUPING(col1, col2, col3)",
        "postgres": "GROUPING(col1, col2, col3)",
        "clickhouse": "GROUPING(col1, col2, col3)",
        "redshift": "GROUPING(col1, col2, col3)"
      },
      "write": {
        "snowflake": "GROUPING(col1, col2, col3)",
        "mysql": "GROUPING(col1, col2, col3)",
        "postgres": "GROUPING(col1, col2, col3)",
        "clickhouse": "GROUPING(col1, col2, col3)",
        "redshift": "GROUPING(col1, col2, col3)"
      }
    },
    {
      "sql": "FARM_FINGERPRINT(x)",
      "read": {
        "bigquery": "FARM_FINGERPRINT(x)",
        "clickhouse": "farmFingerprint64(x)",
        "redshift": "FARMFINGERPRINT64(x)"
      },
      "write": {
        "bigquery": "FARM_FINGERPRINT(x)",
        "clickhouse": "farmFingerprint64(x)",
        "redshift": "FARMFINGERPRINT64(x)"
      }
    },
    {
      "sql": "FROM_BASE32(x)",
      "read": {
        "bigquery": "FROM_BASE32(x)",
        "presto": "FROM_BASE32(x)",
        "trino": "FROM_BASE32(x)"
      },
      "write": {
        "bigquery": "FROM_BASE32(x)",
        "presto": "FROM_BASE32(x)",
        "trino": "FROM_BASE32(x)"
      }
    },
    {
      "sql": "TO_BASE32(x)",
      "read": {
        "bigquery": "TO_BASE32(x)",
        "presto": "TO_BASE32(x)",
        "trino": "TO_BASE32(x)"
      },
      "write": {
        "bigquery": "TO_BASE32(x)",
        "presto": "TO_BASE32(x)",
        "trino": "TO_BASE32(x)"
      }
    },
    {
      "sql": "REGEXP_INSTR(src, reg)",
      "read": {
        "bigquery": "REGEXP_INSTR(src, reg)",
        "snowflake": "REGEXP_INSTR(src, reg)",
        "oracle": "REGEXP_INSTR(src, reg)",
        "spark": "REGEXP_INSTR(src, reg)",
        "databricks": "REGEXP_INSTR(src, reg)",
        "tsql": "REGEXP_INSTR(src, reg)",
        "mysql": "REGEXP_INSTR(src, reg)",
        "postgres": "REGEXP_INSTR(src, reg)",
        "redshift": "REGEXP_INSTR(src, reg)"
      },
      "write": {
        "bigquery": "REGEXP_INSTR(src, reg)",
        "snowflake": "REGEXP_INSTR(src, reg)",
        "oracle": "REGEXP_INSTR(src, reg)",
        "spark": "REGEXP_INSTR(src, reg)",
        "databricks": "REGEXP_INSTR(src, reg)",
        "tsql": "REGEXP_INSTR(src, reg)",
        "mysql": "REGEXP_INSTR(src, reg)",
        "postgres": "REGEXP_INSTR(src, reg)",
        "redshift": "REGEXP_INSTR(src, reg)"
      }
    },
    {
      "sql": "REGEXP_INSTR(src, reg, pos, occ, opt)",
      "read": {
        "bigquery": "REGEXP_INSTR(src, reg, pos, occ, opt)",
        "snowflake": "REGEXP_INSTR(src, reg, pos, occ, opt)",
        "oracle": "REGEXP_INSTR(src, reg, pos, occ, opt)",
        "tsql": "REGEXP_INSTR(src, reg, pos, occ, opt)",
        "mysql": "REGEXP_INSTR(src, reg, pos, occ, opt)",
        "postgres": "REGEXP_INSTR(src, reg, pos, occ, opt)",
        "redshift": "REGEXP_INSTR(src, reg, pos, occ, opt)"
      },
      "write": {
        "bigquery": "REGEXP_INSTR(src, reg, pos, occ, opt)",
        "snowflake": "REGEXP_INSTR(src, reg, pos, occ, opt)",
        "oracle": "REGEXP_INSTR(src, reg, pos, occ, opt)",
        "tsql": "REGEXP_INSTR(src, reg, pos, occ, opt)",
        "mysql": "REGEXP_INSTR(src, reg, pos, occ, opt)",
        "postgres": "REGEXP_INSTR(src, reg, pos, occ, opt)",
        "redshift": "REGEXP_INSTR(src, reg, pos, occ, opt)"
      }
    },
    {
      "sql": "REGEXP_INSTR(src, reg, pos, occ, opt, par)",
      "read": {
        "snowflake": "REGEXP_INSTR(src, reg, pos, occ, opt, par)",
        "oracle": "REGEXP_INSTR(src, reg, pos, occ, opt, par)",
        "tsql": "REGEXP_INSTR(src, reg, pos, occ, opt, par)",
        "mysql": "REGEXP_INSTR(src, reg, pos, occ, opt, par)",
        "postgres": "REGEXP_INSTR(src, reg, pos, occ, opt, par)",
        "redshift": "REGEXP_INSTR(src, reg, pos, occ, opt, par)"
      },
      "write": {
        "snowflake": "REGEXP_INSTR(src, reg, pos, occ, opt, par)",
        "oracle": "REGEXP_INSTR(src, reg, pos, occ, opt, par)",
        "tsql": "REGEXP_INSTR(src, reg, pos, occ, opt, par)",
        "mysql": "REGEXP_INSTR(src, reg, pos, occ, opt, par)",
        "postgres": "REGEXP_INSTR(src, reg, pos, occ, opt, par)",
        "redshift": "REGEXP_INSTR(src, reg, pos, occ, opt, par)"
      }
    },
    {
      "sql": "REGEXP_INSTR(src, reg, pos, occ, opt, par, grp)",
      "read": {
        "snowflake": "REGEXP_INSTR(src, reg, pos, occ, opt, par, grp)",
        "oracle": "REGEXP_INSTR(src, reg, pos, occ, opt, par, grp)",
        "tsql": "REGEXP_INSTR(src, reg, pos, occ, opt, par, grp)",
        "postgres": "REGEXP_INSTR(src, reg, pos, occ, opt, par, grp)"
      },
      "write": {
        "snowflake": "REGEXP_INSTR(src, reg, pos, occ, opt, par, grp)",
        "oracle": "REGEXP_INSTR(src, reg, pos, occ, opt, par, grp)",
        "tsql": "REGEXP_INSTR(src, reg, pos, occ, opt, par, grp)",
        "postgres": "REGEXP_INSTR(src, reg, pos, occ, opt, par, grp)"
      }
    },
    {
      "sql": "FORMAT('str fmt1 fmt2', 1, 'a')",
      "read": {
        "bigquery": "FORMAT('str fmt1 fmt2', 1, 'a')",
        "postgres": "FORMAT('str fmt1 fmt2', 1, 'a')",
        "duckdb": "FORMAT('str fmt1 fmt2', 1, 'a')"
      },
      "write": {
        "bigquery": "FORMAT('str fmt1 fmt2', 1, 'a')",
        "postgres": "FORMAT('str fmt1 fmt2', 1, 'a')",
        "spark2": "FORMAT_STRING('str fmt1 fmt2', 1, 'a')",
        "spark": "FORMAT_STRING('str fmt1 fmt2', 1, 'a')",
        "databricks": "FORMAT_STRING('str fmt1 fmt2', 1, 'a')",
        "duckdb": "FORMAT('str fmt1 fmt2', 1, 'a')"
      }
    },
    {
      "sql": "JSON_ARRAY_APPEND(PARSE_JSON('[\"a\", \"b\", \"c\"]'), '$', 1)",
      "read": {
        "bigquery": "JSON_ARRAY_APPEND(PARSE_JSON('[\"a\", \"b\", \"c\"]'), '$', 1)"
      },
      "write": {
        "bigquery": "JSON_ARRAY_APPEND(PARSE_JSON('[\"a\", \"b\", \"c\"]'), '$', 1)",
        "mysql": "JSON_ARRAY_APPEND('[\"a\", \"b\", \"c\"]', '$', 1)"
      }
    },
    {
      "sql": "JSON_ARRAY_INSERT(PARSE_JSON('[\"a\", [\"b\", \"c\"], \"d\"]'), '$[1]', 1)",
      "read": {
        "bigquery": "JSON_ARRAY_INSERT(PARSE_JSON('[\"a\", [\"b\", \"c\"], \"d\"]'), '$[1]', 1)"
      },
      "write": {
        "bigquery": "JSON_ARRAY_INSERT(PARSE_JSON('[\"a\", [\"b\", \"c\"], \"d\"]'), '$[1]', 1)",
        "mysql": "JSON_ARRAY_INSERT('[\"a\", [\"b\", \"c\"], \"d\"]', '$[1]', 1)"
      }
    },
    {
      "sql": "JSON_REMOVE(PARSE_JSON('[\"a\", [\"b\", \"c\"], \"d\"]'), '$[1]', '$[1]')",
      "read": {
        "bigquery": "JSON_REMOVE(PARSE_JSON('[\"a\", [\"b\", \"c\"], \"d\"]'), '$[1]', '$[1]')"
      },
      "write": {
        "bigquery": "JSON_REMOVE(PARSE_JSON('[\"a\", [\"b\", \"c\"], \"d\"]'), '$[1]', '$[1]')",
        "mysql": "JSON_REMOVE('[\"a\", [\"b\", \"c\"], \"d\"]', '$[1]', '$[1]')",
        "sqlite": "JSON_REMOVE('[\"a\", [\"b\", \"c\"], \"d\"]', '$[1]', '$[1]')"
      }
    },
    {
      "sql": "JSON_SET(PARSE_JSON('{\"a\": 1}'), '$', PARSE_JSON('{\"b\": 2, \"c\": 3}'))",
      "read": {
        "bigquery": "JSON_SET(PARSE_JSON('{\"a\": 1}'), '$', PARSE_JSON('{\"b\": 2, \"c\": 3}'))"
      },
      "write": {
        "bigquery": "JSON_SET(PARSE_JSON('{\"a\": 1}'), '$', PARSE_JSON('{\"b\": 2, \"c\": 3}'))",
        "mysql": "JSON_SET('{\"a\": 1}', '$', '{\"b\": 2, \"c\": 3}')",
        "sqlite": "JSON_SET('{\"a\": 1}', '$', '{\"b\": 2, \"c\": 3}')",
        "doris": "JSON_SET('{\"a\": 1}', '$', '{\"b\": 2, \"c\": 3}')"
      }
    },
    {
      "sql": "JSON_STRIP_NULLS(PARSE_JSON('[{\"f1\":1,\"f2\":null},2,null,3]'))",
      "read": {
        "bigquery": "JSON_STRIP_NULLS(PARSE_JSON('[{\"f1\":1,\"f2\":null},2,null,3]'))"
      },
      "write": {
        "bigquery": "JSON_STRIP_NULLS(PARSE_JSON('[{\"f1\":1,\"f2\":null},2,null,3]'))",
        "postgres": "JSON_STRIP_NULLS(CAST('[{\"f1\":1,\"f2\":null},2,null,3]' AS JSON))"
      }
    },
    {
      "sql": "x IS NULL",
      "read": {
        "bigquery": "x IS UNKNOWN",
        "mysql": "x IS UNKNOWN",
        "postgres": "x IS UNKNOWN",
        "redshift": "x IS UNKNOWN",
        "duckdb": "x IS UNKNOWN",
        "spark": "x IS UNKNOWN",
        "databricks": "x IS UNKNOWN"
      },
      "write": {}
    },
    {
      "sql": "NOT x IS NULL",
      "read": {
        "bigquery": "x IS NOT UNKNOWN",
        "mysql": "x IS NOT UNKNOWN",
        "postgres": "x IS NOT UNKNOWN",
        "redshift": "x IS NOT UNKNOWN",
        "duckdb": "x IS NOT UNKNOWN",
        "spark": "x IS NOT UNKNOWN",
        "databricks": "x IS NOT UNKNOWN"
      },
      "write": {}
    },
    {
      "sql": "SELECT CAST(col IS NULL AS BOOLEAN) FROM (SELECT 1 AS col) AS t",
      "read": {
        "duckdb": "SELECT col IS NULL::BOOLEAN FROM (SELECT 1 AS col) AS t",
        "redshift": "SELECT col IS NULL::BOOLEAN FROM (SELECT 1 AS col) AS t",
        "postgres": "SELECT col IS NULL::BOOLEAN FROM (SELECT 1 AS col) AS t"
      },
      "write": {}
    },
    {
      "sql": "SELECT CAST(NOT col IS NULL AS BOOLEAN) FROM (SELECT 1 AS col) AS t",
      "read": {
        "duckdb": "SELECT col IS NOT NULL::BOOLEAN FROM (SELECT 1 AS col) AS t",
        "redshift": "SELECT col IS NOT NULL::BOOLEAN FROM (SELECT 1 AS col) AS t",
        "postgres": "SELECT col IS NOT NULL::BOOLEAN FROM (SELECT 1 AS col) AS t"
      },
      "write": {}
    },
    {
      "sql": "JSON_KEYS(foo)",
      "read": {
        "spark": "JSON_OBJECT_KEYS(foo)",
        "databricks": "JSON_OBJECT_KEYS(foo)",
        "mysql": "JSON_KEYS(foo)",
        "starrocks": "JSON_KEYS(foo)",
        "duckdb": "JSON_KEYS(foo)",
        "snowflake": "OBJECT_KEYS(foo)",
        "doris": "JSON_KEYS(foo)",
        "singlestore": "JSON_KEYS(foo)"
      },
      "write": {
        "spark": "JSON_OBJECT_KEYS(foo)",
        "databricks": "JSON_OBJECT_KEYS(foo)",
        "mysql": "JSON_KEYS(foo)",
        "starrocks": "JSON_KEYS(foo)",
        "duckdb": "JSON_KEYS(foo)",
        "snowflake": "OBJECT_KEYS(foo)",
        "doris": "JSON_KEYS(foo)",
        "singlestore": "JSON_KEYS(foo)"
      }
    },
    {
      "sql": "JSON_KEYS(foo, '$.a')",
      "read": {
        "mysql": "JSON_KEYS(foo, '$.a')",
        "starrocks": "JSON_KEYS(foo, '$.a')",
        "duckdb": "JSON_KEYS(foo, '$.a')",
        "doris": "JSON_KEYS(foo, '$.a')"
      },
      "write": {
        "mysql": "JSON_KEYS(foo, '$.a')",
        "starrocks": "JSON_KEYS(foo, '$.a')",
        "duckdb": "JSON_KEYS(foo, '$.a')",
        "doris": "JSON_KEYS(foo, '$.a')"
      }
    },
    {
      "sql": "SELECT 1 AS \"x\"\"\"",
      "read": {},
      "write": {
        "clickhouse": "SELECT 1 AS \"x\"\"\""
      }
    },
    {
      "sql": "SELECT 1 AS \"[x]\"",
      "read": {},
      "write": {
        "tsql": "SELECT 1 AS [[x]]]"
      }
    },
    {
      "sql": "SELECT 1 AS \"x\"\"\"",
      "read": {},
      "write": {
        "tsql": "SELECT 1 AS [x\"]"
      }
    }
  ]
}