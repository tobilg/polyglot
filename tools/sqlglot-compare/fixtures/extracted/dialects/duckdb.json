{
  "dialect": "duckdb",
  "identity": [
    {
      "sql": "SELECT ([1,2,3])[:-:-1]",
      "expected": "SELECT ([1, 2, 3])[:-1:-1]"
    },
    {
      "sql": "SELECT INTERVAL '1 hour'::VARCHAR",
      "expected": "SELECT CAST(INTERVAL '1' HOUR AS TEXT)"
    },
    {
      "sql": "PIVOT duckdb_functions() ON schema_name USING AVG(LENGTH(function_name))::INTEGER GROUP BY schema_name",
      "expected": "PIVOT DUCKDB_FUNCTIONS() ON schema_name USING CAST(AVG(LENGTH(function_name)) AS INT) GROUP BY schema_name"
    },
    {
      "sql": "SELECT str[0:1]",
      "expected": null
    },
    {
      "sql": "SELECT COSH(1.5)",
      "expected": null
    },
    {
      "sql": "SELECT MODE(category)",
      "expected": null
    },
    {
      "sql": "SELECT e'\\n'",
      "expected": null
    },
    {
      "sql": "SELECT e'\\t'",
      "expected": null
    },
    {
      "sql": "SELECT e'update table_name set a = \\'foo\\' where 1 = 0' AS x FROM tab",
      "expected": "SELECT e'update table_name set a = ''foo'' where 1 = 0' AS x FROM tab"
    },
    {
      "sql": "SELECT GET_BIT(CAST('0110010' AS BIT), 2)",
      "expected": null
    },
    {
      "sql": "SELECT 1 WHERE x > $1",
      "expected": null
    },
    {
      "sql": "SELECT 1 WHERE x > $name",
      "expected": null
    },
    {
      "sql": "SELECT '{\"x\": 1}' -> c FROM t",
      "expected": null
    },
    {
      "sql": "SELECT EXP(1)",
      "expected": null
    },
    {
      "sql": "SELECT '{\"duck\": [1, 2, 3]}' -> '$.duck[#-1]'",
      "expected": null
    },
    {
      "sql": "SELECT tbl.x*1e4+tbl.y FROM tbl",
      "expected": "SELECT tbl.x * 1e4 + tbl.y FROM tbl"
    },
    {
      "sql": "DAYNAME(x)",
      "expected": null
    },
    {
      "sql": "MONTHNAME(x)",
      "expected": null
    },
    {
      "sql": "SELECT LIST_TRANSFORM([5, NULL, 6], (x, y) -> COALESCE(x, y, 0) + 1)",
      "expected": null
    },
    {
      "sql": "SELECT LIST_TRANSFORM([5, NULL, 6], LAMBDA x, y : COALESCE(x, y, 0) + 1)",
      "expected": null
    },
    {
      "sql": "SELECT LIST_TRANSFORM(LIST_FILTER([0, 1, 2, 3, 4, 5], LAMBDA x : x % 2 = 0), LAMBDA y : y * y)",
      "expected": null
    },
    {
      "sql": "ARG_MIN({'d': \"DATE\", 'ts': \"TIMESTAMP\", 'i': \"INT\", 'b': \"BIGINT\", 's': \"VARCHAR\"}, \"DOUBLE\")",
      "expected": null
    },
    {
      "sql": "ARG_MAX(keyword_name, keyword_category, 3 ORDER BY keyword_name DESC)",
      "expected": null
    },
    {
      "sql": "INSERT INTO t DEFAULT VALUES RETURNING (c1)",
      "expected": null
    },
    {
      "sql": "CREATE TABLE notes (watermark TEXT)",
      "expected": null
    },
    {
      "sql": "SELECT LIST_TRANSFORM([5, NULL, 6], LAMBDA x : COALESCE(x, 0) + 1)",
      "expected": null
    },
    {
      "sql": "SELECT LIST_TRANSFORM(nbr, LAMBDA x : x + 1) FROM article AS a",
      "expected": null
    },
    {
      "sql": "SELECT * FROM my_ducklake.demo AT (VERSION => 2)",
      "expected": null
    },
    {
      "sql": "SELECT TO_BINARY('test')",
      "expected": null
    },
    {
      "sql": "SELECT UUIDV7()",
      "expected": null
    },
    {
      "sql": "SELECT TRY(LOG(0))",
      "expected": null
    },
    {
      "sql": "x::timestamp",
      "expected": "CAST(x AS TIMESTAMP)"
    },
    {
      "sql": "x::timestamp without time zone",
      "expected": "CAST(x AS TIMESTAMP)"
    },
    {
      "sql": "x::timestamp with time zone",
      "expected": "CAST(x AS TIMESTAMPTZ)"
    },
    {
      "sql": "CAST(x AS FOO)",
      "expected": null
    },
    {
      "sql": "'red' IN tbl.flags",
      "expected": null
    },
    {
      "sql": "CREATE TABLE tbl1 (u UNION(num INT, str TEXT))",
      "expected": null
    },
    {
      "sql": "INSERT INTO x BY NAME SELECT 1 AS y",
      "expected": null
    },
    {
      "sql": "SELECT 1 AS x UNION ALL BY NAME SELECT 2 AS x",
      "expected": null
    },
    {
      "sql": "SELECT SUM(x) FILTER (x = 1)",
      "expected": "SELECT SUM(x) FILTER(WHERE x = 1)"
    },
    {
      "sql": "SELECT * FROM GLOB(x)",
      "expected": null
    },
    {
      "sql": "SELECT MAP(['key1', 'key2', 'key3'], [10, 20, 30])",
      "expected": null
    },
    {
      "sql": "SELECT MAP {'x': 1}",
      "expected": null
    },
    {
      "sql": "SELECT (MAP {'x': 1})['x']",
      "expected": null
    },
    {
      "sql": "SELECT df1.*, df2.* FROM df1 POSITIONAL JOIN df2",
      "expected": null
    },
    {
      "sql": "MAKE_TIMESTAMP(1992, 9, 20, 13, 34, 27.123456)",
      "expected": null
    },
    {
      "sql": "MAKE_TIMESTAMP(1667810584123456)",
      "expected": null
    },
    {
      "sql": "SELECT EPOCH_MS(10) AS t",
      "expected": null
    },
    {
      "sql": "SELECT MAKE_TIMESTAMP(10) AS t",
      "expected": null
    },
    {
      "sql": "SELECT TO_TIMESTAMP(10) AS t",
      "expected": null
    },
    {
      "sql": "SELECT UNNEST(col, recursive := TRUE) FROM t",
      "expected": null
    },
    {
      "sql": "VAR_POP(a)",
      "expected": null
    },
    {
      "sql": "SELECT * FROM foo ASOF LEFT JOIN bar ON a = b",
      "expected": null
    },
    {
      "sql": "SELECT {'a': 1} AS x",
      "expected": null
    },
    {
      "sql": "SELECT {'a': {'b': {'c': 1}}, 'd': {'e': 2}} AS x",
      "expected": null
    },
    {
      "sql": "SELECT {'x': 1, 'y': 2, 'z': 3}",
      "expected": null
    },
    {
      "sql": "SELECT {'key1': 'string', 'key2': 1, 'key3': 12.345}",
      "expected": null
    },
    {
      "sql": "SELECT ROW(x, x + 1, y) FROM (SELECT 1 AS x, 'a' AS y)",
      "expected": null
    },
    {
      "sql": "SELECT (x, x + 1, y) FROM (SELECT 1 AS x, 'a' AS y)",
      "expected": null
    },
    {
      "sql": "SELECT a.x FROM (SELECT {'x': 1, 'y': 2, 'z': 3} AS a)",
      "expected": null
    },
    {
      "sql": "FROM  x SELECT x UNION SELECT 1",
      "expected": "SELECT x FROM x UNION SELECT 1"
    },
    {
      "sql": "FROM (FROM tbl)",
      "expected": "SELECT * FROM (SELECT * FROM tbl)"
    },
    {
      "sql": "FROM tbl",
      "expected": "SELECT * FROM tbl"
    },
    {
      "sql": "x -> '$.family'",
      "expected": null
    },
    {
      "sql": "CREATE TABLE color (name ENUM('RED', 'GREEN', 'BLUE'))",
      "expected": null
    },
    {
      "sql": "SELECT * FROM foo WHERE bar > $baz AND bla = $bob",
      "expected": null
    },
    {
      "sql": "CREATE TABLE tbl_summary AS SELECT * FROM (SUMMARIZE tbl)",
      "expected": null
    },
    {
      "sql": "SELECT STAR(tbl, exclude := [foo])",
      "expected": null
    },
    {
      "sql": "MERGE INTO people USING (SELECT 1 AS id, 98000.0 AS salary) AS salary_updates USING (id) WHEN MATCHED THEN UPDATE SET salary = salary_updates.salary",
      "expected": null
    },
    {
      "sql": "MERGE INTO people USING (SELECT 1 AS id, 98000.0 AS salary) AS salary_updates USING (id) WHEN MATCHED THEN UPDATE",
      "expected": null
    },
    {
      "sql": "SELECT species, island, COUNT(*) FROM t GROUP BY GROUPING SETS (species), GROUPING SETS (island)",
      "expected": null
    },
    {
      "sql": "SELECT species, island, COUNT(*) FROM t GROUP BY CUBE (species), CUBE (island)",
      "expected": null
    },
    {
      "sql": "SELECT species, island, COUNT(*) FROM t GROUP BY ROLLUP (species), ROLLUP (island)",
      "expected": null
    },
    {
      "sql": "COPY (SELECT * FROM \"input.parquet\" USING SAMPLE RESERVOIR (5000 ROWS)) TO 'output.parquet' WITH (FORMAT PARQUET, KV_METADATA {'origin': 'Dagster', 'dagster_run_id': '98c85a11-d05c-4935-bfa2-198214c2204'})",
      "expected": null
    },
    {
      "sql": "SELECT '{ \"family\": \"anatidae\", \"species\": [ \"duck\", \"goose\", \"swan\", null ] }' ->> ['$.family', '$.species']",
      "expected": null
    },
    {
      "sql": "SELECT $ðŸ¦†$foo$ðŸ¦†$",
      "expected": "SELECT 'foo'"
    },
    {
      "sql": "SELECT * FROM t PIVOT(FIRST(t) AS t, FOR quarter IN ('Q1', 'Q2'))",
      "expected": "SELECT * FROM t PIVOT(FIRST(t) AS t FOR quarter IN ('Q1', 'Q2'))"
    },
    {
      "sql": "SELECT JSON_EXTRACT_STRING('{ \"family\": \"anatidae\", \"species\": [ \"duck\", \"goose\", \"swan\", null ] }', ['$.family', '$.species'])",
      "expected": "SELECT '{ \"family\": \"anatidae\", \"species\": [ \"duck\", \"goose\", \"swan\", null ] }' ->> ['$.family', '$.species']"
    },
    {
      "sql": "SELECT col FROM t WHERE JSON_EXTRACT_STRING(col, '$.id') NOT IN ('b')",
      "expected": "SELECT col FROM t WHERE NOT (col ->> '$.id') IN ('b')"
    },
    {
      "sql": "SELECT a, LOGICAL_OR(b) FROM foo GROUP BY a",
      "expected": "SELECT a, BOOL_OR(CAST(b AS BOOLEAN)) FROM foo GROUP BY a"
    },
    {
      "sql": "SELECT JSON_EXTRACT_STRING(c, '$.k1') = 'v1'",
      "expected": "SELECT (c ->> '$.k1') = 'v1'"
    },
    {
      "sql": "SELECT JSON_EXTRACT(c, '$.k1') = 'v1'",
      "expected": "SELECT (c -> '$.k1') = 'v1'"
    },
    {
      "sql": "SELECT JSON_EXTRACT(c, '$[*].id')[0:2]",
      "expected": "SELECT (c -> '$[*].id')[0:2]"
    },
    {
      "sql": "SELECT JSON_EXTRACT_STRING(c, '$[*].id')[0:2]",
      "expected": "SELECT (c ->> '$[*].id')[0:2]"
    },
    {
      "sql": "SELECT '{\"foo\": [1, 2, 3]}' -> 'foo' -> 0",
      "expected": "SELECT '{\"foo\": [1, 2, 3]}' -> '$.foo' -> '$[0]'"
    },
    {
      "sql": "SELECT ($$hello)'world$$)",
      "expected": "SELECT ('hello)''world')"
    },
    {
      "sql": "SELECT $$foo$$",
      "expected": "SELECT 'foo'"
    },
    {
      "sql": "SELECT $tag$foo$tag$",
      "expected": "SELECT 'foo'"
    },
    {
      "sql": "JSON_EXTRACT(x, '$.family')",
      "expected": "x -> '$.family'"
    },
    {
      "sql": "JSON_EXTRACT_PATH(x, '$.family')",
      "expected": "x -> '$.family'"
    },
    {
      "sql": "JSON_EXTRACT_STRING(x, '$.family')",
      "expected": "x ->> '$.family'"
    },
    {
      "sql": "JSON_EXTRACT_PATH_TEXT(x, '$.family')",
      "expected": "x ->> '$.family'"
    },
    {
      "sql": "SELECT {'yes': 'duck', 'maybe': 'goose', 'huh': NULL, 'no': 'heron'}",
      "expected": null
    },
    {
      "sql": "SELECT a['x space'] FROM (SELECT {'x space': 1, 'y': 2, 'z': 3} AS a)",
      "expected": null
    },
    {
      "sql": "PIVOT Cities ON Year IN (2000, 2010) USING SUM(Population) GROUP BY Country",
      "expected": null
    },
    {
      "sql": "PIVOT Cities ON Year USING SUM(Population) AS total, MAX(Population) AS max GROUP BY Country",
      "expected": null
    },
    {
      "sql": "WITH pivot_alias AS (PIVOT Cities ON Year USING SUM(Population) GROUP BY Country) SELECT * FROM pivot_alias",
      "expected": null
    },
    {
      "sql": "SELECT * FROM (PIVOT Cities ON Year USING SUM(Population) GROUP BY Country) AS pivot_alias",
      "expected": null
    },
    {
      "sql": "SELECT * FROM cities PIVOT(SUM(population) FOR year IN (2000, 2010, 2020) GROUP BY country)",
      "expected": null
    },
    {
      "sql": "SELECT schema_name, function_name, ROW_NUMBER() OVER my_window AS function_rank FROM DUCKDB_FUNCTIONS() WINDOW my_window AS (PARTITION BY schema_name ORDER BY function_name) QUALIFY ROW_NUMBER() OVER my_window < 3",
      "expected": null
    },
    {
      "sql": "SELECT SHA256('abc')",
      "expected": null
    },
    {
      "sql": "x ~ y",
      "expected": "REGEXP_FULL_MATCH(x, y)"
    },
    {
      "sql": "x !~ y",
      "expected": "NOT REGEXP_FULL_MATCH(x, y)"
    },
    {
      "sql": "REGEXP_FULL_MATCH(x, y, 'i')",
      "expected": null
    },
    {
      "sql": "REGEXP_REPLACE(this, pattern, replacement, modifiers)",
      "expected": null
    },
    {
      "sql": "SELECT PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY y DESC) FROM t",
      "expected": "SELECT QUANTILE_CONT(y, 0.25 ORDER BY y DESC) FROM t"
    },
    {
      "sql": "SELECT PERCENTILE_DISC(0.25) WITHIN GROUP (ORDER BY y DESC) FROM t",
      "expected": "SELECT QUANTILE_DISC(y, 0.25 ORDER BY y DESC) FROM t"
    },
    {
      "sql": "SELECT REGEXP_EXTRACT(a, 'pattern', 0)",
      "expected": "SELECT REGEXP_EXTRACT(a, 'pattern')"
    },
    {
      "sql": "SELECT REGEXP_EXTRACT(a, 'pattern', 0, 'i')",
      "expected": null
    },
    {
      "sql": "SELECT REGEXP_EXTRACT(a, 'pattern', 1, 'i')",
      "expected": null
    },
    {
      "sql": "SELECT ISNAN(x)",
      "expected": null
    },
    {
      "sql": "SELECT * FROM RANGE(1, 5, 10)",
      "expected": null
    },
    {
      "sql": "SELECT * FROM GENERATE_SERIES(2, 13, 4)",
      "expected": null
    },
    {
      "sql": "SELECT i FROM RANGE(5) AS _(i) ORDER BY i ASC",
      "expected": "SELECT i FROM RANGE(0, 5) AS _(i) ORDER BY i ASC"
    },
    {
      "sql": "SELECT i FROM GENERATE_SERIES(12) AS _(i) ORDER BY i ASC",
      "expected": "SELECT i FROM GENERATE_SERIES(0, 12) AS _(i) ORDER BY i ASC"
    },
    {
      "sql": "COPY lineitem FROM 'lineitem.ndjson' WITH (FORMAT JSON, DELIMITER ',', AUTO_DETECT TRUE, COMPRESSION SNAPPY, CODEC ZSTD, FORCE_NOT_NULL (col1, col2))",
      "expected": null
    },
    {
      "sql": "COPY (SELECT 42 AS a, 'hello' AS b) TO 'query.json' WITH (FORMAT JSON, ARRAY TRUE)",
      "expected": null
    },
    {
      "sql": "COPY lineitem (l_orderkey) TO 'orderkey.tbl' WITH (DELIMITER '|')",
      "expected": null
    },
    {
      "sql": "EDITDIST3(col1, col2)",
      "expected": "LEVENSHTEIN(col1, col2)"
    },
    {
      "sql": "SELECT LENGTH(foo)",
      "expected": null
    },
    {
      "sql": "SELECT ARRAY[1, 2, 3]",
      "expected": "SELECT [1, 2, 3]"
    },
    {
      "sql": "SELECT * FROM (DESCRIBE t)",
      "expected": null
    },
    {
      "sql": "SELECT UNNEST([*COLUMNS('alias_.*')]) AS column_name",
      "expected": null
    },
    {
      "sql": "SELECT COALESCE(*COLUMNS(*)) FROM (SELECT NULL, 2, 3) AS t(a, b, c)",
      "expected": null
    },
    {
      "sql": "SELECT id, STRUCT_PACK(*COLUMNS('m\\d')) AS measurements FROM many_measurements",
      "expected": "SELECT id, {'_0': *COLUMNS('m\\d')} AS measurements FROM many_measurements"
    },
    {
      "sql": "SELECT COLUMNS(c -> c LIKE '%num%') FROM numbers",
      "expected": null
    },
    {
      "sql": "SELECT MIN(COLUMNS(* REPLACE (number + id AS number))), COUNT(COLUMNS(* EXCLUDE (number))) FROM numbers",
      "expected": null
    },
    {
      "sql": "SELECT COLUMNS(*) + COLUMNS(*) FROM numbers",
      "expected": null
    },
    {
      "sql": "SELECT COLUMNS('(id|numbers?)') FROM numbers",
      "expected": null
    },
    {
      "sql": "SELECT COALESCE(COLUMNS(['a', 'b', 'c'])) AS result FROM (SELECT NULL AS a, 42 AS b, TRUE AS c)",
      "expected": null
    },
    {
      "sql": "SELECT COALESCE(*COLUMNS(['a', 'b', 'c'])) AS result FROM (SELECT NULL AS a, 42 AS b, TRUE AS c)",
      "expected": null
    },
    {
      "sql": "a ^ b",
      "expected": "POWER(a, b)"
    },
    {
      "sql": "a ** b",
      "expected": "POWER(a, b)"
    },
    {
      "sql": "a ~~~ b",
      "expected": "a GLOB b"
    },
    {
      "sql": "a ~~ b",
      "expected": "a LIKE b"
    },
    {
      "sql": "a @> b",
      "expected": null
    },
    {
      "sql": "a <@ b",
      "expected": "b @> a"
    },
    {
      "sql": "a ^@ b",
      "expected": "STARTS_WITH(a, b)"
    },
    {
      "sql": "a !~~ b",
      "expected": "NOT a LIKE b"
    },
    {
      "sql": "a !~~* b",
      "expected": "NOT a ILIKE b"
    },
    {
      "sql": "SELECT DATE_ADD(CAST('2020-01-01' AS DATE), INTERVAL 1 DAY)",
      "expected": "SELECT CAST('2020-01-01' AS DATE) + INTERVAL '1' DAY"
    },
    {
      "sql": "ARRAY_SLICE(x, 1, 3, 2)",
      "expected": null
    },
    {
      "sql": "SELECT #2, #1 FROM (VALUES (1, 'foo'))",
      "expected": null
    },
    {
      "sql": "SELECT #2 AS a, #1 AS b FROM (VALUES (1, 'foo'))",
      "expected": null
    },
    {
      "sql": "LISTAGG(x, ', ')",
      "expected": null
    },
    {
      "sql": "STRING_AGG(x, ', ')",
      "expected": "LISTAGG(x, ', ')"
    },
    {
      "sql": "SELECT CUME_DIST( ORDER BY foo) OVER (ORDER BY 1) FROM (SELECT 1 AS foo)",
      "expected": null
    },
    {
      "sql": "SELECT NTILE(1 ORDER BY foo) OVER (ORDER BY 1) FROM (SELECT 1 AS foo)",
      "expected": null
    },
    {
      "sql": "SELECT RANK( ORDER BY foo) OVER (ORDER BY 1) FROM (SELECT 1 AS foo)",
      "expected": null
    },
    {
      "sql": "SELECT PERCENT_RANK( ORDER BY foo) OVER (ORDER BY 1) FROM (SELECT 1 AS foo)",
      "expected": null
    },
    {
      "sql": "LIST_COSINE_DISTANCE(x, y)",
      "expected": null
    },
    {
      "sql": "LIST_DISTANCE(x, y)",
      "expected": null
    },
    {
      "sql": "SELECT * FROM t LIMIT 10 PERCENT",
      "expected": null
    },
    {
      "sql": "SELECT * FROM t LIMIT 10%",
      "expected": "SELECT * FROM t LIMIT 10 PERCENT"
    },
    {
      "sql": "SELECT * FROM t LIMIT 10 PERCENT OFFSET 1",
      "expected": null
    },
    {
      "sql": "SELECT * FROM t LIMIT 10% OFFSET 1",
      "expected": "SELECT * FROM t LIMIT 10 PERCENT OFFSET 1"
    },
    {
      "sql": "SELECT CAST(ROW(1, 2) AS ROW(a INTEGER, b INTEGER))",
      "expected": "SELECT CAST(ROW(1, 2) AS STRUCT(a INT, b INT))"
    },
    {
      "sql": "SELECT row",
      "expected": null
    },
    {
      "sql": "SELECT TRY_STRPTIME('2013-04-28T20:57:01.123456789+07:00', '%Y-%m-%dT%H:%M:%S.%n%z')",
      "expected": null
    },
    {
      "sql": "DELETE FROM t USING (VALUES (1)) AS t1(c), (VALUES (1), (2)) AS t2(c) WHERE t.c = t1.c AND t.c = t2.c",
      "expected": null
    },
    {
      "sql": "FROM (FROM t1 UNION FROM t2)",
      "expected": "SELECT * FROM (SELECT * FROM t1 UNION SELECT * FROM t2)"
    },
    {
      "sql": "FROM (FROM (SELECT 1) AS t2(c), (SELECT t2.c AS c0))",
      "expected": "SELECT * FROM (SELECT * FROM (SELECT 1) AS t2(c), (SELECT t2.c AS c0))"
    },
    {
      "sql": "FROM (FROM (SELECT 2000 as amount) t GROUP BY amount HAVING SUM(amount) > 1000)",
      "expected": "SELECT * FROM (SELECT * FROM (SELECT 2000 AS amount) AS t GROUP BY amount HAVING SUM(amount) > 1000)"
    },
    {
      "sql": "(FROM (SELECT 1) t1(c) EXCEPT FROM (SELECT 2) t2(c)) UNION ALL (FROM (SELECT 3) t3(c) EXCEPT FROM (SELECT 4) t4(c))",
      "expected": "(SELECT * FROM (SELECT 1) AS t1(c) EXCEPT SELECT * FROM (SELECT 2) AS t2(c)) UNION ALL (SELECT * FROM (SELECT 3) AS t3(c) EXCEPT SELECT * FROM (SELECT 4) AS t4(c))"
    },
    {
      "sql": "FORMAT('foo')",
      "expected": null
    },
    {
      "sql": "FORMAT('foo', 'foo2', 'foo3')",
      "expected": null
    },
    {
      "sql": "SELECT REPLACE('apple pie', 'pie', 'cobbler') AS result",
      "expected": null
    },
    {
      "sql": "SELECT REPLACE(CAST(CAST('apple pie' AS BLOB) AS TEXT), CAST(CAST('pie' AS BLOB) AS TEXT), CAST(CAST('cobbler' AS BLOB) AS TEXT)) AS result",
      "expected": null
    },
    {
      "sql": "SELECT TRIM('***apple***', '*') AS result",
      "expected": null
    },
    {
      "sql": "SELECT CAST(TRIM(CAST(CAST('***apple***' AS BLOB) AS TEXT), CAST(CAST('*' AS BLOB) AS TEXT)) AS BLOB) AS result",
      "expected": null
    },
    {
      "sql": "SELECT GREATEST(1.0, 2.5, NULL, 3.7)",
      "expected": null
    },
    {
      "sql": "FROM t1, t2 SELECT *",
      "expected": "SELECT * FROM t1, t2"
    },
    {
      "sql": "ROUND(2.256, 1)",
      "expected": null
    },
    {
      "sql": "SELECT MAKE_DATE(DATE_PART(['year', 'month', 'day'], TODAY()))",
      "expected": null
    },
    {
      "sql": "SELECT * FROM t PIVOT(SUM(y) FOR foo IN y_enum)",
      "expected": null
    },
    {
      "sql": "SELECT 20_000 AS literal",
      "expected": null
    },
    {
      "sql": "SELECT 1_2E+1_0::FLOAT",
      "expected": "SELECT CAST(1_2E+1_0 AS REAL)"
    },
    {
      "sql": "SELECT CURRENT_DATE",
      "expected": null
    },
    {
      "sql": "SELECT CURRENT_TIMESTAMP",
      "expected": null
    },
    {
      "sql": "SELECT * FROM tbl USING SAMPLE 5",
      "expected": "SELECT * FROM tbl USING SAMPLE RESERVOIR (5 ROWS)"
    },
    {
      "sql": "SELECT * FROM tbl USING SAMPLE 10%",
      "expected": "SELECT * FROM tbl USING SAMPLE SYSTEM (10 PERCENT)"
    },
    {
      "sql": "SELECT * FROM tbl USING SAMPLE 10 PERCENT (bernoulli)",
      "expected": "SELECT * FROM tbl USING SAMPLE BERNOULLI (10 PERCENT)"
    },
    {
      "sql": "SELECT * FROM tbl USING SAMPLE reservoir(50 ROWS) REPEATABLE (100)",
      "expected": "SELECT * FROM tbl USING SAMPLE RESERVOIR (50 ROWS) REPEATABLE (100)"
    },
    {
      "sql": "SELECT * FROM tbl USING SAMPLE 10% (system, 377)",
      "expected": "SELECT * FROM tbl USING SAMPLE SYSTEM (10 PERCENT) REPEATABLE (377)"
    },
    {
      "sql": "SELECT * FROM tbl TABLESAMPLE RESERVOIR(20%), tbl2 WHERE tbl.i=tbl2.i",
      "expected": "SELECT * FROM tbl TABLESAMPLE RESERVOIR (20 PERCENT), tbl2 WHERE tbl.i = tbl2.i"
    },
    {
      "sql": "SELECT * FROM tbl, tbl2 WHERE tbl.i=tbl2.i USING SAMPLE RESERVOIR(20%)",
      "expected": "SELECT * FROM tbl, tbl2 WHERE tbl.i = tbl2.i USING SAMPLE RESERVOIR (20 PERCENT)"
    },
    {
      "sql": "ARRAY(SELECT id FROM t)",
      "expected": null
    },
    {
      "sql": "ARRAY((SELECT id FROM t))",
      "expected": null
    },
    {
      "sql": "x::int[3]",
      "expected": "CAST(x AS INT[3])"
    },
    {
      "sql": "CAST(x AS REAL)",
      "expected": null
    },
    {
      "sql": "CAST(x AS UINTEGER)",
      "expected": null
    },
    {
      "sql": "CAST(x AS UBIGINT)",
      "expected": null
    },
    {
      "sql": "CAST(x AS USMALLINT)",
      "expected": null
    },
    {
      "sql": "CAST(x AS UTINYINT)",
      "expected": null
    },
    {
      "sql": "CAST(x AS TEXT)",
      "expected": null
    },
    {
      "sql": "CAST(x AS INT128)",
      "expected": null
    },
    {
      "sql": "CAST(x AS DOUBLE)",
      "expected": null
    },
    {
      "sql": "CAST(x AS DECIMAL(15, 4))",
      "expected": null
    },
    {
      "sql": "CAST(x AS STRUCT(number BIGINT))",
      "expected": null
    },
    {
      "sql": "CAST(x AS INT64)",
      "expected": "CAST(x AS BIGINT)"
    },
    {
      "sql": "CAST(x AS INT32)",
      "expected": "CAST(x AS INT)"
    },
    {
      "sql": "CAST(x AS INT16)",
      "expected": "CAST(x AS SMALLINT)"
    },
    {
      "sql": "CAST(x AS INT8)",
      "expected": "CAST(x AS BIGINT)"
    },
    {
      "sql": "CAST(x AS NUMERIC(1, 2))",
      "expected": "CAST(x AS DECIMAL(1, 2))"
    },
    {
      "sql": "CAST(x AS HUGEINT)",
      "expected": "CAST(x AS INT128)"
    },
    {
      "sql": "CAST(x AS UHUGEINT)",
      "expected": "CAST(x AS UINT128)"
    },
    {
      "sql": "CAST(x AS CHAR)",
      "expected": "CAST(x AS TEXT)"
    },
    {
      "sql": "CAST(x AS BPCHAR)",
      "expected": "CAST(x AS TEXT)"
    },
    {
      "sql": "CAST(x AS STRING)",
      "expected": "CAST(x AS TEXT)"
    },
    {
      "sql": "CAST(x AS VARCHAR)",
      "expected": "CAST(x AS TEXT)"
    },
    {
      "sql": "CAST(x AS INT1)",
      "expected": "CAST(x AS TINYINT)"
    },
    {
      "sql": "CAST(x AS FLOAT4)",
      "expected": "CAST(x AS REAL)"
    },
    {
      "sql": "CAST(x AS FLOAT)",
      "expected": "CAST(x AS REAL)"
    },
    {
      "sql": "CAST(x AS INT4)",
      "expected": "CAST(x AS INT)"
    },
    {
      "sql": "CAST(x AS INTEGER)",
      "expected": "CAST(x AS INT)"
    },
    {
      "sql": "CAST(x AS SIGNED)",
      "expected": "CAST(x AS INT)"
    },
    {
      "sql": "CAST(x AS BLOB)",
      "expected": "CAST(x AS BLOB)"
    },
    {
      "sql": "CAST(x AS BYTEA)",
      "expected": "CAST(x AS BLOB)"
    },
    {
      "sql": "CAST(x AS BINARY)",
      "expected": "CAST(x AS BLOB)"
    },
    {
      "sql": "CAST(x AS VARBINARY)",
      "expected": "CAST(x AS BLOB)"
    },
    {
      "sql": "CAST(x AS LOGICAL)",
      "expected": "CAST(x AS BOOLEAN)"
    },
    {
      "sql": "CAST({'i': 1, 's': 'foo'} AS STRUCT(\"s\" TEXT, \"i\" INT))",
      "expected": null
    },
    {
      "sql": "CAST(ROW(1, ROW(1)) AS STRUCT(number BIGINT, row STRUCT(number BIGINT)))",
      "expected": null
    },
    {
      "sql": "123::CHARACTER VARYING",
      "expected": "CAST(123 AS TEXT)"
    },
    {
      "sql": "CAST([[STRUCT_PACK(a := 1)]] AS STRUCT(a BIGINT)[][])",
      "expected": "CAST([[{'a': 1}]] AS STRUCT(a BIGINT)[][])"
    },
    {
      "sql": "CAST([STRUCT_PACK(a := 1)] AS STRUCT(a BIGINT)[])",
      "expected": "CAST([{'a': 1}] AS STRUCT(a BIGINT)[])"
    },
    {
      "sql": "STRUCT_PACK(a := 'b')::json",
      "expected": "CAST({'a': 'b'} AS JSON)"
    },
    {
      "sql": "STRUCT_PACK(a := 'b')::STRUCT(a TEXT)",
      "expected": "CAST({'a': 'b'} AS STRUCT(a TEXT))"
    },
    {
      "sql": "SELECT x::INT[3][3]",
      "expected": "SELECT CAST(x AS INT[3][3])"
    },
    {
      "sql": "SELECT ARRAY[[[1]]]::INT[1][1][1]",
      "expected": "SELECT CAST([[[1]]] AS INT[1][1][1])"
    },
    {
      "sql": "ATTACH 'file.db'",
      "expected": null
    },
    {
      "sql": "ATTACH ':memory:' AS db_alias",
      "expected": null
    },
    {
      "sql": "ATTACH IF NOT EXISTS 'file.db' AS db_alias",
      "expected": null
    },
    {
      "sql": "ATTACH 'file.db' AS db_alias (READ_ONLY)",
      "expected": null
    },
    {
      "sql": "ATTACH 'file.db' (READ_ONLY FALSE, TYPE sqlite)",
      "expected": null
    },
    {
      "sql": "ATTACH 'file.db' (TYPE POSTGRES, SCHEMA 'public')",
      "expected": null
    },
    {
      "sql": "ATTACH DATABASE 'file.db'",
      "expected": "ATTACH 'file.db'"
    },
    {
      "sql": "DETACH new_database",
      "expected": null
    },
    {
      "sql": "DETACH IF EXISTS file",
      "expected": "DETACH DATABASE IF EXISTS file"
    },
    {
      "sql": "DETACH DATABASE IF EXISTS file",
      "expected": "DETACH DATABASE IF EXISTS file"
    },
    {
      "sql": "DETACH DATABASE db",
      "expected": "DETACH db"
    },
    {
      "sql": "PIVOT Cities ON Year USING SUM(Population)",
      "expected": null
    },
    {
      "sql": "PIVOT Cities ON Year USING FIRST(Population)",
      "expected": null
    },
    {
      "sql": "PIVOT Cities ON Year USING SUM(Population) GROUP BY Country",
      "expected": null
    },
    {
      "sql": "PIVOT Cities ON Country, Name USING SUM(Population)",
      "expected": null
    },
    {
      "sql": "PIVOT Cities ON Country || '_' || Name USING SUM(Population)",
      "expected": null
    },
    {
      "sql": "PIVOT Cities ON Year USING SUM(Population) GROUP BY Country, Name",
      "expected": null
    },
    {
      "sql": "UNPIVOT (SELECT 1 AS col1, 2 AS col2) ON foo, bar",
      "expected": null
    },
    {
      "sql": "UNPIVOT monthly_sales ON jan, feb, mar, apr, may, jun INTO NAME month VALUE sales",
      "expected": null
    },
    {
      "sql": "UNPIVOT monthly_sales ON COLUMNS(* EXCLUDE (empid, dept)) INTO NAME month VALUE sales",
      "expected": null
    },
    {
      "sql": "UNPIVOT monthly_sales ON (jan, feb, mar) AS q1, (apr, may, jun) AS q2 INTO NAME quarter VALUE month_1_sales, month_2_sales, month_3_sales",
      "expected": null
    },
    {
      "sql": "WITH unpivot_alias AS (UNPIVOT monthly_sales ON COLUMNS(* EXCLUDE (empid, dept)) INTO NAME month VALUE sales) SELECT * FROM unpivot_alias",
      "expected": null
    },
    {
      "sql": "SELECT * FROM (UNPIVOT monthly_sales ON COLUMNS(* EXCLUDE (empid, dept)) INTO NAME month VALUE sales) AS unpivot_alias",
      "expected": null
    },
    {
      "sql": "WITH cities(country, name, year, population) AS (SELECT 'NL', 'Amsterdam', 2000, 1005 UNION ALL SELECT 'US', 'Seattle', 2020, 738) PIVOT cities ON year USING SUM(population)",
      "expected": null
    },
    {
      "sql": "CREATE TABLE t1 AS (FROM t2 SELECT foo1, foo2)",
      "expected": "CREATE TABLE t1 AS (SELECT foo1, foo2 FROM t2)"
    },
    {
      "sql": "FROM (FROM t1 SELECT foo1, foo2)",
      "expected": "SELECT * FROM (SELECT foo1, foo2 FROM t1)"
    },
    {
      "sql": "WITH t1 AS (FROM (FROM t2 SELECT foo1, foo2)) FROM t1",
      "expected": "WITH t1 AS (SELECT * FROM (SELECT foo1, foo2 FROM t2)) SELECT * FROM t1"
    },
    {
      "sql": "ANALYZE",
      "expected": null
    },
    {
      "sql": "SELECT foo: 1",
      "expected": "SELECT 1 AS foo"
    },
    {
      "sql": "SELECT foo: bar",
      "expected": "SELECT bar AS foo"
    },
    {
      "sql": "SELECT foo: t.col FROM t",
      "expected": "SELECT t.col AS foo FROM t"
    },
    {
      "sql": "SELECT \"foo\" /* bla */: 1",
      "expected": "SELECT 1 AS \"foo\" /* bla */"
    },
    {
      "sql": "SELECT \"foo\": 1 /* bla */",
      "expected": "SELECT 1 AS \"foo\" /* bla */"
    },
    {
      "sql": "SELECT \"foo\": /* bla */ 1",
      "expected": "SELECT 1 AS \"foo\" /* bla */"
    },
    {
      "sql": "SELECT \"foo\": /* bla */ 1 /* foo */",
      "expected": "SELECT 1 AS \"foo\" /* bla */ /* foo */"
    },
    {
      "sql": "SELECT \"foo\": 1",
      "expected": "SELECT 1 AS \"foo\""
    },
    {
      "sql": "SELECT foo: 1, bar: 2, baz: 3",
      "expected": "SELECT 1 AS foo, 2 AS bar, 3 AS baz"
    },
    {
      "sql": "SELECT e: 1 + 2, f: len('asdf'), s: (SELECT 42)",
      "expected": "SELECT 1 + 2 AS e, LENGTH('asdf') AS f, (SELECT 42) AS s"
    },
    {
      "sql": "SELECT * FROM foo: bar",
      "expected": "SELECT * FROM bar AS foo"
    },
    {
      "sql": "SELECT * FROM foo: c.db.tbl",
      "expected": "SELECT * FROM c.db.tbl AS foo"
    },
    {
      "sql": "SELECT * FROM foo /* bla */: bar",
      "expected": "SELECT * FROM bar AS foo /* bla */"
    },
    {
      "sql": "SELECT * FROM foo /* bla */: bar /* baz */",
      "expected": "SELECT * FROM bar AS foo /* bla */ /* baz */"
    },
    {
      "sql": "SELECT * FROM foo /* bla */: /* baz */ bar /* boo */",
      "expected": "SELECT * FROM bar AS foo /* bla */ /* baz */ /* boo */"
    },
    {
      "sql": "SELECT * FROM r: range(10), v: (VALUES (42)), s: (FROM range(10))",
      "expected": "SELECT * FROM RANGE(0, 10) AS r, (VALUES (42)) AS v, (SELECT * FROM RANGE(0, 10)) AS s"
    },
    {
      "sql": "\n            SELECT\n                l_returnflag,\n                l_linestatus,\n                sum_qty:        sum(l_quantity),\n                sum_base_price: sum(l_extendedprice),\n                sum_disc_price: sum(l_extendedprice * (1-l_discount)),\n                sum_charge:     sum(l_extendedprice * (1-l_discount) * (1+l_tax)),\n                avg_qty:        avg(l_quantity),\n                avg_price:      avg(l_extendedprice),\n                avg_disc:       avg(l_discount),\n                count_order:    count(*)\n            ",
      "expected": "SELECT l_returnflag, l_linestatus, SUM(l_quantity) AS sum_qty, SUM(l_extendedprice) AS sum_base_price, SUM(l_extendedprice * (1 - l_discount)) AS sum_disc_price, SUM(l_extendedprice * (1 - l_discount) * (1 + l_tax)) AS sum_charge, AVG(l_quantity) AS avg_qty, AVG(l_extendedprice) AS avg_price, AVG(l_discount) AS avg_disc, COUNT(*) AS count_order"
    },
    {
      "sql": "SELECT @col FROM t",
      "expected": "SELECT ABS(col) FROM t"
    },
    {
      "sql": "SELECT @col + 1 FROM t",
      "expected": "SELECT ABS(col + 1) FROM t"
    },
    {
      "sql": "SELECT (@col) + 1 FROM t",
      "expected": "SELECT (ABS(col)) + 1 FROM t"
    },
    {
      "sql": "SELECT @(-1)",
      "expected": "SELECT ABS((-1))"
    },
    {
      "sql": "SELECT @(-1) + 1",
      "expected": "SELECT ABS((-1) + 1)"
    },
    {
      "sql": "SELECT (@-1) + 1",
      "expected": "SELECT (ABS(-1)) + 1"
    },
    {
      "sql": "SET memory_limit = '10GB'",
      "expected": null
    },
    {
      "sql": "SET SESSION default_collation = 'nocase'",
      "expected": null
    },
    {
      "sql": "SET GLOBAL sort_order = 'desc'",
      "expected": null
    },
    {
      "sql": "SET VARIABLE my_var = 30",
      "expected": null
    },
    {
      "sql": "SET VARIABLE location_map = (SELECT foo FROM bar)",
      "expected": null
    },
    {
      "sql": "SET VARIABLE my_var TO 30",
      "expected": "SET VARIABLE my_var = 30"
    },
    {
      "sql": "RESET threads",
      "expected": null
    },
    {
      "sql": "RESET memory_limit",
      "expected": null
    },
    {
      "sql": "RESET default_collation",
      "expected": null
    },
    {
      "sql": "RESET SESSION threads",
      "expected": null
    },
    {
      "sql": "RESET GLOBAL memory_limit",
      "expected": null
    },
    {
      "sql": "RESET LOCAL threads",
      "expected": null
    },
    {
      "sql": "RESET SESSION default_collation",
      "expected": null
    },
    {
      "sql": "MAP {1: 'a', 2: 'b'}",
      "expected": null
    },
    {
      "sql": "MAP {'1': 'a', '2': 'b'}",
      "expected": null
    },
    {
      "sql": "MAP {[1, 2]: 'a', [3, 4]: 'b'}",
      "expected": null
    },
    {
      "sql": "CREATE SEQUENCE serial START 101",
      "expected": "CREATE SEQUENCE serial START WITH 101"
    },
    {
      "sql": "CREATE SEQUENCE serial START WITH 1 INCREMENT BY 2",
      "expected": null
    },
    {
      "sql": "CREATE SEQUENCE serial START WITH 99 INCREMENT BY -1 MAXVALUE 99",
      "expected": null
    },
    {
      "sql": "CREATE SEQUENCE serial START WITH 1 MAXVALUE 10 NO CYCLE",
      "expected": null
    },
    {
      "sql": "CREATE SEQUENCE serial START WITH 1 MAXVALUE 10 CYCLE",
      "expected": null
    },
    {
      "sql": "INSTALL httpfs",
      "expected": null
    },
    {
      "sql": "INSTALL httpfs FROM community",
      "expected": null
    },
    {
      "sql": "INSTALL httpfs FROM 'https://extensions.duckdb.org'",
      "expected": null
    },
    {
      "sql": "FORCE INSTALL httpfs FROM community",
      "expected": null
    },
    {
      "sql": "FORCE INSTALL httpfs FROM 'https://extensions.duckdb.org'",
      "expected": null
    },
    {
      "sql": "FORCE CHECKPOINT db",
      "expected": null
    },
    {
      "sql": "WITH RECURSIVE tbl(a, b) USING KEY (a) AS (SELECT a, b FROM (VALUES (1, 3), (2, 4)) AS t(a, b) UNION SELECT a + 1, b FROM tbl WHERE a < 3) SELECT * FROM tbl",
      "expected": null
    },
    {
      "sql": "WITH RECURSIVE tbl(a, b) USING KEY (a, b) AS (SELECT a, b FROM (VALUES (1, 3), (2, 4)) AS t(a, b) UNION SELECT a + 1, b FROM tbl WHERE a < 3) SELECT * FROM tbl",
      "expected": null
    },
    {
      "sql": "WITH _data AS (SELECT [{'a': 1, 'b': 2}, {'a': 2, 'b': 3}] AS col) SELECT t.col['b'] FROM _data, UNNEST(_data.col) AS t(col) WHERE t.col['a'] = 1",
      "expected": "WITH _data AS (SELECT [{'a': 1, 'b': 2}, {'a': 2, 'b': 3}] AS col) SELECT t.col['b'] FROM _data JOIN UNNEST(_data.col) AS t(col) ON TRUE WHERE t.col['a'] = 1"
    },
    {
      "sql": "[x.STRING_SPLIT(' ')[i] FOR x IN ['1', '2', 3] IF x.CONTAINS('1')]",
      "expected": null
    },
    {
      "sql": "SELECT [4, 5, 6] AS l, [x FOR x, i IN l IF i = 2] AS filtered",
      "expected": null
    },
    {
      "sql": "SELECT LIST_VALUE(1)[i]",
      "expected": "SELECT [1][i]"
    },
    {
      "sql": "{'x': LIST_VALUE(1)[i]}",
      "expected": "{'x': [1][i]}"
    },
    {
      "sql": "SELECT LIST_APPLY(RANGE(1, 4), i -> {'f1': LIST_VALUE(1, 2, 3)[i], 'f2': LIST_VALUE(1, 2, 3)[i]})",
      "expected": "SELECT LIST_APPLY(RANGE(1, 4), i -> {'f1': [1, 2, 3][i], 'f2': [1, 2, 3][i]})"
    },
    {
      "sql": "SUMMARIZE tbl",
      "expected": null
    },
    {
      "sql": "SUMMARIZE SELECT * FROM tbl",
      "expected": null
    },
    {
      "sql": "SUMMARIZE TABLE 'https://blobs.duckdb.org/data/Star_Trek-Season_1.csv'",
      "expected": null
    },
    {
      "sql": "DATE_SUB('YEAR', col, '2020-01-01')",
      "expected": null
    },
    {
      "sql": "DATESUB('YEAR', col, '2020-01-01')",
      "expected": null
    },
    {
      "sql": "a && b",
      "expected": null
    },
    {
      "sql": "SHOW TABLES",
      "expected": null
    },
    {
      "sql": "SHOW ALL TABLES",
      "expected": null
    },
    {
      "sql": "SELECT UNNEST([1, 2])",
      "expected": null
    },
    {
      "sql": "'red' IN flags",
      "expected": null
    },
    {
      "sql": "FORCE INSTALL httpfs",
      "expected": null
    },
    {
      "sql": "UNION_VALUE(k1 := 1)",
      "expected": null
    }
  ],
  "transpilation": [
    {
      "sql": "(c LIKE 'a' OR c LIKE 'b') AND other_cond",
      "read": {
        "databricks": "c LIKE ANY ('a', 'b') AND other_cond"
      },
      "write": {}
    },
    {
      "sql": "SELECT FIRST_VALUE(c IGNORE NULLS) OVER (PARTITION BY gb ORDER BY ob) FROM t",
      "read": {},
      "write": {
        "duckdb": "SELECT FIRST_VALUE(c IGNORE NULLS) OVER (PARTITION BY gb ORDER BY ob) FROM t"
      }
    },
    {
      "sql": "SELECT FIRST_VALUE(c RESPECT NULLS) OVER (PARTITION BY gb ORDER BY ob) FROM t",
      "read": {},
      "write": {
        "duckdb": "SELECT FIRST_VALUE(c RESPECT NULLS) OVER (PARTITION BY gb ORDER BY ob) FROM t",
        "sqlite": "SELECT FIRST_VALUE(c) OVER (PARTITION BY gb ORDER BY ob NULLS LAST) FROM t",
        "mysql": "SELECT FIRST_VALUE(c) RESPECT NULLS OVER (PARTITION BY gb ORDER BY CASE WHEN ob IS NULL THEN 1 ELSE 0 END, ob) FROM t"
      }
    },
    {
      "sql": "CAST(x AS UUID)",
      "read": {},
      "write": {
        "bigquery": "CAST(x AS STRING)",
        "duckdb": "CAST(x AS UUID)"
      }
    },
    {
      "sql": "SELECT APPROX_TOP_K(category, 3) FROM t",
      "read": {},
      "write": {
        "snowflake": "SELECT APPROX_TOP_K(category, 3) FROM t"
      }
    },
    {
      "sql": "SELECT CASE WHEN JSON_VALID('{\"x: 1}') THEN '{\"x: 1}' ELSE NULL END",
      "read": {
        "duckdb": "SELECT CASE WHEN JSON_VALID('{\"x: 1}') THEN '{\"x: 1}' ELSE NULL END",
        "snowflake": "SELECT TRY_PARSE_JSON('{\"x: 1}')"
      },
      "write": {}
    },
    {
      "sql": "SELECT straight_join",
      "read": {},
      "write": {
        "duckdb": "SELECT straight_join",
        "mysql": "SELECT `straight_join`"
      }
    },
    {
      "sql": "STRUCT_PACK(\"a b\" := 1)",
      "read": {},
      "write": {
        "duckdb": "{'a b': 1}",
        "spark": "STRUCT(1 AS `a b`)",
        "snowflake": "OBJECT_CONSTRUCT('a b', 1)"
      }
    },
    {
      "sql": "ARRAY_TO_STRING(arr, delim)",
      "read": {
        "bigquery": "ARRAY_TO_STRING(arr, delim)",
        "postgres": "ARRAY_TO_STRING(arr, delim)",
        "presto": "ARRAY_JOIN(arr, delim)",
        "snowflake": "ARRAY_TO_STRING(arr, delim)",
        "spark": "ARRAY_JOIN(arr, delim)"
      },
      "write": {
        "bigquery": "ARRAY_TO_STRING(arr, delim)",
        "duckdb": "ARRAY_TO_STRING(arr, delim)",
        "postgres": "ARRAY_TO_STRING(arr, delim)",
        "presto": "ARRAY_JOIN(arr, delim)",
        "snowflake": "ARRAY_TO_STRING(arr, delim)",
        "spark": "ARRAY_JOIN(arr, delim)",
        "tsql": "STRING_AGG(arr, delim)"
      }
    },
    {
      "sql": "SELECT SUM(X) OVER (ORDER BY x)",
      "read": {},
      "write": {
        "bigquery": "SELECT SUM(X) OVER (ORDER BY x)",
        "duckdb": "SELECT SUM(X) OVER (ORDER BY x)",
        "mysql": "SELECT SUM(X) OVER (ORDER BY CASE WHEN x IS NULL THEN 1 ELSE 0 END, x)"
      }
    },
    {
      "sql": "SELECT SUM(X) OVER (ORDER BY x RANGE BETWEEN 1 PRECEDING AND CURRENT ROW)",
      "read": {},
      "write": {
        "bigquery": "SELECT SUM(X) OVER (ORDER BY x RANGE BETWEEN 1 PRECEDING AND CURRENT ROW)",
        "duckdb": "SELECT SUM(X) OVER (ORDER BY x RANGE BETWEEN 1 PRECEDING AND CURRENT ROW)",
        "mysql": "SELECT SUM(X) OVER (ORDER BY x RANGE BETWEEN 1 PRECEDING AND CURRENT ROW)"
      }
    },
    {
      "sql": "SELECT * FROM x ORDER BY 1 NULLS LAST",
      "read": {},
      "write": {
        "duckdb": "SELECT * FROM x ORDER BY 1",
        "mysql": "SELECT * FROM x ORDER BY 1"
      }
    },
    {
      "sql": "CREATE TEMPORARY FUNCTION f1(a, b) AS (a + b)",
      "read": {
        "bigquery": "CREATE TEMP FUNCTION f1(a INT64, b INT64) AS (a + b)"
      },
      "write": {}
    },
    {
      "sql": "{'a': 1, 'b': '2'}",
      "read": {},
      "write": {
        "presto": "CAST(ROW(1, '2') AS ROW(a INTEGER, b VARCHAR))"
      }
    },
    {
      "sql": "struct_pack(a := 1, b := 2)",
      "read": {},
      "write": {
        "presto": "CAST(ROW(1, 2) AS ROW(a INTEGER, b INTEGER))"
      }
    },
    {
      "sql": "struct_pack(a := 1, b := x)",
      "read": {},
      "write": {
        "duckdb": "{'a': 1, 'b': x}"
      }
    },
    {
      "sql": "SELECT RANGE(1, 5)",
      "read": {},
      "write": {
        "duckdb": "SELECT RANGE(1, 5)",
        "spark": "SELECT SEQUENCE(1, 4)"
      }
    },
    {
      "sql": "SELECT RANGE(1, 5, 2)",
      "read": {},
      "write": {
        "duckdb": "SELECT RANGE(1, 5, 2)",
        "spark": "SELECT SEQUENCE(1, 3, 2)"
      }
    },
    {
      "sql": "SELECT RANGE(1, 1)",
      "read": {},
      "write": {
        "duckdb": "SELECT RANGE(1, 1)",
        "spark": "SELECT ARRAY()"
      }
    },
    {
      "sql": "SELECT RANGE(5, 1, -1)",
      "read": {},
      "write": {
        "duckdb": "SELECT RANGE(5, 1, -1)",
        "spark": "SELECT SEQUENCE(5, 2, -1)"
      }
    },
    {
      "sql": "SELECT RANGE(5, 1, 0)",
      "read": {},
      "write": {
        "duckdb": "SELECT RANGE(5, 1, 0)",
        "spark": "SELECT ARRAY()"
      }
    },
    {
      "sql": "WITH t AS (SELECT 5 AS c) SELECT RANGE(1, c) FROM t",
      "read": {},
      "write": {
        "duckdb": "WITH t AS (SELECT 5 AS c) SELECT RANGE(1, c) FROM t",
        "spark": "WITH t AS (SELECT 5 AS c) SELECT IF((c - 1) <= 1, ARRAY(), SEQUENCE(1, (c - 1))) FROM t"
      }
    },
    {
      "sql": "SELECT JSON_EXTRACT('{\"duck\": [1, 2, 3]}', '/duck/0')",
      "read": {},
      "write": {
        "duckdb": "SELECT '{\"duck\": [1, 2, 3]}' -> '/duck/0'"
      }
    },
    {
      "sql": "SELECT JSON('{\"fruit\":\"banana\"}') -> 'fruit'",
      "read": {},
      "write": {
        "duckdb": "SELECT JSON('{\"fruit\":\"banana\"}') -> '$.fruit'",
        "snowflake": "SELECT GET_PATH(PARSE_JSON('{\"fruit\":\"banana\"}'), 'fruit')"
      }
    },
    {
      "sql": "SELECT JSON('{\"fruit\": {\"foo\": \"banana\"}}') -> 'fruit' -> 'foo'",
      "read": {},
      "write": {
        "duckdb": "SELECT JSON('{\"fruit\": {\"foo\": \"banana\"}}') -> '$.fruit' -> '$.foo'",
        "snowflake": "SELECT GET_PATH(GET_PATH(PARSE_JSON('{\"fruit\": {\"foo\": \"banana\"}}'), 'fruit'), 'foo')"
      }
    },
    {
      "sql": "SELECT {'bla': column1, 'foo': column2, 'bar': column3} AS data FROM source_table",
      "read": {
        "bigquery": "SELECT STRUCT(column1 AS bla, column2 AS foo, column3 AS bar) AS data FROM source_table",
        "duckdb": "SELECT {'bla': column1, 'foo': column2, 'bar': column3} AS data FROM source_table"
      },
      "write": {
        "bigquery": "SELECT STRUCT(column1 AS bla, column2 AS foo, column3 AS bar) AS data FROM source_table"
      }
    },
    {
      "sql": "WITH cte(x) AS (SELECT 1 UNION ALL SELECT 2 UNION ALL SELECT 3) SELECT AVG(x) FILTER (WHERE x > 1) FROM cte",
      "read": {},
      "write": {
        "duckdb": "WITH cte(x) AS (SELECT 1 UNION ALL SELECT 2 UNION ALL SELECT 3) SELECT AVG(x) FILTER(WHERE x > 1) FROM cte",
        "snowflake": "WITH cte(x) AS (SELECT 1 UNION ALL SELECT 2 UNION ALL SELECT 3) SELECT AVG(IFF(x > 1, x, NULL)) FROM cte"
      }
    },
    {
      "sql": "SELECT AVG(x) FILTER (WHERE TRUE) FROM t",
      "read": {},
      "write": {
        "duckdb": "SELECT AVG(x) FILTER(WHERE TRUE) FROM t",
        "snowflake": "SELECT AVG(IFF(TRUE, x, NULL)) FROM t"
      }
    },
    {
      "sql": "SELECT UNNEST(ARRAY[1, 2, 3]), UNNEST(ARRAY[4, 5]), UNNEST(ARRAY[6])",
      "read": {},
      "write": {
        "bigquery": "SELECT IF(pos = pos_2, col, NULL) AS col, IF(pos = pos_3, col_2, NULL) AS col_2, IF(pos = pos_4, col_3, NULL) AS col_3 FROM UNNEST(GENERATE_ARRAY(0, GREATEST(ARRAY_LENGTH([1, 2, 3]), ARRAY_LENGTH([4, 5]), ARRAY_LENGTH([6])) - 1)) AS pos CROSS JOIN UNNEST([1, 2, 3]) AS col WITH OFFSET AS pos_2 CROSS JOIN UNNEST([4, 5]) AS col_2 WITH OFFSET AS pos_3 CROSS JOIN UNNEST([6]) AS col_3 WITH OFFSET AS pos_4 WHERE ((pos = pos_2 OR (pos > (ARRAY_LENGTH([1, 2, 3]) - 1) AND pos_2 = (ARRAY_LENGTH([1, 2, 3]) - 1))) AND (pos = pos_3 OR (pos > (ARRAY_LENGTH([4, 5]) - 1) AND pos_3 = (ARRAY_LENGTH([4, 5]) - 1)))) AND (pos = pos_4 OR (pos > (ARRAY_LENGTH([6]) - 1) AND pos_4 = (ARRAY_LENGTH([6]) - 1)))",
        "presto": "SELECT IF(_u.pos = _u_2.pos_2, _u_2.col) AS col, IF(_u.pos = _u_3.pos_3, _u_3.col_2) AS col_2, IF(_u.pos = _u_4.pos_4, _u_4.col_3) AS col_3 FROM UNNEST(SEQUENCE(1, GREATEST(CARDINALITY(ARRAY[1, 2, 3]), CARDINALITY(ARRAY[4, 5]), CARDINALITY(ARRAY[6])))) AS _u(pos) CROSS JOIN UNNEST(ARRAY[1, 2, 3]) WITH ORDINALITY AS _u_2(col, pos_2) CROSS JOIN UNNEST(ARRAY[4, 5]) WITH ORDINALITY AS _u_3(col_2, pos_3) CROSS JOIN UNNEST(ARRAY[6]) WITH ORDINALITY AS _u_4(col_3, pos_4) WHERE ((_u.pos = _u_2.pos_2 OR (_u.pos > CARDINALITY(ARRAY[1, 2, 3]) AND _u_2.pos_2 = CARDINALITY(ARRAY[1, 2, 3]))) AND (_u.pos = _u_3.pos_3 OR (_u.pos > CARDINALITY(ARRAY[4, 5]) AND _u_3.pos_3 = CARDINALITY(ARRAY[4, 5])))) AND (_u.pos = _u_4.pos_4 OR (_u.pos > CARDINALITY(ARRAY[6]) AND _u_4.pos_4 = CARDINALITY(ARRAY[6])))"
      }
    },
    {
      "sql": "SELECT UNNEST(ARRAY[1, 2, 3]), UNNEST(ARRAY[4, 5]), UNNEST(ARRAY[6]) FROM x",
      "read": {},
      "write": {
        "bigquery": "SELECT IF(pos = pos_2, col, NULL) AS col, IF(pos = pos_3, col_2, NULL) AS col_2, IF(pos = pos_4, col_3, NULL) AS col_3 FROM x CROSS JOIN UNNEST(GENERATE_ARRAY(0, GREATEST(ARRAY_LENGTH([1, 2, 3]), ARRAY_LENGTH([4, 5]), ARRAY_LENGTH([6])) - 1)) AS pos CROSS JOIN UNNEST([1, 2, 3]) AS col WITH OFFSET AS pos_2 CROSS JOIN UNNEST([4, 5]) AS col_2 WITH OFFSET AS pos_3 CROSS JOIN UNNEST([6]) AS col_3 WITH OFFSET AS pos_4 WHERE ((pos = pos_2 OR (pos > (ARRAY_LENGTH([1, 2, 3]) - 1) AND pos_2 = (ARRAY_LENGTH([1, 2, 3]) - 1))) AND (pos = pos_3 OR (pos > (ARRAY_LENGTH([4, 5]) - 1) AND pos_3 = (ARRAY_LENGTH([4, 5]) - 1)))) AND (pos = pos_4 OR (pos > (ARRAY_LENGTH([6]) - 1) AND pos_4 = (ARRAY_LENGTH([6]) - 1)))",
        "presto": "SELECT IF(_u.pos = _u_2.pos_2, _u_2.col) AS col, IF(_u.pos = _u_3.pos_3, _u_3.col_2) AS col_2, IF(_u.pos = _u_4.pos_4, _u_4.col_3) AS col_3 FROM x CROSS JOIN UNNEST(SEQUENCE(1, GREATEST(CARDINALITY(ARRAY[1, 2, 3]), CARDINALITY(ARRAY[4, 5]), CARDINALITY(ARRAY[6])))) AS _u(pos) CROSS JOIN UNNEST(ARRAY[1, 2, 3]) WITH ORDINALITY AS _u_2(col, pos_2) CROSS JOIN UNNEST(ARRAY[4, 5]) WITH ORDINALITY AS _u_3(col_2, pos_3) CROSS JOIN UNNEST(ARRAY[6]) WITH ORDINALITY AS _u_4(col_3, pos_4) WHERE ((_u.pos = _u_2.pos_2 OR (_u.pos > CARDINALITY(ARRAY[1, 2, 3]) AND _u_2.pos_2 = CARDINALITY(ARRAY[1, 2, 3]))) AND (_u.pos = _u_3.pos_3 OR (_u.pos > CARDINALITY(ARRAY[4, 5]) AND _u_3.pos_3 = CARDINALITY(ARRAY[4, 5])))) AND (_u.pos = _u_4.pos_4 OR (_u.pos > CARDINALITY(ARRAY[6]) AND _u_4.pos_4 = CARDINALITY(ARRAY[6])))"
      }
    },
    {
      "sql": "SELECT UNNEST(x) + 1",
      "read": {},
      "write": {
        "bigquery": "SELECT IF(pos = pos_2, col, NULL) + 1 AS col FROM UNNEST(GENERATE_ARRAY(0, GREATEST(ARRAY_LENGTH(x)) - 1)) AS pos CROSS JOIN UNNEST(x) AS col WITH OFFSET AS pos_2 WHERE pos = pos_2 OR (pos > (ARRAY_LENGTH(x) - 1) AND pos_2 = (ARRAY_LENGTH(x) - 1))"
      }
    },
    {
      "sql": "SELECT UNNEST(x) + 1 AS y",
      "read": {},
      "write": {
        "bigquery": "SELECT IF(pos = pos_2, y, NULL) + 1 AS y FROM UNNEST(GENERATE_ARRAY(0, GREATEST(ARRAY_LENGTH(x)) - 1)) AS pos CROSS JOIN UNNEST(x) AS y WITH OFFSET AS pos_2 WHERE pos = pos_2 OR (pos > (ARRAY_LENGTH(x) - 1) AND pos_2 = (ARRAY_LENGTH(x) - 1))"
      }
    },
    {
      "sql": "SELECT DATE_DIFF('DAY', CAST('2020-01-01' AS DATE), CAST('2025-10-12' AS DATE))",
      "read": {
        "snowflake": "SELECT DATEDIFF('day', '2020-01-01', '2025-10-12')"
      },
      "write": {}
    },
    {
      "sql": "SELECT DATE_DIFF('SECOND', CAST('2020-01-01' AS DATE), CAST('2025-10-12 00:56:42.345' AS TIMESTAMP))",
      "read": {
        "duckdb": "SELECT DATE_DIFF('SECOND', CAST('2020-01-01' AS DATE), CAST('2025-10-12 00:56:42.345' AS TIMESTAMP))",
        "snowflake": "SELECT DATEDIFF('second', '2020-01-01', '2025-10-12 00:56:42.345')"
      },
      "write": {}
    },
    {
      "sql": "SELECT DATE_DIFF('SECOND', CAST('2020-01-01' AS DATE), CAST('2025-10-12 00:56:42.345+07:00' AS TIMESTAMPTZ))",
      "read": {
        "duckdb": "SELECT DATE_DIFF('SECOND', CAST('2020-01-01' AS DATE), CAST('2025-10-12 00:56:42.345+07:00' AS TIMESTAMPTZ))",
        "snowflake": "SELECT DATEDIFF('second', '2020-01-01', '2025-10-12 00:56:42.345+07:00')"
      },
      "write": {}
    },
    {
      "sql": "SELECT NOT (data -> '$.value')",
      "read": {
        "snowflake": "SELECT NOT data:value"
      },
      "write": {}
    },
    {
      "sql": "SELECT NOT (data -> '$.value.nested')",
      "read": {
        "snowflake": "SELECT NOT data:value:nested"
      },
      "write": {}
    },
    {
      "sql": "SELECT (data -> '$.value') = 1",
      "read": {
        "snowflake": "SELECT data:value = 1"
      },
      "write": {}
    },
    {
      "sql": "SELECT * FROM 'x.y'",
      "read": {},
      "write": {
        "duckdb": "SELECT * FROM \"x.y\""
      }
    },
    {
      "sql": "SELECT LIST(DISTINCT sample_col) FROM sample_table",
      "read": {
        "duckdb": "SELECT LIST(DISTINCT sample_col) FROM sample_table",
        "spark": "SELECT COLLECT_SET(sample_col) FROM sample_table"
      },
      "write": {}
    },
    {
      "sql": "SELECT LIST_TRANSFORM(STR_SPLIT_REGEX('abc , dfg ', ','), x -> TRIM(x))",
      "read": {},
      "write": {
        "duckdb": "SELECT LIST_TRANSFORM(STR_SPLIT_REGEX('abc , dfg ', ','), x -> TRIM(x))",
        "spark": "SELECT TRANSFORM(SPLIT('abc , dfg ', ','), x -> TRIM(x))"
      }
    },
    {
      "sql": "SELECT LIST_FILTER([4, 5, 6], x -> x > 4)",
      "read": {},
      "write": {
        "duckdb": "SELECT LIST_FILTER([4, 5, 6], x -> x > 4)",
        "spark": "SELECT FILTER(ARRAY(4, 5, 6), x -> x > 4)"
      }
    },
    {
      "sql": "SELECT ANY_VALUE(sample_column) FROM sample_table",
      "read": {},
      "write": {
        "duckdb": "SELECT ANY_VALUE(sample_column) FROM sample_table",
        "spark": "SELECT ANY_VALUE(sample_column) IGNORE NULLS FROM sample_table"
      }
    },
    {
      "sql": "COUNT_IF(x)",
      "read": {},
      "write": {
        "duckdb": "COUNT_IF(x)",
        "duckdb, version=1.0": "SUM(CASE WHEN x THEN 1 ELSE 0 END)",
        "duckdb, version=1.2": "COUNT_IF(x)"
      }
    },
    {
      "sql": "SELECT STRFTIME(CAST('2020-01-01' AS TIMESTAMP), CONCAT('%Y', '%m'))",
      "read": {},
      "write": {
        "duckdb": "SELECT STRFTIME(CAST('2020-01-01' AS TIMESTAMP), CONCAT('%Y', '%m'))",
        "spark": "SELECT DATE_FORMAT(CAST('2020-01-01' AS TIMESTAMP_NTZ), CONCAT('yyyy', 'MM'))",
        "tsql": "SELECT FORMAT(CAST('2020-01-01' AS DATETIME2), CONCAT('yyyy', 'MM'))"
      }
    },
    {
      "sql": "SELECT CAST('{\"x\": 1}' AS JSON)",
      "read": {
        "duckdb": "SELECT '{\"x\": 1}'::JSON",
        "postgres": "SELECT '{\"x\": 1}'::JSONB"
      },
      "write": {}
    },
    {
      "sql": "SELECT * FROM produce PIVOT(SUM(sales) FOR quarter IN ('Q1', 'Q2'))",
      "read": {
        "duckdb": "SELECT * FROM produce PIVOT(SUM(sales) FOR quarter IN ('Q1', 'Q2'))",
        "snowflake": "SELECT * FROM produce PIVOT(SUM(produce.sales) FOR produce.quarter IN ('Q1', 'Q2'))"
      },
      "write": {}
    },
    {
      "sql": "SELECT UNNEST([1, 2, 3])",
      "read": {},
      "write": {
        "duckdb": "SELECT UNNEST([1, 2, 3])",
        "snowflake": "SELECT IFF(_u.pos = _u_2.pos_2, _u_2.col, NULL) AS col FROM TABLE(FLATTEN(INPUT => ARRAY_GENERATE_RANGE(0, (GREATEST(ARRAY_SIZE([1, 2, 3])) - 1) + 1))) AS _u(seq, key, path, index, pos, this) CROSS JOIN TABLE(FLATTEN(INPUT => [1, 2, 3])) AS _u_2(seq, key, path, pos_2, col, this) WHERE _u.pos = _u_2.pos_2 OR (_u.pos > (ARRAY_SIZE([1, 2, 3]) - 1) AND _u_2.pos_2 = (ARRAY_SIZE([1, 2, 3]) - 1))"
      }
    },
    {
      "sql": "VAR_POP(x)",
      "read": {},
      "write": {
        "duckdb": "VAR_POP(x)"
      }
    },
    {
      "sql": "DATE_DIFF('DAY', CAST(b AS DATE), CAST(a AS DATE))",
      "read": {
        "duckdb": "DATE_DIFF('day', CAST(b AS DATE), CAST(a AS DATE))",
        "hive": "DATEDIFF(a, b)",
        "spark": "DATEDIFF(a, b)",
        "spark2": "DATEDIFF(a, b)"
      },
      "write": {}
    },
    {
      "sql": "XOR(a, b)",
      "read": {
        "bigquery": "a ^ b",
        "presto": "BITWISE_XOR(a, b)",
        "postgres": "a # b"
      },
      "write": {
        "bigquery": "a ^ b",
        "duckdb": "XOR(a, b)",
        "presto": "BITWISE_XOR(a, b)",
        "postgres": "a # b"
      }
    },
    {
      "sql": "PIVOT_WIDER Cities ON Year USING SUM(Population)",
      "read": {},
      "write": {
        "duckdb": "PIVOT Cities ON Year USING SUM(Population)"
      }
    },
    {
      "sql": "WITH t AS (SELECT 1) FROM t",
      "read": {},
      "write": {
        "duckdb": "WITH t AS (SELECT 1) SELECT * FROM t"
      }
    },
    {
      "sql": "WITH t AS (SELECT 1) SELECT * FROM (FROM t)",
      "read": {},
      "write": {
        "duckdb": "WITH t AS (SELECT 1) SELECT * FROM (SELECT * FROM t)"
      }
    },
    {
      "sql": "SELECT DATEDIFF('day', t1.\"A\", t1.\"B\") FROM \"table\" AS t1",
      "read": {},
      "write": {
        "duckdb": "SELECT DATE_DIFF('DAY', t1.\"A\", t1.\"B\") FROM \"table\" AS t1",
        "trino": "SELECT DATE_DIFF('DAY', t1.\"A\", t1.\"B\") FROM \"table\" AS t1"
      }
    },
    {
      "sql": "SELECT DATE_DIFF('day', DATE '2020-01-01', DATE '2020-01-05')",
      "read": {},
      "write": {
        "duckdb": "SELECT DATE_DIFF('DAY', CAST('2020-01-01' AS DATE), CAST('2020-01-05' AS DATE))",
        "trino": "SELECT DATE_DIFF('DAY', CAST('2020-01-01' AS DATE), CAST('2020-01-05' AS DATE))"
      }
    },
    {
      "sql": "WITH 'x' AS (SELECT 1) SELECT * FROM x",
      "read": {},
      "write": {
        "duckdb": "WITH \"x\" AS (SELECT 1) SELECT * FROM x"
      }
    },
    {
      "sql": "CREATE TABLE IF NOT EXISTS t (cola INT, colb STRING) USING ICEBERG PARTITIONED BY (colb)",
      "read": {},
      "write": {
        "duckdb": "CREATE TABLE IF NOT EXISTS t (cola INT, colb TEXT)"
      }
    },
    {
      "sql": "CREATE TABLE IF NOT EXISTS t (cola INT COMMENT 'cola', colb STRING) USING ICEBERG PARTITIONED BY (colb)",
      "read": {},
      "write": {
        "duckdb": "CREATE TABLE IF NOT EXISTS t (cola INT, colb TEXT)"
      }
    },
    {
      "sql": "[0, 1, 2]",
      "read": {
        "spark": "ARRAY(0, 1, 2)"
      },
      "write": {
        "bigquery": "[0, 1, 2]",
        "duckdb": "[0, 1, 2]",
        "presto": "ARRAY[0, 1, 2]",
        "spark": "ARRAY(0, 1, 2)"
      }
    },
    {
      "sql": "SELECT ARRAY_LENGTH([0], 1) AS x",
      "read": {},
      "write": {
        "duckdb": "SELECT ARRAY_LENGTH([0], 1) AS x"
      }
    },
    {
      "sql": "REGEXP_MATCHES(x, y)",
      "read": {},
      "write": {
        "duckdb": "REGEXP_MATCHES(x, y)",
        "presto": "REGEXP_LIKE(x, y)",
        "hive": "x RLIKE y",
        "spark": "x RLIKE y"
      }
    },
    {
      "sql": "STR_SPLIT(x, 'a')",
      "read": {},
      "write": {
        "duckdb": "STR_SPLIT(x, 'a')",
        "presto": "SPLIT(x, 'a')",
        "hive": "SPLIT(x, CONCAT('\\\\Q', 'a', '\\\\E'))",
        "spark": "SPLIT(x, CONCAT('\\\\Q', 'a', '\\\\E'))"
      }
    },
    {
      "sql": "STRING_TO_ARRAY(x, 'a')",
      "read": {
        "snowflake": "STRTOK_TO_ARRAY(x, 'a')"
      },
      "write": {
        "duckdb": "STR_SPLIT(x, 'a')",
        "presto": "SPLIT(x, 'a')",
        "hive": "SPLIT(x, CONCAT('\\\\Q', 'a', '\\\\E'))",
        "spark": "SPLIT(x, CONCAT('\\\\Q', 'a', '\\\\E'))"
      }
    },
    {
      "sql": "STR_SPLIT_REGEX(x, 'a')",
      "read": {},
      "write": {
        "duckdb": "STR_SPLIT_REGEX(x, 'a')",
        "presto": "REGEXP_SPLIT(x, 'a')",
        "hive": "SPLIT(x, 'a')",
        "spark": "SPLIT(x, 'a')"
      }
    },
    {
      "sql": "STRUCT_EXTRACT(x, 'abc')",
      "read": {},
      "write": {
        "duckdb": "STRUCT_EXTRACT(x, 'abc')",
        "presto": "x.abc",
        "hive": "x.abc",
        "postgres": "x.abc",
        "redshift": "x.abc",
        "spark": "x.abc"
      }
    },
    {
      "sql": "STRUCT_EXTRACT(STRUCT_EXTRACT(x, 'y'), 'abc')",
      "read": {},
      "write": {
        "duckdb": "STRUCT_EXTRACT(STRUCT_EXTRACT(x, 'y'), 'abc')",
        "presto": "x.y.abc",
        "hive": "x.y.abc",
        "spark": "x.y.abc"
      }
    },
    {
      "sql": "QUANTILE(x, 0.5)",
      "read": {},
      "write": {
        "duckdb": "QUANTILE(x, 0.5)",
        "presto": "APPROX_PERCENTILE(x, 0.5)",
        "hive": "PERCENTILE(x, 0.5)",
        "spark": "PERCENTILE(x, 0.5)"
      }
    },
    {
      "sql": "UNNEST(x)",
      "read": {
        "spark": "EXPLODE(x)"
      },
      "write": {
        "duckdb": "UNNEST(x)",
        "spark": "EXPLODE(x)"
      }
    },
    {
      "sql": "1d",
      "read": {},
      "write": {
        "duckdb": "1 AS d",
        "spark": "1 AS d"
      }
    },
    {
      "sql": "POWER(TRY_CAST(2 AS SMALLINT), 3)",
      "read": {
        "hive": "POW(2S, 3)",
        "spark": "POW(2S, 3)"
      },
      "write": {}
    },
    {
      "sql": "LIST_SUM([1, 2])",
      "read": {
        "spark": "ARRAY_SUM(ARRAY(1, 2))"
      },
      "write": {}
    },
    {
      "sql": "STRUCT_PACK(x := 1, y := '2')",
      "read": {},
      "write": {
        "bigquery": "STRUCT(1 AS x, '2' AS y)",
        "duckdb": "{'x': 1, 'y': '2'}",
        "spark": "STRUCT(1 AS x, '2' AS y)"
      }
    },
    {
      "sql": "STRUCT_PACK(key1 := 'value1', key2 := 42)",
      "read": {},
      "write": {
        "bigquery": "STRUCT('value1' AS key1, 42 AS key2)",
        "duckdb": "{'key1': 'value1', 'key2': 42}",
        "spark": "STRUCT('value1' AS key1, 42 AS key2)"
      }
    },
    {
      "sql": "ARRAY_REVERSE_SORT(x)",
      "read": {},
      "write": {
        "duckdb": "ARRAY_REVERSE_SORT(x)",
        "presto": "ARRAY_SORT(x, (a, b) -> CASE WHEN a < b THEN 1 WHEN a > b THEN -1 ELSE 0 END)",
        "hive": "SORT_ARRAY(x, FALSE)",
        "spark": "SORT_ARRAY(x, FALSE)"
      }
    },
    {
      "sql": "LIST_REVERSE_SORT(x)",
      "read": {},
      "write": {
        "duckdb": "ARRAY_REVERSE_SORT(x)",
        "presto": "ARRAY_SORT(x, (a, b) -> CASE WHEN a < b THEN 1 WHEN a > b THEN -1 ELSE 0 END)",
        "hive": "SORT_ARRAY(x, FALSE)",
        "spark": "SORT_ARRAY(x, FALSE)"
      }
    },
    {
      "sql": "LIST_SORT(x)",
      "read": {},
      "write": {
        "duckdb": "ARRAY_SORT(x)",
        "presto": "ARRAY_SORT(x)",
        "hive": "SORT_ARRAY(x)",
        "spark": "SORT_ARRAY(x)"
      }
    },
    {
      "sql": "SELECT fname, lname, age FROM person ORDER BY age DESC NULLS FIRST, fname ASC NULLS LAST, lname",
      "read": {},
      "write": {
        "duckdb": "SELECT fname, lname, age FROM person ORDER BY age DESC NULLS FIRST, fname ASC, lname"
      }
    },
    {
      "sql": "MONTH('2021-03-01')",
      "read": {},
      "write": {
        "duckdb": "MONTH('2021-03-01')",
        "presto": "MONTH('2021-03-01')",
        "hive": "MONTH('2021-03-01')",
        "spark": "MONTH('2021-03-01')"
      }
    },
    {
      "sql": "ARRAY_CONCAT([1, 2], [3, 4])",
      "read": {
        "bigquery": "ARRAY_CONCAT([1, 2], [3, 4])",
        "postgres": "ARRAY_CAT(ARRAY[1, 2], ARRAY[3, 4])",
        "snowflake": "ARRAY_CAT([1, 2], [3, 4])"
      },
      "write": {
        "bigquery": "ARRAY_CONCAT([1, 2], [3, 4])",
        "duckdb": "ARRAY_CONCAT([1, 2], [3, 4])",
        "hive": "CONCAT(ARRAY(1, 2), ARRAY(3, 4))",
        "postgres": "ARRAY_CAT(ARRAY[1, 2], ARRAY[3, 4])",
        "presto": "CONCAT(ARRAY[1, 2], ARRAY[3, 4])",
        "snowflake": "ARRAY_CAT([1, 2], [3, 4])",
        "spark": "CONCAT(ARRAY(1, 2), ARRAY(3, 4))"
      }
    },
    {
      "sql": "SELECT CAST(TRY_CAST(x AS DATE) AS DATE) + INTERVAL 1 DAY",
      "read": {
        "hive": "SELECT DATE_ADD(TO_DATE(x), 1)"
      },
      "write": {}
    },
    {
      "sql": "SELECT CAST('2018-01-01 00:00:00' AS DATE) + INTERVAL 3 DAY",
      "read": {
        "hive": "SELECT DATE_ADD('2018-01-01 00:00:00', 3)"
      },
      "write": {
        "duckdb": "SELECT CAST('2018-01-01 00:00:00' AS DATE) + INTERVAL '3' DAY",
        "hive": "SELECT CAST('2018-01-01 00:00:00' AS DATE) + INTERVAL '3' DAY"
      }
    },
    {
      "sql": "SELECT CAST('2020-05-06' AS DATE) - INTERVAL '5' DAY",
      "read": {
        "bigquery": "SELECT DATE_SUB(CAST('2020-05-06' AS DATE), INTERVAL 5 DAY)"
      },
      "write": {}
    },
    {
      "sql": "SELECT CAST('2020-05-06' AS DATE) + INTERVAL '5' DAY",
      "read": {
        "bigquery": "SELECT DATE_ADD(CAST('2020-05-06' AS DATE), INTERVAL 5 DAY)"
      },
      "write": {}
    },
    {
      "sql": "SELECT QUANTILE_CONT(x, q) FROM t",
      "read": {},
      "write": {
        "duckdb": "SELECT QUANTILE_CONT(x, q) FROM t",
        "postgres": "SELECT PERCENTILE_CONT(q) WITHIN GROUP (ORDER BY x) FROM t",
        "snowflake": "SELECT PERCENTILE_CONT(q) WITHIN GROUP (ORDER BY x) FROM t"
      }
    },
    {
      "sql": "SELECT QUANTILE_DISC(x, q) FROM t",
      "read": {},
      "write": {
        "duckdb": "SELECT QUANTILE_DISC(x, q) FROM t",
        "postgres": "SELECT PERCENTILE_DISC(q) WITHIN GROUP (ORDER BY x) FROM t",
        "snowflake": "SELECT PERCENTILE_DISC(q) WITHIN GROUP (ORDER BY x) FROM t"
      }
    },
    {
      "sql": "SELECT REGEXP_EXTRACT(a, 'pattern') FROM t",
      "read": {
        "duckdb": "SELECT REGEXP_EXTRACT(a, 'pattern') FROM t",
        "bigquery": "SELECT REGEXP_EXTRACT(a, 'pattern') FROM t",
        "snowflake": "SELECT REGEXP_SUBSTR(a, 'pattern') FROM t"
      },
      "write": {
        "duckdb": "SELECT REGEXP_EXTRACT(a, 'pattern') FROM t",
        "bigquery": "SELECT REGEXP_EXTRACT(a, 'pattern') FROM t",
        "snowflake": "SELECT REGEXP_SUBSTR(a, 'pattern') FROM t"
      }
    },
    {
      "sql": "SELECT REGEXP_EXTRACT(a, 'pattern', 2, 'i') FROM t",
      "read": {
        "snowflake": "SELECT REGEXP_SUBSTR(a, 'pattern', 1, 1, 'i', 2) FROM t"
      },
      "write": {
        "duckdb": "SELECT REGEXP_EXTRACT(a, 'pattern', 2, 'i') FROM t",
        "snowflake": "SELECT REGEXP_SUBSTR(a, 'pattern', 1, 1, 'i', 2) FROM t"
      }
    },
    {
      "sql": "SELECT COUNT_IF(x)",
      "read": {},
      "write": {
        "duckdb": "SELECT COUNT_IF(x)",
        "bigquery": "SELECT COUNTIF(x)"
      }
    },
    {
      "sql": "WITH t AS (SELECT i, i * i * i * i * i AS i5 FROM RANGE(1, 5) t(i)) SELECT * FROM t",
      "read": {},
      "write": {
        "duckdb": "WITH t AS (SELECT i, i * i * i * i * i AS i5 FROM RANGE(1, 5) AS t(i)) SELECT * FROM t",
        "sqlite": "WITH t AS (SELECT i, i * i * i * i * i AS i5 FROM (SELECT value AS i FROM GENERATE_SERIES(1, 5)) AS t) SELECT * FROM t"
      }
    },
    {
      "sql": "VARIANCE(a)",
      "read": {},
      "write": {
        "duckdb": "VARIANCE(a)",
        "clickhouse": "varSamp(a)"
      }
    },
    {
      "sql": "STDDEV(a)",
      "read": {},
      "write": {
        "duckdb": "STDDEV(a)",
        "clickhouse": "stddevSamp(a)"
      }
    },
    {
      "sql": "DATE_TRUNC('DAY', x)",
      "read": {},
      "write": {
        "duckdb": "DATE_TRUNC('DAY', x)",
        "clickhouse": "dateTrunc('DAY', x)"
      }
    },
    {
      "sql": "SELECT e'Hello\nworld'",
      "read": {},
      "write": {
        "duckdb": "SELECT e'Hello\\nworld'",
        "bigquery": "SELECT CAST(b'Hello\\nworld' AS STRING)"
      }
    },
    {
      "sql": "SELECT REGEXP_MATCHES('ThOmAs', 'thomas', 'i')",
      "read": {
        "postgres": "SELECT 'ThOmAs' ~* 'thomas'"
      },
      "write": {}
    },
    {
      "sql": "LIST_CONTAINS([1, 2, NULL], 1)",
      "read": {},
      "write": {
        "duckdb": "ARRAY_CONTAINS([1, 2, NULL], 1)",
        "postgres": "CASE WHEN 1 IS NULL THEN NULL ELSE COALESCE(1 = ANY(ARRAY[1, 2, NULL]), FALSE) END"
      }
    },
    {
      "sql": "LIST_CONTAINS([1, 2, NULL], NULL)",
      "read": {},
      "write": {
        "duckdb": "ARRAY_CONTAINS([1, 2, NULL], NULL)",
        "postgres": "CASE WHEN NULL IS NULL THEN NULL ELSE COALESCE(NULL = ANY(ARRAY[1, 2, NULL]), FALSE) END"
      }
    },
    {
      "sql": "LIST_HAS_ANY([1, 2, 3], [1,2])",
      "read": {},
      "write": {
        "duckdb": "[1, 2, 3] && [1, 2]",
        "postgres": "ARRAY[1, 2, 3] && ARRAY[1, 2]"
      }
    },
    {
      "sql": "SELECT CONCAT(foo)",
      "read": {},
      "write": {
        "duckdb": "SELECT CONCAT(foo)",
        "spark": "SELECT CONCAT(COALESCE(foo, ''))"
      }
    },
    {
      "sql": "SELECT CONCAT(COALESCE(['abc'], []), ['bcg'])",
      "read": {},
      "write": {
        "duckdb": "SELECT CONCAT(COALESCE(['abc'], []), ['bcg'])",
        "spark": "SELECT CONCAT(COALESCE(ARRAY('abc'), ARRAY()), ARRAY('bcg'))"
      }
    },
    {
      "sql": "SELECT UUID()",
      "read": {},
      "write": {
        "duckdb": "SELECT UUID()",
        "bigquery": "SELECT GENERATE_UUID()"
      }
    },
    {
      "sql": "CASE WHEN 2500 > 0 THEN ((2500 - 1) // 32768) + 1 ELSE 2500 // 32768 END",
      "read": {
        "snowflake": "BITMAP_BUCKET_NUMBER(2500)"
      },
      "write": {}
    },
    {
      "sql": "CASE WHEN 32768 > 0 THEN ((32768 - 1) // 32768) + 1 ELSE 32768 // 32768 END",
      "read": {
        "snowflake": "BITMAP_BUCKET_NUMBER(32768)"
      },
      "write": {}
    },
    {
      "sql": "CASE WHEN 32769 > 0 THEN ((32769 - 1) // 32768) + 1 ELSE 32769 // 32768 END",
      "read": {
        "snowflake": "BITMAP_BUCKET_NUMBER(32769)"
      },
      "write": {}
    },
    {
      "sql": "CASE WHEN -100 > 0 THEN ((-100 - 1) // 32768) + 1 ELSE -100 // 32768 END",
      "read": {
        "snowflake": "BITMAP_BUCKET_NUMBER(-100)"
      },
      "write": {}
    },
    {
      "sql": "CASE WHEN NULL > 0 THEN ((NULL - 1) // 32768) + 1 ELSE NULL // 32768 END",
      "read": {
        "snowflake": "BITMAP_BUCKET_NUMBER(NULL)"
      },
      "write": {}
    },
    {
      "sql": "SELECT CAST(CURRENT_TIMESTAMP AT TIME ZONE 'UTC' AS DATE)",
      "read": {
        "bigquery": "SELECT CURRENT_DATE('UTC')",
        "duckdb": "SELECT CAST(CURRENT_TIMESTAMP AT TIME ZONE 'UTC' AS DATE)"
      },
      "write": {}
    },
    {
      "sql": "SELECT MAKE_DATE(2016, 12, 25)",
      "read": {
        "bigquery": "SELECT DATE(2016, 12, 25)"
      },
      "write": {
        "bigquery": "SELECT DATE(2016, 12, 25)",
        "duckdb": "SELECT MAKE_DATE(2016, 12, 25)"
      }
    },
    {
      "sql": "SELECT CAST(CAST('2016-12-25 23:59:59' AS TIMESTAMP) AS DATE)",
      "read": {
        "bigquery": "SELECT DATE(DATETIME '2016-12-25 23:59:59')"
      },
      "write": {}
    },
    {
      "sql": "SELECT CAST(CAST(CAST('2016-12-25' AS TIMESTAMPTZ) AS TIMESTAMP) AT TIME ZONE 'UTC' AT TIME ZONE 'America/Los_Angeles' AS DATE)",
      "read": {
        "bigquery": "SELECT DATE(TIMESTAMP '2016-12-25', 'America/Los_Angeles')"
      },
      "write": {}
    },
    {
      "sql": "SELECT CAST(CAST('2024-01-15 23:30:00' AS TIMESTAMP) AT TIME ZONE 'UTC' AT TIME ZONE 'Europe/Berlin' AS DATE)",
      "read": {
        "bigquery": "SELECT DATE('2024-01-15 23:30:00', 'Europe/Berlin')"
      },
      "write": {}
    },
    {
      "sql": "SELECT CAST(CAST(STRPTIME('05/06/2020', '%m/%d/%Y') AS DATE) AS DATE)",
      "read": {
        "bigquery": "SELECT DATE(PARSE_DATE('%m/%d/%Y', '05/06/2020'))"
      },
      "write": {}
    },
    {
      "sql": "SELECT CAST('2020-01-01' AS DATE) + INTERVAL '-1' DAY",
      "read": {
        "mysql": "SELECT DATE '2020-01-01' + INTERVAL -1 DAY"
      },
      "write": {}
    },
    {
      "sql": "SELECT INTERVAL '1 quarter'",
      "read": {},
      "write": {
        "duckdb": "SELECT INTERVAL '1' QUARTER"
      }
    },
    {
      "sql": "SELECT ((DATE_TRUNC('DAY', CAST(CAST(DATE_TRUNC('DAY', CURRENT_TIMESTAMP) AS DATE) AS TIMESTAMP) + INTERVAL (0 - ((ISODOW(CAST(CAST(DATE_TRUNC('DAY', CURRENT_TIMESTAMP) AS DATE) AS TIMESTAMP)) % 7) - 1 + 7) % 7) DAY) + INTERVAL (-5) WEEK)) AS t1",
      "read": {
        "presto": "SELECT ((DATE_ADD('week', -5, DATE_TRUNC('DAY', DATE_ADD('day', (0 - MOD((DAY_OF_WEEK(CAST(CAST(DATE_TRUNC('DAY', NOW()) AS DATE) AS TIMESTAMP)) % 7) - 1 + 7, 7)), CAST(CAST(DATE_TRUNC('DAY', NOW()) AS DATE) AS TIMESTAMP)))))) AS t1"
      },
      "write": {}
    },
    {
      "sql": "EPOCH(x)",
      "read": {
        "presto": "TO_UNIXTIME(x)"
      },
      "write": {
        "bigquery": "TIME_TO_UNIX(x)",
        "duckdb": "EPOCH(x)",
        "presto": "TO_UNIXTIME(x)",
        "spark": "UNIX_TIMESTAMP(x)"
      }
    },
    {
      "sql": "EPOCH_MS(x)",
      "read": {},
      "write": {
        "bigquery": "TIMESTAMP_MILLIS(x)",
        "clickhouse": "fromUnixTimestamp64Milli(CAST(x AS Nullable(Int64)))",
        "duckdb": "EPOCH_MS(x)",
        "mysql": "FROM_UNIXTIME(x / POWER(10, 3))",
        "postgres": "TO_TIMESTAMP(CAST(x AS DOUBLE PRECISION) / POWER(10, 3))",
        "presto": "FROM_UNIXTIME(CAST(x AS DOUBLE) / POW(10, 3))",
        "spark": "TIMESTAMP_MILLIS(x)"
      }
    },
    {
      "sql": "STRFTIME(x, '%y-%-m-%S')",
      "read": {},
      "write": {
        "bigquery": "FORMAT_DATE('%y-%-m-%S', x)",
        "duckdb": "STRFTIME(x, '%y-%-m-%S')",
        "postgres": "TO_CHAR(x, 'YY-FMMM-SS')",
        "presto": "DATE_FORMAT(x, '%y-%c-%s')",
        "spark": "DATE_FORMAT(x, 'yy-M-ss')"
      }
    },
    {
      "sql": "SHA1(x)",
      "read": {},
      "write": {
        "duckdb": "SHA1(x)"
      }
    },
    {
      "sql": "STRFTIME(x, '%Y-%m-%d %H:%M:%S')",
      "read": {},
      "write": {
        "bigquery": "FORMAT_DATE('%F %T', x)",
        "duckdb": "STRFTIME(x, '%Y-%m-%d %H:%M:%S')",
        "presto": "DATE_FORMAT(x, '%Y-%m-%d %T')",
        "hive": "DATE_FORMAT(x, 'yyyy-MM-dd HH:mm:ss')"
      }
    },
    {
      "sql": "STRPTIME(x, '%y-%-m')",
      "read": {},
      "write": {
        "bigquery": "PARSE_TIMESTAMP('%y-%-m', x)",
        "duckdb": "STRPTIME(x, '%y-%-m')",
        "presto": "DATE_PARSE(x, '%y-%c')",
        "hive": "CAST(FROM_UNIXTIME(UNIX_TIMESTAMP(x, 'yy-M')) AS TIMESTAMP)",
        "spark": "TO_TIMESTAMP(x, 'yy-M')"
      }
    },
    {
      "sql": "TO_TIMESTAMP(x)",
      "read": {},
      "write": {
        "bigquery": "TIMESTAMP_SECONDS(x)",
        "duckdb": "TO_TIMESTAMP(x)",
        "presto": "FROM_UNIXTIME(x)",
        "hive": "FROM_UNIXTIME(x)"
      }
    },
    {
      "sql": "STRPTIME(x, '%-m/%-d/%y %-I:%M %p')",
      "read": {},
      "write": {
        "bigquery": "PARSE_TIMESTAMP('%-m/%e/%y %-I:%M %p', x)",
        "duckdb": "STRPTIME(x, '%-m/%-d/%y %-I:%M %p')",
        "presto": "DATE_PARSE(x, '%c/%e/%y %l:%i %p')",
        "hive": "CAST(FROM_UNIXTIME(UNIX_TIMESTAMP(x, 'M/d/yy h:mm a')) AS TIMESTAMP)",
        "spark": "TO_TIMESTAMP(x, 'M/d/yy h:mm a')"
      }
    },
    {
      "sql": "CAST(start AS TIMESTAMPTZ) AT TIME ZONE 'America/New_York'",
      "read": {
        "snowflake": "CONVERT_TIMEZONE('America/New_York', CAST(start AS TIMESTAMPTZ))"
      },
      "write": {
        "bigquery": "TIMESTAMP(DATETIME(CAST(start AS TIMESTAMP), 'America/New_York'))",
        "duckdb": "CAST(start AS TIMESTAMPTZ) AT TIME ZONE 'America/New_York'",
        "snowflake": "CONVERT_TIMEZONE('America/New_York', CAST(start AS TIMESTAMPTZ))"
      }
    },
    {
      "sql": "SELECT TIMESTAMP 'foo'",
      "read": {},
      "write": {
        "duckdb": "SELECT CAST('foo' AS TIMESTAMP)",
        "hive": "SELECT CAST('foo' AS TIMESTAMP)",
        "spark2": "SELECT CAST('foo' AS TIMESTAMP)",
        "spark": "SELECT CAST('foo' AS TIMESTAMP_NTZ)",
        "postgres": "SELECT CAST('foo' AS TIMESTAMP)",
        "mysql": "SELECT CAST('foo' AS DATETIME)",
        "clickhouse": "SELECT CAST('foo' AS Nullable(DateTime))",
        "databricks": "SELECT CAST('foo' AS TIMESTAMP_NTZ)",
        "snowflake": "SELECT CAST('foo' AS TIMESTAMPNTZ)",
        "redshift": "SELECT CAST('foo' AS TIMESTAMP)",
        "tsql": "SELECT CAST('foo' AS DATETIME2)",
        "presto": "SELECT CAST('foo' AS TIMESTAMP)",
        "trino": "SELECT CAST('foo' AS TIMESTAMP)",
        "oracle": "SELECT CAST('foo' AS TIMESTAMP)",
        "bigquery": "SELECT CAST('foo' AS DATETIME)",
        "starrocks": "SELECT CAST('foo' AS DATETIME)"
      }
    },
    {
      "sql": "SELECT * FROM example TABLESAMPLE RESERVOIR (3 ROWS) REPEATABLE (82)",
      "read": {
        "duckdb": "SELECT * FROM example TABLESAMPLE (3) REPEATABLE (82)",
        "snowflake": "SELECT * FROM example SAMPLE (3 ROWS) SEED (82)"
      },
      "write": {
        "duckdb": "SELECT * FROM example TABLESAMPLE RESERVOIR (3 ROWS) REPEATABLE (82)"
      }
    },
    {
      "sql": "SELECT * FROM (SELECT * FROM t) AS t1 TABLESAMPLE (1 ROWS), (SELECT * FROM t) AS t2 TABLESAMPLE (2 ROWS)",
      "read": {},
      "write": {
        "duckdb": "SELECT * FROM (SELECT * FROM t) AS t1 TABLESAMPLE RESERVOIR (1 ROWS), (SELECT * FROM t) AS t2 TABLESAMPLE RESERVOIR (2 ROWS)",
        "spark": "SELECT * FROM (SELECT * FROM t) TABLESAMPLE (1 ROWS) AS t1, (SELECT * FROM t) TABLESAMPLE (2 ROWS) AS t2"
      }
    },
    {
      "sql": "CAST(x AS TIME)",
      "read": {
        "duckdb": "CAST(x AS TIME)",
        "presto": "CAST(x AS TIME(6))"
      },
      "write": {}
    },
    {
      "sql": "SELECT CAST('2020-01-01 12:05:01' AS TIMESTAMP)",
      "read": {
        "duckdb": "SELECT CAST('2020-01-01 12:05:01' AS TIMESTAMP)",
        "snowflake": "SELECT CAST('2020-01-01 12:05:01' AS TIMESTAMPNTZ)"
      },
      "write": {}
    },
    {
      "sql": "SELECT CAST('2020-01-01' AS DATE) + INTERVAL (day_offset) DAY FROM t",
      "read": {
        "duckdb": "SELECT CAST('2020-01-01' AS DATE) + INTERVAL (day_offset) DAY FROM t",
        "mysql": "SELECT DATE '2020-01-01' + INTERVAL day_offset DAY FROM t"
      },
      "write": {}
    },
    {
      "sql": "SELECT CAST('09:05:03' AS TIME) + INTERVAL 2 HOUR",
      "read": {
        "snowflake": "SELECT TIMEADD(HOUR, 2, TO_TIME('09:05:03'))"
      },
      "write": {
        "duckdb": "SELECT CAST('09:05:03' AS TIME) + INTERVAL '2' HOUR",
        "snowflake": "SELECT CAST('09:05:03' AS TIME) + INTERVAL '2 HOUR'"
      }
    },
    {
      "sql": "CAST(x AS VARCHAR(5))",
      "read": {},
      "write": {
        "duckdb": "CAST(x AS TEXT)",
        "postgres": "CAST(x AS TEXT)"
      }
    },
    {
      "sql": "CAST(x AS DECIMAL(38, 0))",
      "read": {
        "snowflake": "CAST(x AS NUMBER)",
        "duckdb": "CAST(x AS DECIMAL(38, 0))"
      },
      "write": {
        "snowflake": "CAST(x AS DECIMAL(38, 0))"
      }
    },
    {
      "sql": "CAST(x AS NUMERIC)",
      "read": {},
      "write": {
        "duckdb": "CAST(x AS DECIMAL(18, 3))",
        "postgres": "CAST(x AS DECIMAL(18, 3))"
      }
    },
    {
      "sql": "CAST(x AS DECIMAL)",
      "read": {},
      "write": {
        "duckdb": "CAST(x AS DECIMAL(18, 3))",
        "postgres": "CAST(x AS DECIMAL(18, 3))"
      }
    },
    {
      "sql": "CAST(x AS BIT)",
      "read": {
        "duckdb": "CAST(x AS BITSTRING)"
      },
      "write": {
        "duckdb": "CAST(x AS BIT)",
        "tsql": "CAST(x AS BIT)"
      }
    },
    {
      "sql": "cast([[1]] as int[][])",
      "read": {},
      "write": {
        "duckdb": "CAST([[1]] AS INT[][])",
        "spark": "CAST(ARRAY(ARRAY(1)) AS ARRAY<ARRAY<INT>>)"
      }
    },
    {
      "sql": "CAST(x AS DATE) + INTERVAL (7 * -1) DAY",
      "read": {
        "spark": "DATE_SUB(x, 7)"
      },
      "write": {}
    },
    {
      "sql": "TRY_CAST(1 AS DOUBLE)",
      "read": {
        "hive": "1d",
        "spark": "1d"
      },
      "write": {}
    },
    {
      "sql": "CAST(x AS DATE)",
      "read": {},
      "write": {
        "duckdb": "CAST(x AS DATE)"
      }
    },
    {
      "sql": "COL::BIGINT[]",
      "read": {},
      "write": {
        "duckdb": "CAST(COL AS BIGINT[])",
        "presto": "CAST(COL AS ARRAY(BIGINT))",
        "hive": "CAST(COL AS ARRAY<BIGINT>)",
        "spark": "CAST(COL AS ARRAY<BIGINT>)",
        "postgres": "CAST(COL AS BIGINT[])",
        "snowflake": "CAST(COL AS ARRAY(BIGINT))"
      }
    },
    {
      "sql": "ENCODE(x)",
      "read": {
        "spark": "ENCODE(x, 'utf-8')",
        "presto": "TO_UTF8(x)"
      },
      "write": {
        "duckdb": "ENCODE(x)",
        "spark": "ENCODE(x, 'utf-8')",
        "presto": "TO_UTF8(x)"
      }
    },
    {
      "sql": "DECODE(x)",
      "read": {
        "spark": "DECODE(x, 'utf-8')",
        "presto": "FROM_UTF8(x)"
      },
      "write": {
        "duckdb": "DECODE(x)",
        "spark": "DECODE(x, 'utf-8')",
        "presto": "FROM_UTF8(x)"
      }
    },
    {
      "sql": "DECODE(x)",
      "read": {
        "presto": "FROM_UTF8(x, y)"
      },
      "write": {}
    },
    {
      "sql": "ALTER TABLE db.t1 RENAME TO db.t2",
      "read": {},
      "write": {
        "snowflake": "ALTER TABLE db.t1 RENAME TO db.t2",
        "duckdb": "ALTER TABLE db.t1 RENAME TO t2",
        "tsql": "EXEC sp_rename 'db.t1', 't2'"
      }
    },
    {
      "sql": "ALTER TABLE \"db\".\"t1\" RENAME TO \"db\".\"t2\"",
      "read": {},
      "write": {
        "snowflake": "ALTER TABLE \"db\".\"t1\" RENAME TO \"db\".\"t2\"",
        "duckdb": "ALTER TABLE \"db\".\"t1\" RENAME TO \"t2\"",
        "tsql": "EXEC sp_rename '[db].[t1]', 't2'"
      }
    },
    {
      "sql": "SELECT w::TIMESTAMP_S, x::TIMESTAMP_MS, y::TIMESTAMP_US, z::TIMESTAMP_NS",
      "read": {},
      "write": {
        "duckdb": "SELECT CAST(w AS TIMESTAMP_S), CAST(x AS TIMESTAMP_MS), CAST(y AS TIMESTAMP), CAST(z AS TIMESTAMP_NS)"
      }
    },
    {
      "sql": "ISNAN(x)",
      "read": {
        "bigquery": "IS_NAN(x)"
      },
      "write": {
        "bigquery": "IS_NAN(x)",
        "duckdb": "ISNAN(x)"
      }
    },
    {
      "sql": "ISINF(x)",
      "read": {
        "bigquery": "IS_INF(x)"
      },
      "write": {
        "bigquery": "IS_INF(x)",
        "duckdb": "ISINF(x)"
      }
    },
    {
      "sql": "SELECT $foo",
      "read": {
        "bigquery": "SELECT @foo"
      },
      "write": {
        "bigquery": "SELECT @foo",
        "duckdb": "SELECT $foo"
      }
    },
    {
      "sql": "SET VARIABLE a = 1",
      "read": {},
      "write": {
        "duckdb": "SET VARIABLE a = 1",
        "bigquery": "SET a = 1",
        "snowflake": "SET a = 1"
      }
    },
    {
      "sql": "SELECT BIT_OR(int_value) FROM t",
      "read": {
        "snowflake": "SELECT BITOR_AGG(int_value) FROM t",
        "duckdb": "SELECT BIT_OR(int_value) FROM t"
      },
      "write": {}
    },
    {
      "sql": "SELECT BIT_AND(int_value) FROM t",
      "read": {
        "snowflake": "SELECT BITAND_AGG(int_value) FROM t",
        "duckdb": "SELECT BIT_AND(int_value) FROM t"
      },
      "write": {}
    },
    {
      "sql": "SELECT BIT_XOR(int_value) FROM t",
      "read": {
        "snowflake": "SELECT BITXOR_AGG(int_value) FROM t",
        "duckdb": "SELECT BIT_XOR(int_value) FROM t"
      },
      "write": {}
    },
    {
      "sql": "SELECT BIT_OR(CAST(val AS FLOAT)) FROM t",
      "read": {},
      "write": {
        "duckdb": "SELECT BIT_OR(CAST(ROUND(CAST(val AS REAL)) AS INT)) FROM t",
        "snowflake": "SELECT BITORAGG(CAST(val AS FLOAT)) FROM t"
      }
    },
    {
      "sql": "SELECT BIT_AND(CAST(val AS DOUBLE)) FROM t",
      "read": {},
      "write": {
        "duckdb": "SELECT BIT_AND(CAST(ROUND(CAST(val AS DOUBLE)) AS INT)) FROM t",
        "snowflake": "SELECT BITANDAGG(CAST(val AS DOUBLE)) FROM t"
      }
    },
    {
      "sql": "SELECT BIT_OR(CAST(val AS DECIMAL(10, 2))) FROM t",
      "read": {},
      "write": {
        "duckdb": "SELECT BIT_OR(CAST(CAST(val AS DECIMAL(10, 2)) AS INT)) FROM t",
        "snowflake": "SELECT BITORAGG(CAST(val AS DECIMAL(10, 2))) FROM t"
      }
    },
    {
      "sql": "SELECT BIT_XOR(CAST(val AS DECIMAL)) FROM t",
      "read": {},
      "write": {
        "duckdb": "SELECT BIT_XOR(CAST(CAST(val AS DECIMAL(18, 3)) AS INT)) FROM t",
        "snowflake": "SELECT BITXORAGG(CAST(val AS DECIMAL(18, 3))) FROM t"
      }
    },
    {
      "sql": "SELECT APPROX_QUANTILE(a, 0.5) FROM t",
      "read": {
        "snowflake": "SELECT APPROX_PERCENTILE(a, 0.5) FROM t"
      },
      "write": {
        "duckdb": "SELECT APPROX_QUANTILE(a, 0.5) FROM t",
        "snowflake": "SELECT APPROX_PERCENTILE(a, 0.5) FROM t"
      }
    },
    {
      "sql": "SELECT some_arr[1] AS first FROM blah",
      "read": {
        "bigquery": "SELECT some_arr[0] AS first FROM blah"
      },
      "write": {
        "bigquery": "SELECT some_arr[0] AS first FROM blah",
        "duckdb": "SELECT some_arr[1] AS first FROM blah",
        "presto": "SELECT some_arr[1] AS first FROM blah"
      }
    }
  ]
}