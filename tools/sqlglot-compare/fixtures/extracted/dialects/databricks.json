{
  "dialect": "databricks",
  "identity": [
    {
      "sql": "SELECT COSH(1.5)",
      "expected": null
    },
    {
      "sql": "SELECT BITMAP_BIT_POSITION(10)",
      "expected": null
    },
    {
      "sql": "SELECT BITMAP_BUCKET_NUMBER(32769)",
      "expected": null
    },
    {
      "sql": "SELECT BITMAP_CONSTRUCT_AGG(value)",
      "expected": null
    },
    {
      "sql": "SELECT EXP(1)",
      "expected": null
    },
    {
      "sql": "SELECT MODE(category)",
      "expected": null
    },
    {
      "sql": "SELECT MODE(price, TRUE) AS deterministic_mode FROM products",
      "expected": null
    },
    {
      "sql": "REGEXP_LIKE(x, y)",
      "expected": null
    },
    {
      "sql": "SELECT CAST(NULL AS VOID)",
      "expected": null
    },
    {
      "sql": "SELECT void FROM t",
      "expected": null
    },
    {
      "sql": "SELECT * FROM stream",
      "expected": null
    },
    {
      "sql": "SELECT * FROM STREAM t",
      "expected": null
    },
    {
      "sql": "SELECT t.current_time FROM t",
      "expected": null
    },
    {
      "sql": "ALTER TABLE labels ADD COLUMN label_score FLOAT",
      "expected": null
    },
    {
      "sql": "DESCRIBE HISTORY a.b",
      "expected": null
    },
    {
      "sql": "DESCRIBE history.tbl",
      "expected": null
    },
    {
      "sql": "CREATE TABLE t (a STRUCT<c: MAP<STRING, STRING>>)",
      "expected": null
    },
    {
      "sql": "CREATE TABLE t (c STRUCT<interval: DOUBLE COMMENT 'aaa'>)",
      "expected": null
    },
    {
      "sql": "CREATE TABLE my_table TBLPROPERTIES (a.b=15)",
      "expected": null
    },
    {
      "sql": "CREATE TABLE my_table TBLPROPERTIES ('a.b'=15)",
      "expected": null
    },
    {
      "sql": "SELECT CAST('11 23:4:0' AS INTERVAL DAY TO HOUR)",
      "expected": null
    },
    {
      "sql": "SELECT CAST('11 23:4:0' AS INTERVAL DAY TO MINUTE)",
      "expected": null
    },
    {
      "sql": "SELECT CAST('11 23:4:0' AS INTERVAL DAY TO SECOND)",
      "expected": null
    },
    {
      "sql": "SELECT CAST('23:00:00' AS INTERVAL HOUR TO MINUTE)",
      "expected": null
    },
    {
      "sql": "SELECT CAST('23:00:00' AS INTERVAL HOUR TO SECOND)",
      "expected": null
    },
    {
      "sql": "SELECT CAST('23:00:00' AS INTERVAL MINUTE TO SECOND)",
      "expected": null
    },
    {
      "sql": "CREATE TABLE target SHALLOW CLONE source",
      "expected": null
    },
    {
      "sql": "INSERT INTO a REPLACE WHERE cond VALUES (1), (2)",
      "expected": null
    },
    {
      "sql": "CREATE FUNCTION a.b(x INT) RETURNS INT RETURN x + 1",
      "expected": null
    },
    {
      "sql": "CREATE FUNCTION a AS b",
      "expected": null
    },
    {
      "sql": "SELECT ${x} FROM ${y} WHERE ${z} > 1",
      "expected": null
    },
    {
      "sql": "CREATE TABLE foo (x DATE GENERATED ALWAYS AS (CAST(y AS DATE)))",
      "expected": null
    },
    {
      "sql": "TRUNCATE TABLE t1 PARTITION(age = 10, name = 'test', address)",
      "expected": null
    },
    {
      "sql": "SELECT PARSE_JSON('{}')",
      "expected": null
    },
    {
      "sql": "SELECT RANDSTR(123)",
      "expected": null
    },
    {
      "sql": "SELECT RANDSTR(123, 456)",
      "expected": null
    },
    {
      "sql": "PARSE_URL('https://example.com/path')",
      "expected": null
    },
    {
      "sql": "PARSE_URL('https://example.com/path', 'HOST')",
      "expected": null
    },
    {
      "sql": "PARSE_URL('https://example.com/path', 'QUERY', 'param')",
      "expected": null
    },
    {
      "sql": "CREATE TABLE IF NOT EXISTS db.table (a TIMESTAMP, b BOOLEAN GENERATED ALWAYS AS (NOT a IS NULL)) USING DELTA",
      "expected": null
    },
    {
      "sql": "SELECT * FROM sales UNPIVOT INCLUDE NULLS (sales FOR quarter IN (q1 AS `Jan-Mar`))",
      "expected": null
    },
    {
      "sql": "SELECT * FROM sales UNPIVOT EXCLUDE NULLS (sales FOR quarter IN (q1 AS `Jan-Mar`))",
      "expected": null
    },
    {
      "sql": "CREATE FUNCTION add_one(x INT) RETURNS INT LANGUAGE PYTHON AS $$def add_one(x):\n  return x+1$$",
      "expected": null
    },
    {
      "sql": "CREATE FUNCTION add_one(x INT) RETURNS INT LANGUAGE PYTHON AS $FOO$def add_one(x):\n  return x+1$FOO$",
      "expected": null
    },
    {
      "sql": "TRUNCATE TABLE t1 PARTITION(age = 10, name = 'test', city LIKE 'LA')",
      "expected": null
    },
    {
      "sql": "COPY INTO target FROM `s3://link` FILEFORMAT = AVRO VALIDATE = ALL FILES = ('file1', 'file2') FORMAT_OPTIONS ('opt1'='true', 'opt2'='test') COPY_OPTIONS ('mergeSchema'='true')",
      "expected": null
    },
    {
      "sql": "SELECT * FROM t1, t2",
      "expected": "SELECT * FROM t1 CROSS JOIN t2"
    },
    {
      "sql": "SELECT TIMESTAMP '2025-04-29 18.47.18'::DATE",
      "expected": "SELECT CAST(CAST('2025-04-29 18.47.18' AS DATE) AS TIMESTAMP)"
    },
    {
      "sql": "SELECT DATE_FORMAT(CAST(FROM_UTC_TIMESTAMP(foo, 'America/Los_Angeles') AS TIMESTAMP), 'yyyy-MM-dd HH:mm:ss') AS foo FROM t",
      "expected": "SELECT DATE_FORMAT(CAST(FROM_UTC_TIMESTAMP(CAST(foo AS TIMESTAMP), 'America/Los_Angeles') AS TIMESTAMP), 'yyyy-MM-dd HH:mm:ss') AS foo FROM t"
    },
    {
      "sql": "SELECT r\"\\\\foo.bar\\\"",
      "expected": "SELECT '\\\\\\\\foo.bar\\\\'"
    },
    {
      "sql": "FROM_UTC_TIMESTAMP(x::TIMESTAMP, tz)",
      "expected": "FROM_UTC_TIMESTAMP(CAST(x AS TIMESTAMP), tz)"
    },
    {
      "sql": "SELECT SUBSTRING_INDEX(str, delim, count)",
      "expected": null
    },
    {
      "sql": "BITMAP_OR_AGG(x)",
      "expected": null
    },
    {
      "sql": "SELECT SUBSTR('Spark' FROM 5 FOR 1)",
      "expected": "SELECT SUBSTRING('Spark', 5, 1)"
    },
    {
      "sql": "SELECT SUBSTR('Spark SQL', 5)",
      "expected": "SELECT SUBSTRING('Spark SQL', 5)"
    },
    {
      "sql": "SELECT SUBSTR(ENCODE('Spark SQL', 'utf-8'), 5)",
      "expected": "SELECT SUBSTRING(ENCODE('Spark SQL', 'utf-8'), 5)"
    },
    {
      "sql": "SELECT test, LISTAGG(email, '') AS Email FROM organizations GROUP BY test",
      "expected": null
    },
    {
      "sql": "WITH t AS (VALUES ('foo_val') AS t(foo1)) SELECT foo1 FROM t",
      "expected": "WITH t AS (SELECT * FROM VALUES ('foo_val') AS t(foo1)) SELECT foo1 FROM t"
    },
    {
      "sql": "NTILE() OVER (ORDER BY 1)",
      "expected": null
    },
    {
      "sql": "CURRENT_VERSION()",
      "expected": null
    },
    {
      "sql": "SELECT ELT(2, 'foo', 'bar', 'baz') AS Result",
      "expected": null
    },
    {
      "sql": "GETDATE()",
      "expected": "CURRENT_TIMESTAMP()"
    },
    {
      "sql": "NOW()",
      "expected": "CURRENT_TIMESTAMP()"
    },
    {
      "sql": "CURRENT_TIMEZONE()",
      "expected": null
    },
    {
      "sql": "CURDATE()",
      "expected": "CURRENT_DATE"
    },
    {
      "sql": "CURDATE",
      "expected": "CURRENT_DATE"
    },
    {
      "sql": "SELECT MAKE_INTERVAL(100, 11, 12, 13, 14, 14, 15)",
      "expected": null
    },
    {
      "sql": "SELECT name, GROUPING_ID() FROM customer GROUP BY ROLLUP (name)",
      "expected": null
    },
    {
      "sql": "SELECT c1:price, c1:price.foo, c1:price.bar[1]",
      "expected": null
    },
    {
      "sql": "SELECT TRY_CAST(c1:price AS ARRAY<VARIANT>)",
      "expected": null
    },
    {
      "sql": "SELECT TRY_CAST(c1:[\"foo bar\"][\"baz qux\"] AS ARRAY<VARIANT>)",
      "expected": null
    },
    {
      "sql": "SELECT c1:item[1].price FROM VALUES ('{ \"item\": [ { \"model\" : \"basic\", \"price\" : 6.12 }, { \"model\" : \"medium\", \"price\" : 9.24 } ] }') AS T(c1)",
      "expected": null
    },
    {
      "sql": "SELECT c1:item[*].price FROM VALUES ('{ \"item\": [ { \"model\" : \"basic\", \"price\" : 6.12 }, { \"model\" : \"medium\", \"price\" : 9.24 } ] }') AS T(c1)",
      "expected": null
    },
    {
      "sql": "SELECT FROM_JSON(c1:item[*].price, 'ARRAY<DOUBLE>')[0] FROM VALUES ('{ \"item\": [ { \"model\" : \"basic\", \"price\" : 6.12 }, { \"model\" : \"medium\", \"price\" : 9.24 } ] }') AS T(c1)",
      "expected": null
    },
    {
      "sql": "SELECT INLINE(FROM_JSON(c1:item[*], 'ARRAY<STRUCT<model STRING, price DOUBLE>>')) FROM VALUES ('{ \"item\": [ { \"model\" : \"basic\", \"price\" : 6.12 }, { \"model\" : \"medium\", \"price\" : 9.24 } ] }') AS T(c1)",
      "expected": null
    },
    {
      "sql": "SELECT c1:['price'] FROM VALUES ('{ \"price\": 5 }') AS T(c1)",
      "expected": "SELECT c1:price FROM VALUES ('{ \"price\": 5 }') AS T(c1)"
    },
    {
      "sql": "SELECT GET_JSON_OBJECT(c1, '$.price') FROM VALUES ('{ \"price\": 5 }') AS T(c1)",
      "expected": "SELECT c1:price FROM VALUES ('{ \"price\": 5 }') AS T(c1)"
    },
    {
      "sql": "SELECT raw:`zip code`, raw:`fb:testid`, raw:store['bicycle'], raw:store[\"zip code\"]",
      "expected": "SELECT raw:[\"zip code\"], raw:[\"fb:testid\"], raw:store.bicycle, raw:store[\"zip code\"]"
    },
    {
      "sql": "CREATE STREAMING TABLE raw_data AS SELECT * FROM STREAM READ_FILES('abfss://container@storageAccount.dfs.core.windows.net/base/path')",
      "expected": null
    },
    {
      "sql": "CREATE OR REFRESH STREAMING TABLE csv_data (id INT, ts TIMESTAMP, event STRING) AS SELECT * FROM STREAM READ_FILES('s3://bucket/path', format => 'csv', schema => 'id int, ts timestamp, event string')",
      "expected": null
    },
    {
      "sql": "GRANT CREATE ON SCHEMA my_schema TO `alf@melmak.et`",
      "expected": null
    },
    {
      "sql": "GRANT SELECT ON TABLE sample_data TO `alf@melmak.et`",
      "expected": null
    },
    {
      "sql": "GRANT ALL PRIVILEGES ON TABLE forecasts TO finance",
      "expected": null
    },
    {
      "sql": "GRANT SELECT ON TABLE t TO `fab9e00e-ca35-11ec-9d64-0242ac120002`",
      "expected": null
    },
    {
      "sql": "REVOKE CREATE ON SCHEMA my_schema FROM `alf@melmak.et`",
      "expected": null
    },
    {
      "sql": "REVOKE SELECT ON TABLE sample_data FROM `alf@melmak.et`",
      "expected": null
    },
    {
      "sql": "REVOKE ALL PRIVILEGES ON TABLE forecasts FROM finance",
      "expected": null
    },
    {
      "sql": "REVOKE SELECT ON TABLE t FROM `fab9e00e-ca35-11ec-9d64-0242ac120002`",
      "expected": null
    },
    {
      "sql": "ANALYZE TABLE tbl COMPUTE DELTA STATISTICS NOSCAN",
      "expected": null
    },
    {
      "sql": "ANALYZE TABLE tbl COMPUTE DELTA STATISTICS FOR ALL COLUMNS",
      "expected": null
    },
    {
      "sql": "ANALYZE TABLE tbl COMPUTE DELTA STATISTICS FOR COLUMNS foo, bar",
      "expected": null
    },
    {
      "sql": "ANALYZE TABLE ctlg.db.tbl COMPUTE DELTA STATISTICS NOSCAN",
      "expected": null
    },
    {
      "sql": "ANALYZE TABLES COMPUTE STATISTICS NOSCAN",
      "expected": null
    },
    {
      "sql": "ANALYZE TABLES FROM db COMPUTE STATISTICS",
      "expected": null
    },
    {
      "sql": "ANALYZE TABLES IN db COMPUTE STATISTICS",
      "expected": null
    },
    {
      "sql": "ANALYZE TABLE ctlg.db.tbl PARTITION(foo = 'foo', bar = 'bar') COMPUTE STATISTICS NOSCAN",
      "expected": null
    },
    {
      "sql": "CREATE FUNCTION a() ENVIRONMENT (dependencies = '[\"foo1==1\", \"foo2==2\"]', environment_version = 'None')",
      "expected": null
    },
    {
      "sql": "SELECT '20'?::INTEGER",
      "expected": "SELECT TRY_CAST('20' AS INTEGER)"
    },
    {
      "sql": "SELECT OVERLAY('Spark SQL', 'ANSI ', 7, 0)",
      "expected": "SELECT OVERLAY('Spark SQL' PLACING 'ANSI ' FROM 7 FOR 0)"
    },
    {
      "sql": "SELECT OVERLAY('Spark SQL' PLACING 'CORE' FROM 7)",
      "expected": null
    },
    {
      "sql": "SELECT OVERLAY(ENCODE('Spark SQL', 'utf-8') PLACING ENCODE('_', 'utf-8') FROM 6)",
      "expected": null
    },
    {
      "sql": "SELECT OVERLAY('Spark SQL' PLACING 'ANSI ' FROM 7 FOR 0)",
      "expected": null
    },
    {
      "sql": "DATE_DIFF(day, created_at, current_date())",
      "expected": "DATEDIFF(DAY, created_at, CURRENT_DATE)"
    }
  ],
  "transpilation": [
    {
      "sql": "SELECT SUBSTRING_INDEX('a.b.c.d', '.', 2)",
      "read": {},
      "write": {
        "databricks": "SELECT SUBSTRING_INDEX('a.b.c.d', '.', 2)",
        "spark": "SELECT SUBSTRING_INDEX('a.b.c.d', '.', 2)",
        "mysql": "SELECT SUBSTRING_INDEX('a.b.c.d', '.', 2)"
      }
    },
    {
      "sql": "SELECT TYPEOF(1)",
      "read": {
        "databricks": "SELECT TYPEOF(1)",
        "snowflake": "SELECT TYPEOF(1)",
        "hive": "SELECT TYPEOF(1)",
        "clickhouse": "SELECT toTypeName(1)"
      },
      "write": {
        "clickhouse": "SELECT toTypeName(1)"
      }
    },
    {
      "sql": "SELECT c1:item[1].price",
      "read": {
        "spark": "SELECT GET_JSON_OBJECT(c1, '$.item[1].price')"
      },
      "write": {
        "databricks": "SELECT c1:item[1].price",
        "spark": "SELECT GET_JSON_OBJECT(c1, '$.item[1].price')"
      }
    },
    {
      "sql": "SELECT GET_JSON_OBJECT(c1, '$.item[1].price')",
      "read": {},
      "write": {
        "databricks": "SELECT c1:item[1].price",
        "spark": "SELECT GET_JSON_OBJECT(c1, '$.item[1].price')"
      }
    },
    {
      "sql": "CREATE TABLE foo (x INT GENERATED ALWAYS AS (YEAR(y)))",
      "read": {},
      "write": {
        "databricks": "CREATE TABLE foo (x INT GENERATED ALWAYS AS (YEAR(y)))",
        "tsql": "CREATE TABLE foo (x AS YEAR(CAST(y AS DATE)))"
      }
    },
    {
      "sql": "CREATE TABLE t1 AS (SELECT c FROM t2)",
      "read": {
        "teradata": "CREATE TABLE t1 AS (SELECT c FROM t2) WITH DATA"
      },
      "write": {}
    },
    {
      "sql": "SELECT X'1A2B'",
      "read": {
        "spark2": "SELECT X'1A2B'",
        "spark": "SELECT X'1A2B'",
        "databricks": "SELECT x'1A2B'"
      },
      "write": {
        "spark2": "SELECT X'1A2B'",
        "spark": "SELECT X'1A2B'",
        "databricks": "SELECT X'1A2B'"
      }
    },
    {
      "sql": "CREATE OR REPLACE FUNCTION func(a BIGINT, b BIGINT) RETURNS TABLE (a INT) RETURN SELECT a",
      "read": {},
      "write": {
        "databricks": "CREATE OR REPLACE FUNCTION func(a BIGINT, b BIGINT) RETURNS TABLE (a INT) RETURN SELECT a",
        "duckdb": "CREATE OR REPLACE FUNCTION func(a, b) AS TABLE SELECT a"
      }
    },
    {
      "sql": "CREATE OR REPLACE FUNCTION func(a BIGINT, b BIGINT) RETURNS BIGINT RETURN a",
      "read": {},
      "write": {
        "databricks": "CREATE OR REPLACE FUNCTION func(a BIGINT, b BIGINT) RETURNS BIGINT RETURN a",
        "duckdb": "CREATE OR REPLACE FUNCTION func(a, b) AS a"
      }
    },
    {
      "sql": "SELECT ANY(col) FROM VALUES (TRUE), (FALSE) AS tab(col)",
      "read": {
        "databricks": "SELECT ANY(col) FROM VALUES (TRUE), (FALSE) AS tab(col)",
        "spark": "SELECT ANY(col) FROM VALUES (TRUE), (FALSE) AS tab(col)"
      },
      "write": {
        "spark": "SELECT ANY(col) FROM VALUES (TRUE), (FALSE) AS tab(col)"
      }
    },
    {
      "sql": "UNIFORM(1, 10, 5)",
      "read": {},
      "write": {
        "snowflake": "UNIFORM(1, 10, RANDOM(5))",
        "databricks": "UNIFORM(1, 10, 5)"
      }
    },
    {
      "sql": "UNIFORM(1, 10)",
      "read": {},
      "write": {
        "databricks": "UNIFORM(1, 10)",
        "snowflake": "UNIFORM(1, 10, RANDOM())"
      }
    },
    {
      "sql": "SELECT col:`fr'uit`",
      "read": {},
      "write": {
        "databricks": "SELECT col:[\"fr'uit\"]",
        "postgres": "SELECT JSON_EXTRACT_PATH(col, 'fr''uit')"
      }
    },
    {
      "sql": "SELECT DATEDIFF(year, 'start', 'end')",
      "read": {},
      "write": {
        "tsql": "SELECT DATEDIFF(YEAR, 'start', 'end')",
        "databricks": "SELECT DATEDIFF(YEAR, 'start', 'end')"
      }
    },
    {
      "sql": "SELECT DATEDIFF(microsecond, 'start', 'end')",
      "read": {},
      "write": {
        "databricks": "SELECT DATEDIFF(MICROSECOND, 'start', 'end')",
        "postgres": "SELECT CAST(EXTRACT(epoch FROM CAST('end' AS TIMESTAMP) - CAST('start' AS TIMESTAMP)) * 1000000 AS BIGINT)"
      }
    },
    {
      "sql": "SELECT DATEDIFF(millisecond, 'start', 'end')",
      "read": {},
      "write": {
        "databricks": "SELECT DATEDIFF(MILLISECOND, 'start', 'end')",
        "postgres": "SELECT CAST(EXTRACT(epoch FROM CAST('end' AS TIMESTAMP) - CAST('start' AS TIMESTAMP)) * 1000 AS BIGINT)"
      }
    },
    {
      "sql": "SELECT DATEDIFF(second, 'start', 'end')",
      "read": {},
      "write": {
        "databricks": "SELECT DATEDIFF(SECOND, 'start', 'end')",
        "postgres": "SELECT CAST(EXTRACT(epoch FROM CAST('end' AS TIMESTAMP) - CAST('start' AS TIMESTAMP)) AS BIGINT)"
      }
    },
    {
      "sql": "SELECT DATEDIFF(minute, 'start', 'end')",
      "read": {},
      "write": {
        "databricks": "SELECT DATEDIFF(MINUTE, 'start', 'end')",
        "postgres": "SELECT CAST(EXTRACT(epoch FROM CAST('end' AS TIMESTAMP) - CAST('start' AS TIMESTAMP)) / 60 AS BIGINT)"
      }
    },
    {
      "sql": "SELECT DATEDIFF(hour, 'start', 'end')",
      "read": {},
      "write": {
        "databricks": "SELECT DATEDIFF(HOUR, 'start', 'end')",
        "postgres": "SELECT CAST(EXTRACT(epoch FROM CAST('end' AS TIMESTAMP) - CAST('start' AS TIMESTAMP)) / 3600 AS BIGINT)"
      }
    },
    {
      "sql": "SELECT DATEDIFF(day, 'start', 'end')",
      "read": {},
      "write": {
        "databricks": "SELECT DATEDIFF(DAY, 'start', 'end')",
        "postgres": "SELECT CAST(EXTRACT(epoch FROM CAST('end' AS TIMESTAMP) - CAST('start' AS TIMESTAMP)) / 86400 AS BIGINT)"
      }
    },
    {
      "sql": "SELECT DATEDIFF(week, 'start', 'end')",
      "read": {},
      "write": {
        "databricks": "SELECT DATEDIFF(WEEK, 'start', 'end')",
        "postgres": "SELECT CAST(EXTRACT(days FROM (CAST('end' AS TIMESTAMP) - CAST('start' AS TIMESTAMP))) / 7 AS BIGINT)"
      }
    },
    {
      "sql": "SELECT DATEDIFF(month, 'start', 'end')",
      "read": {},
      "write": {
        "databricks": "SELECT DATEDIFF(MONTH, 'start', 'end')",
        "postgres": "SELECT CAST(EXTRACT(year FROM AGE(CAST('end' AS TIMESTAMP), CAST('start' AS TIMESTAMP))) * 12 + EXTRACT(month FROM AGE(CAST('end' AS TIMESTAMP), CAST('start' AS TIMESTAMP))) AS BIGINT)"
      }
    },
    {
      "sql": "SELECT DATEDIFF(quarter, 'start', 'end')",
      "read": {},
      "write": {
        "databricks": "SELECT DATEDIFF(QUARTER, 'start', 'end')",
        "postgres": "SELECT CAST(EXTRACT(year FROM AGE(CAST('end' AS TIMESTAMP), CAST('start' AS TIMESTAMP))) * 4 + EXTRACT(month FROM AGE(CAST('end' AS TIMESTAMP), CAST('start' AS TIMESTAMP))) / 3 AS BIGINT)"
      }
    },
    {
      "sql": "SELECT DATEDIFF(year, 'start', 'end')",
      "read": {},
      "write": {
        "databricks": "SELECT DATEDIFF(YEAR, 'start', 'end')",
        "postgres": "SELECT CAST(EXTRACT(year FROM AGE(CAST('end' AS TIMESTAMP), CAST('start' AS TIMESTAMP))) AS BIGINT)"
      }
    },
    {
      "sql": "SELECT DATEADD(year, 1, '2020-01-01')",
      "read": {},
      "write": {
        "tsql": "SELECT DATEADD(YEAR, 1, '2020-01-01')",
        "databricks": "SELECT DATEADD(YEAR, 1, '2020-01-01')"
      }
    },
    {
      "sql": "SELECT DATEDIFF('end', 'start')",
      "read": {},
      "write": {
        "databricks": "SELECT DATEDIFF(DAY, 'start', 'end')"
      }
    },
    {
      "sql": "SELECT DATE_ADD('2020-01-01', 1)",
      "read": {},
      "write": {
        "tsql": "SELECT DATEADD(DAY, 1, '2020-01-01')",
        "databricks": "SELECT DATEADD(DAY, 1, '2020-01-01')"
      }
    },
    {
      "sql": "CREATE TABLE x (SELECT 1)",
      "read": {},
      "write": {
        "databricks": "CREATE TABLE x AS (SELECT 1)"
      }
    },
    {
      "sql": "WITH x (select 1) SELECT * FROM x",
      "read": {},
      "write": {
        "databricks": "WITH x AS (SELECT 1) SELECT * FROM x"
      }
    }
  ]
}